{
  "hash": "0752d2b9cd1f64b38daa236b2cf25d49",
  "result": {
    "markdown": "---\ntitle: \"SCF Computing Cluster\"\nauthor: \"Andrew Vaughn, James Duncan\"\ndate: \"2022-10-07\"\nformat:\n  pdf:\n    documentclass: article\n    margin-left: 30mm\n    margin-right: 30mm\n    toc: true\n  html:\n    theme: cosmo\n    css: ../../styles.css\n    toc: true\n    code-copy: true\n    code-block-background: true\nexecute:\n  freeze: auto\n---\n\n[PDF](./scf.pdf){.btn .btn-primary}\n\n# Overview\n\nToday we'll discuss the Statistical Computing Facility's (SCF) cluster, which is\nadministrated by the Department of Statistics. Much of this material comes\ndirectly from the SCF's documentation; see the links below for more details:\n\n- [SCF homepage](https://statistics.berkeley.edu/computing/getting-started)\n- [SCF Linux cluster](https://statistics.berkeley.edu/computing/servers/cluster)\n- [SCF FAQs and How-Tos](https://statistics.berkeley.edu/computing/faqs)\n- [SCF Parallel processing in R tutorial](https://berkeley-scf.github.io/tutorial-parallelization/parallel-R)\n- [How to get help on the SCF](https://statistics.berkeley.edu/computing/how-get-help)\n- [SCF JupyterHub](https://jupyter.stat.berkeley.edu/)\n\n# SCF cluster capabilities and hardware\n\n:::{.callout-tip collapse=\"true\"}\n## Clusters, partitions, and nodes! Oh my!\n\nA **cluster** is a collection of computing **nodes**. A node is analogous to a\nlaptop or desktop computer, with an operating system, RAM, CPUs, and sometimes\nGPUs.\n\n![](./images/cluster.png)\n[Image source](https://hbctraining.github.io/Intro-to-shell-flipped/lessons/08_HPC_intro_and_terms.html)\n\n Nodes with similar capabilities or access policies are often grouped together\nas a **partition**. This allows cluster users to request the resources they need\n-- such as number of CPU **cores** -- from a pool of nodes, rather than specify\na specific one.\n\n:::\n\nThe SCF cluster has 1064 cores, across 7 partitions with more than 26 nodes, but\nfor this class we'll use the `low` partition. It's the default, so you won't\nneed to do anything special to use it for your jobs (we'll explain how to submit\njobs later on).\n\nThe `low` partition has the following resources:\n\n- 8 nodes\n- 32 cores per node\n- 256 GB RAM per node\n\nThe SCF Linux cluster uses a batch job scheduler called\n[Slurm](https://en.wikipedia.org/wiki/Slurm_Workload_Manager), commonly used by\nlarge computer centers in both academia and industry. Although the SCF cluster\nhas much greater computational capacity than our personal computers, it is a\nshared resource with many users. With Slurm, the cluster is able to schedule\njobs asynchronously, balancing resource allocations so that everyone gets a\nturn. This also means that when you submit a job, it may not run until many\nminutes or hours later. Be sure to give yourself enough time!\n\n<!--\n## Compute servers\n\nOn the other hand, the SCF cluster also has a number of interactive computing\nservers. If you've already used `ssh` to access the cluster, then you may\nalready be familiar with some of them as a subset are also used as login nodes.\nAfter logging, you can run `sitehosts compute` to see which nodes are intended\nas computing nodes. Currently:\n\n\nBefore using one of the compute servers, be sure to read through\n[Compute Server Policies](https://statistics.berkeley.edu/computing/faqs/what-are-policies-using-compute-servers)\n-->\n\n# Your `~/` on the SCF\n\nNodes in the SCF cluster use the Linux distribution Ubuntu. Every user has a\nprivate home directory which, as usual on Linux-based OSes, has the shortcut\n`~/`. In this section, we'll give an overview of access, disk space, and remote\nfile transfers to and from your SCF home directory.\n\n## Logging in\n\nThe SCF has a number of **login nodes** which you can access via `ssh`. \n\n:::{.callout-note}\n\nFor info on using `ssh` (including on Windows), see\n[here](https://statistics.berkeley.edu/computing/ssh).\n\n:::\n\nFor example, I'll use `ssh` to connect to the `dorothy` node:\n\n```sh\njames@pop-os:~$ ssh jpduncan@dorothy.berkeley.edu\nThe authenticity of host 'dorothy.berkeley.edu (128.32.135.58)' can't be established.\nED25519 key fingerprint is SHA256:rOY7ED/iIiTgI++Y4XHmiEl+tC+OmSGBvWp03CSII5E.\nThis key is not known by any other names\nAre you sure you want to continue connecting (yes/no/[fingerprint])? yes\nWarning: Permanently added 'dorothy.berkeley.edu' (ED25519) to the list of known hosts.\n```\n\nNotice that upon first connecting to a server you haven't visited there is a\nwarning that the \"authenticity of the host .. can't be established\". So long as\nyou have typed in the hostname correctly (`dorothy.berkeley.edu`, in this case),\nand trust the host (we trust the SCF!) then you can type `yes` to add the host\nto your known hosts file (found on your local machine at `~/.ssh/known_hosts`).\n\nYou'll then be asked to enter your password for the SCF cluster. For privacy,\nyou won't see anything happen in your terminal when you type it in, so type\ncarefully (you can use `Backspace` if you make a mistake) and press `Enter` when\nyou're done. If you were successful, you should see a welcome message and your\nshell prompt, like:\n\n\n\n```{bash}\n#| eval: false\njpduncan@dorothy:~$\n/accounts/grad/jpduncan\n```\n\n\n\nTo get your bearings, you can type `pwd` to see where your home directory is\nlocated on the SCF cluster filesystem:\n\n\n\n```{bash}\n#| eval: false\njpduncan@dorothy:~$ pwd\n/accounts/grad/jpduncan\n```\n\n\n\nYour home directory is likely also in the `/accounts/grad/` directory, as mine\nis.\n\n### Other login nodes\n\n:::{.callout-important}\n\nDon't run computationally intensive tasks on the login nodes! \n\nThey are shared by all the SCF users, and should only be used for non-intensive\ninteractive work such as job submission and monitoring, basic compilation,\nmanaging your disk space, and transferring data to/from the server.\n\n\n:::\n\n\nIf for some reason `dorothy` is not working for you, the SCF has a number of\nnodes which can be accessed from your local machine with commands of the form\n`ssh <scf-username>@<hostname>.berkeley.edu`. Currently, these are:\n\n- `aragorn`\n- `arwen`\n- `dorothy`\n- `gandalf`\n- `gollum`\n- `hermione`\n- `quidditch`\n- `radagast`\n- `shelob`\n\n## Disk space\n\n## Data transfer: SCP / SFTP\n\n# Running jobs on the SCF\n\n### Non-interactive jobs: `sbatch`\n\n:::{.callout-note}\n\nThis is the best option when you have a long-running job where no \n\n:::\n\n\n### Interactive jobs\n\n:::{.callout-note}\n\nInteractive jobs are the best option when you need to do many short but\ncomputationally intensive tasks which require your hands at the keyboard.\n\n:::\n\n#### `srun`\n\n:::{.callout-note}\n\nThis is the best option when you want to work at the Linux command line and\ndon't need a GUI, e.g. when debugging components of larger non-interactive job.\n\n:::\n\n\n#### JupyterHub for RStudio or Jupyter Notebooks\n\n:::{.callout-note}\n\nThis is the best option when you want to interact with the RStudio or Jupyter\nGUI while doing your computations or debugging.\n\n:::\n\nThe SCF's [JupyterHub](https://jupyter.stat.berkeley.edu/) is another resource\nfor interactive computing on the SCF, allowing you to use RStudio or Jupyter on\none of the cluster's computing nodes.\n\n### Specifying resources\n\n### Monitoring your jobs\n\n##\n\n",
    "supporting": [
      "scf_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}