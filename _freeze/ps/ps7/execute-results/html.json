{
  "hash": "7ab6342aaaefee4441ea27355c854751",
  "result": {
    "markdown": "---\ntitle: \"Problem Set 7\"\nsubtitle: \"Due Tuesday Nov. 15, 10 am\"\nformat:\n  pdf:\n    documentclass: article\n    margin-left: 30mm\n    margin-right: 30mm\n    toc: false\n  html:\n    theme: cosmo\n    css: ../styles.css\n    toc: false\n    code-copy: true\n    code-block-background: true\nexecute:\n  freeze: auto\n---\n\n\n\n\n## Comments\n\n- This covers Unit 10.\n- It's due at 10 am (Pacific) on Tuesday (yes, Tuesday) November 15, both submitted as a PDF to Gradescope as well as committed to your GitHub repository.\n- Please see PS1 and the grading rubric for formatting and attribution requirements.\n\n## Problems (mostly from 2020)\n\n1. Suppose I need to compute the generalized least squares estimator,\n    $\\hat{\\beta}=(X^{\\top}\\Sigma^{-1}X)^{-1}X^{\\top}\\Sigma^{-1}Y$, where\n    $X$ is $n\\times p$ and $\\Sigma$ is a positive definite $n\\times n$ matrix. Assume that $n>p$ and \n    $n$ could be of order several thousand and $p$ of order in the\n    hundreds. First write out in pseudo-code how you would do this in an\n    efficient way - i.e., the particular linear algebra steps and the\n    order of operations. Then write efficient R code in the form of a\n    function, `gls()`, to do this - you can rely on the various\n    high-level functions for matrix decompositions and solving systems\n    of equations, but you should not use any code that already exists\n    for doing generalized least squares.\n\n2. We've seen how to use Gaussian elimination (i.e., the LU\n    decomposition) to solve $Ax=b$ and that we can do the solution in\n    $n^{3}/3$ operations (plus lower-order terms). Suppose I want to\n    know how inefficient it is to explicitly invert the matrix $A$ and\n    then multiply, thereby finding $x=A^{-1}b$ via matrix-vector\n    multiplication. If we look at R's `solve.default()`, we see it\n    solves the system $AZ=I$ to find $Z=A^{-1}$. Next note that\n    `help(solve)` indicates it calls a Lapack routine [DGESV](http://www.netlib.org/lapack/explore-html/d7/d3b/group__double_g_esolve_ga5ee879032a8365897c3ba91e3dc8d512.html),\n    which uses the LU decomposition.\n    Count the number of computations for\n\n    a.  transforming $AZ=I$ to $UZ=I^{*}$ (where $I^{*}$ is no longer a\n        diagonal matrix),\n    b.  for solving for $Z$ given $UZ=I^{*}$, and\n    c.  for calculating $x=Zb$.\n\n    Then compare the total cost to the $n^{3}/3$ cost of what we saw in\n    class.\n\n    Notes:\n    - In counting the computations you should be able to make use\n    of various results we derived in class concerning the Gaussian\n    elimination computations and computations involved in a backsolve,\n    so your answer should be able to simply combine together results\n    we've already discussed without any detailed new derivation.\n    - Given that R's call to `dgesv` doesn't take account of\n    the special (diagonal) structure of $I$ on the right-hand side, you do not need\n    to take account of the fact that because $I$ has zeroes and ones,\n    one can actually save some computation. You can simply count the\n    calculations as if $I$ were filled with arbitrary values. (Note: if\n    we did actually try to be careful about making use of the structure\n    of $I$, it turns out we could save $n^{3}/3$ calculations.)\n\n3.  Two-stage least squares (2SLS) is a way of implementing a causal\n    inference method called instrumental variables that is commonly used\n    in economics. Consider the following set of regression equations:\n    $$\\begin{aligned}\n    \\hat{X} & = & Z(Z^{\\top}Z)^{-1}Z^{\\top}X\\\\\n    \\hat{\\beta} & = & (\\hat{X}^{\\top}\\hat{X})^{-1}\\hat{X}^{\\top}y\\end{aligned}$$\n    which can be interpreted as regressing $y$ on $X$ after filtering\n    such that we only retain variation in $X$ that is correlated with\n    the instrumental variable $Z$. An economics graduate student asked\n    how he could compute $\\hat{\\beta}$ if $Z$ is 60 million by 630, $X$\n    is 60 million by 600, and $y$ is $60$ million by 1, but both $Z$ and\n    $X$ are sparse matrices.\n\n    a.  Describe briefly why I can't do this calculation in two steps as\n        given in the equations, even if I use the techniques for OLS\n        discussed in class for each stage.\n\n    b.  Figure out how to rewrite the equations such that you can\n        actually calculate $\\hat{\\beta}$ on a computer without a huge\n        amount of memory. You can assume that any matrix multiplications\n        involving sparse matrices can be done on the computer (e.g.,\n        using the *spam* package in R).  Describe the specific steps of\n        how you would do this and/or write out in pseudo-code.\n\n    Notes:\n    - The product of two sparse matrices is not (in general)\n    sparse and would not be sparse in this case.\n    - As discussed in\n    Section 6 of Unit 10, there are R packages (and software packages\n    more generally) for efficiently storing (to save memory) and\n    efficiently doing matrix manipulations (to save computation time)\n    with sparse matrices.\n\n4. (Extra credit) In class we saw that the condition number when\n    solving a system of equations, $Ax=b$, is the ratio of the absolute\n    values of the largest and smallest magnitude eigenvalues of $A$.\n    Show that $\\|A\\|_{2}$ (i.e., the matrix norm induced by the usual L2\n    vector norm; see Section 1 of Unit 10) is the largest of the\n    absolute values of the eigenvalues of $A$ for symmetric $A$. To do\n    so, find the following quantity,\n    $$\\|A\\|_{2}=\\sup_{z:\\|z\\|_{2}=1}\\sqrt{(Az)^{\\top}Az}.$$ If you're\n    not familiar with the notion of the supremum (the *sup* here), just\n    think of it as the maximum. It accounts for situations such as\n    trying to find the maximum of the numbers in the open interval\n    (0,1). The max is undefined in this case since there is always a\n    number closer to 1 than any number you choose, but the *sup* in this\n    case is 1.\n\n    Hints: when you get to having the quantity $\\Gamma^{\\top}z$ for\n    orthogonal $\\Gamma$, set $y=\\Gamma^{\\top}z$ and show that if\n    $\\|z\\|_{2}=1$ then $\\|y\\|_{2}=1$. Finally, if you have the quantity\n    $y^{\\top}Dy$, think about how this can be rewritten given the form\n    of $D$ and think intuitively about how to maximize it if\n    $\\|y\\|_{2}=1$.\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}