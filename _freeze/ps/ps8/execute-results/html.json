{
  "hash": "f4f89c974d34698a2a06cc1ac13ba42d",
  "result": {
    "markdown": "---\ntitle: \"Problem Set 8\"\nsubtitle: \"Due Friday Dec. 2, 5 pm\"\nformat:\n  pdf:\n    documentclass: article\n    margin-left: 30mm\n    margin-right: 30mm\n    toc: false\n  html:\n    theme: cosmo\n    css: ../styles.css\n    toc: false\n    code-copy: true\n    code-block-background: true\nexecute:\n  freeze: auto\n---\n\n\n\n\n## Comments\n\n- This covers Unit 11.\n- It's due at 5 pm (Pacific) on Friday (yes, 5 pm) December 2, both submitted as a PDF to Gradescope as well as committed to your GitHub repository.\n- Please see PS1 and the grading rubric for formatting and attribution requirements.\n- We won't cover the EM algorithm in class until Monday November 28, so you may want to wait until after that to tackle Problem 2. However, you can do parts of Problem 2d/2e without having done the EM part of the problem. \n\n\n## Problems \n\n\n1.  Consider the \"helical valley\" function:\n\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    theta <- function(x1,x2)\n        atan2(x2, x1)/(2*pi)\n    \n    helical <- function(x) {\n        f1 <- 10*(x[3] - 10*theta(x[1],x[2]))\n        f2 <- 10*(sqrt(x[1]^2 + x[2]^2) - 1)\n        f3 <- x[3]\n        return(f1^2 + f2^2 + f3^2)\n    }\n    ```\n    :::\n\n\n\n    Plot slices of the function to get a sense for how it\n    behaves (i.e., for a constant value of one of the inputs, plot as a\n    2-d function of the other two). Syntax for `image()`, `contour()` or\n    `persp()` (or the `ggplot2` equivalents) from the R bootcamp materials\n    will be helpful (you can also plot using Python if you prefer).\n    Now try out `optim()` (using more than one of the methods provided through the `method` argument) and `nlm()` for finding the\n    minimum of this function. Or if you prefer, use `optimx()` with multiple methods. Explore the\n    possibility of multiple local minima by using different starting\n    points.\n\n2.  Consider probit regression, which is an alternative to logistic\n    regression for binary outcomes. The probit model is\n    $Y_{i}\\sim\\mbox{Ber}(p_{i})$ for\n    $p_{i}=P(Y_{i}=1)=\\Phi(X_{i}^{\\top}\\beta)$ where $\\Phi$ is the\n    standard normal CDF, and $\\mbox{Ber}$ is the Bernoulli distribution.\n    We can rewrite this model with latent\n    variables, one latent variable, $z_{i}$, for each observation:\n    $$\\begin{aligned}\n    y_{i} & = & I(z_{i}>0)\\\\\n    z_{i} & \\sim & \\mathcal{N}(X_{i}^{\\top}\\beta,1)\\end{aligned}$$\n\n    a.  Design an EM algorithm to estimate $\\beta$, taking the complete\n        data to be ${Y,Z}$. You'll need to make use of the mean and\n        variance of truncated normal distributions (see hint below). Be\n        careful that you carefully distinguish $\\beta$ from the current\n        value at iteration $t$, $\\beta^{t}$, in writing out the expected\n        log-likelihood and computing the expectation and that your\n        maximization be with respect to $\\beta$ (not $\\beta^{t}$). Also\n        be careful that your calculations respect the fact that for each\n        $z_{i}$ you know that it is either bigger or smaller than $0$\n        based on its $y_{i}$. You should be able to analytically\n        maximize the expected log likelihood. A couple hints:\n\n        i.  From the Johnson and Kotz 'bibles' on distributions, the mean\n            and variance of the truncated normal distribution,\n            $f(w)\\propto\\mathcal{N}(w;\\mu,\\sigma^{2})I(w>\\tau)$, are:\n            $$\\begin{aligned}\n            E(W|W>\\tau) & = & \\mu+\\sigma\\rho(\\tau^{*})\\\\\n            V(W|W>\\tau) & = & \\sigma^{2}\\left(1+\\tau^{*}\\rho(\\tau^{*})-\\rho(\\tau^{*})^{2}\\right)\\\\\n            \\rho(\\tau^{*}) & = & \\frac{\\phi(\\tau^{*})}{1-\\Phi(\\tau^{*})}\\\\\n            \\tau^{*} & = & (\\tau-\\mu)/\\sigma,\\end{aligned}$$ where\n            $\\phi(\\cdot)$ is the standard normal density and\n            $\\Phi(\\cdot)$ is the standard normal CDF. Or see the\n            Wikipedia page on the truncated normal distribution for more\n            general formulae.\n\n        ii.  You should recognize that your expected log-likelihood can\n            be expressed as a regression of some new quantities (which\n            you might denote as $m_{i}$, $i=1,\\ldots,n$, where the\n            $m_{i}$ are functions of $\\beta^{t}$ and $y_{i}$) on $X$.\n\n    b.  Propose how to get reasonable starting values for $\\beta$.\n\n    c.  Write an R function, with auxiliary functions as needed, to\n        estimate the parameters. Make use of the initialization from\n        part (b). You may use `lm()` for the update steps. You'll need\n        to include criteria for deciding when to stop the optimization.\n\n    d.  Test your function using data simulated from the model, with\n        $\\beta_{0},\\beta_{1},\\beta_{2},\\beta_{3}$. Take $n=100$ and the\n        parameters such that\n        $\\hat{\\beta}_{1}/se(\\hat{\\beta}_{1})\\approx2$ and\n        $\\beta_{2}=\\beta_{3}=0$. In other words, I want you to choose\n        $\\beta_{1}$ such that the signal to noise ratio in the\n        relationship between $x_{1}$ and $y$ is moderately large. You\n        can do this via trial and error simply by simulating data for a\n        given $\\beta_{1}$ and fitting a logistic regression to get the\n        estimate and standard error. Then adjust $\\beta_{1}$ as needed.\n\n    e.  A different approach to this problem just directly maximizes the\n        log-likelihood of the observed data under the original probit model (i.e., without the `z`s). Estimate the parameters\n        (and standard errors, based on the Hessian at the optimum) for your test cases using `optim()` with\n        the BFGS option in R. Compare how many iterations EM and BFGS\n        take.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}