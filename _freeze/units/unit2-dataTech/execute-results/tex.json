{
  "hash": "890e911f11b7ed131b242e8d72075164",
  "result": {
    "markdown": "---\ntitle: \"Data technologies, formats, and structures\"\nauthor: \"Chris Paciorek\"\ndate: \"2022-08-23\"\nformat:\n  pdf:\n    documentclass: article\n    margin-left: 30mm\n    margin-right: 30mm\n    toc: true\n  html:\n    theme: cosmo\n    css: ../styles.css\n    toc: true\n    code-copy: true\n    code-block-background: true\nexecute:\n  freeze: auto\n---\n\n\n\n[PDF](./unit2-dataTech.pdf){.btn .btn-primary}\n\nReferences (see [syllabus](../syllabus) for links):\n\n-   Adler\n-   Nolan and Temple Lang, XML and Web Technologies for Data Sciences\n    with R.\n-   Chambers\n-   [R intro manual](http://cran.r-project.org/doc/manuals/R-intro.html)\n    on CRAN (R-intro).\n-   Venables and Ripley, Modern Applied Statistics with S\n-   Murrell, Introduction to Data Technologies.\n-   [R Data Import/Export\n    manual](http://cran.r-project.org/doc/manuals/R-data.html) on CRAN\n    (R-data).\n-   SCF tutorial: [Working with large datasets in SQL, R, and Python](https://berkeley-scf.github.io/tutorial-databases/)\n\n(Optional) Videos\n\nThere are four videos from 2020 in the bCourses Media Gallery that you\ncan use for reference if you want to:\n\n1.  Text files and ASCII\n2.  Encodings and UTF-8\n3.  HTML\n4.  XML and JSON\n\n# 1. Data storage and file formats on a computer\n\nWe're going to start early in the data analysis pipeline: getting data,\nreading data in, writing data out to disk, and webscraping. We'll focus\non doing these manipulations in R, but the concepts and tools involved\nare common to other languages, so familarity with these in R should\nallow you to pick up other tools easily. The main downside to\nworking with datasets in R (true for Python and most other languages as well) is that the entire\ndataset resides in memory, so R is not so good for dealing with very\nlarge datasets. More on alternatives in a later unit. R (and similar\nlanguages) has the capability to read in a wide variety of file formats.\n\n## Text and binary files\n\nIn general, files can be divided into text files and binary files. In\nboth cases, information is stored as a series of bits. Recall that a bit\nis a single value in base 2 (i.e., a 0 or a 1), while a byte is 8 bits.\n\nA **text file** is one in which the bits in the file encode individual\ncharacters. Note that the characters can include the digit characters\n0-9, so one can include numbers in a text file by writing down the\ndigits needed for the number of interest. Examples of text file formats\ninclude CSV, XML, HTML, and JSON.\n\nText files may be simple ASCII files (i.e., files encoded using ASCII)\nor in other encodings such as UTF-8, both covered in Section 5.\n[ASCII](http://en.wikipedia.org/wiki/ASCII) files have 8 bits (1 byte)\nper character and can represent 128 characters (the 52 lower and upper\ncase letters in English, 10 digits, punctuation and a few other things\n-- basically what you see on a standard US keyboard). UTF-8 files have\nbetween 1 and 4 bytes per character.\n\nA **binary file** is one in which the bits in the file encode the\ninformation in a custom format and not simply individual characters.\nBinary formats are not (easily) human readable but can be more\nspace-efficient and faster to work with (because it can allow random\naccess into the data rather than requiring sequential reading). The\nmeaning of the bytes in such files depends on the specific binary format\nbeing used and a program that uses the file needs to know how the format\nrepresents information. Examples of binary files include netCDF files, R\ndata (e.g., .Rda) files, Python pickle files, and compiled code files.\n\nNumbers in binary files are usually stored as 8 bytes per number. We'll\ndiscuss this much more in Unit 8.\n\n## Common file types\n\nHere are some of the common file types, some of which are text formats and some of which are binary formats.\n\n1.  'Flat' text files: data are often provided as simple text files.\n    Often one has one record or observation per row and each column or\n    field is a different variable or type of information about the\n    record. Such files can either have a fixed number of characters in\n    each field (*fixed width format*) or a special character (a *delimiter*)\n    that separates the fields in each row. Common delimiters are tabs,\n    commas, one or more spaces, and the pipe (\\|). Common file\n    extensions are *.txt* and *.csv*. Metadata (information about the\n    data) are often stored in a separate file. CSV files are quite\n    common, but if you have files where the data contain commas, other\n    delimiters can be good. Text can be put in quotes in CSV files, and\n    this can allow use of commas within the data. This is difficult to\n    deal with from the command line, but *read.table()* in R handles this situation.\n\n    -   One occasionally tricky difficulty is as follows. If you have a\n        text file created in Windows, the line endings are coded\n        differently than in UNIX. Windows uses a newline\n        (the ASCII character `\\n`)\n        and a carriage return (the ASCII character `\\r`) whereas\n        UNIX uses onlyl a  newline in UNIX). There are\n        UNIX utilities (`fromdos` in\n        Ubuntu, including the SCF Linux machines and `dos2unix` in other\n        Linux distributions) that can do the necessary conversion. If\n        you see *\\^M* at the end of the lines in a file, that's the tool\n        you need. Alternatively, if you open a UNIX file in Windows, it\n        may treat all the lines as a single line. You can fix this with\n        `todos` or `unix2dos`.\n\n2.  In some contexts, such as textual data and bioinformatics data, the\n    data may be in a text file with one piece of information per row, but\n    without meaningful columns/fields.\n\n3.  In scientific contexts, netCDF (*.nc*) (and the related HDF5) are\n    popular format for gridded data that allows for highly-efficient\n    storage and contains the metadata within the file. The basic\n    structure of a netCDF file is that each variable is an array with\n    multiple dimensions (e.g., latitude, longitude, and time), and one\n    can also extract the values of and metadata about each dimension.\n    The *ncdf4* package in R nicely handles working with netCDF files.\n\n4.  Data may also be in text files in formats designed for data\n    interchange between various languages, in particular XML or JSON.\n    These formats are \"self-describing\"; namely the metadata is part of\n    the file. The *XML2, rvest*, and *jsonlite* packages are useful for\n    reading and writing from these formats. More in Section 4.\n\n5.  You may be scraping information on the web, so dealing with text\n    files in various formats, including HTML. The *XML2* and *rvest*\n    packages are also useful for reading HTML.\n\n6.  Data may already be in a database or in the data storage format of another\n    statistical package (*Stata*, *SAS*, *SPSS*, etc.). The *foreign*\n    package in R has excellent capabilities for importing Stata\n    (*read.dta()*), SPSS (*read.spss()*), and SAS (*read.ssd()* and, for\n    XPORT files, *read.xport()*), among others.\n\n7.  For Excel, there are capabilities to read an Excel file (see the\n    *readxl* and *XLConnect* package among others), but you can also\n    just go into Excel and export as a CSV file or the like and then\n    read that into R. In general, it's best not to pass around data\n    files as Excel or other spreadsheet format files because (1) Excel\n    is proprietary, so someone may not have Excel and the format is\n    subject to change, (2) Excel imposes limits on the number of\n    rows, (3) one can easily manipulate text files such as CSV using\n    UNIX tools, but this is not possible with an Excel file, (4) Excel\n    files often have more than one sheet, graphs, macros, etc., so\n    they're not a data storage format per se.\n\n8.  R can easily interact with databases (SQLite, PostgreSQL, MySQL,\n    Oracle, etc.), querying the database using SQL and returning results\n    to R. More in the big data unit and in the large datasets tutorial\n    mentioned above.\n\n# 2. Reading data from text files into R\n\n## Core R functions\n\n*read.table()* is probably the most commonly-used function for reading\nin data. It reads in delimited files (*read.csv()* and *read.delim()*\nare special cases of *read.table()*). The key arguments are the\ndelimiter (the *sep* argument) and whether the file contains a header, a\nline with the variable names. We can use *read.fwf()* to read from a\nfixed width text file into a data frame.\n\nThe most difficult part of reading in such files can be dealing with how\nR determines the classes of the fields that are read in. There are a\nnumber of arguments to *read.table()* and *read.fwf()* that allow the\nuser to control the classes. One difficulty in older versions of R was\nthat character fields were read in as factors.\n\nLet's work through a couple examples. Before we do that, let's look at\nthe arguments to *read.table()*. Note that `sep=''` separates on any\namount of white space. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat <- read.table(file.path('..', 'data', 'RTADataSub.csv'),\n                  sep = ',', header = TRUE)\nsapply(dat, class)[1:10]  # What are the classes of the columns?\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nX2010.08.02.18.55             X2336              X549             X2086 \n      \"character\"       \"character\"       \"character\"       \"character\" \n             X666              X481              X298             X1624 \n      \"character\"       \"character\"       \"character\"       \"character\" \n            X1732              X593 \n      \"character\"       \"character\" \n```\n:::\n\n```{.r .cell-code}\n## whoops, there is an 'x', presumably indicating missingness:\nunique(dat[ , 2])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  [1] \"2124\" \"1830\" \"1833\" \"1600\" \"1578\" \"1187\" \"1005\" \"918\"  \"865\"  \"871\" \n [11] \"860\"  \"883\"  \"897\"  \"898\"  \"893\"  \"913\"  \"870\"  \"962\"  \"880\"  \"875\" \n [21] \"884\"  \"894\"  \"836\"  \"848\"  \"885\"  \"851\"  \"900\"  \"861\"  \"866\"  \"867\" \n [31] \"829\"  \"853\"  \"920\"  \"877\"  \"908\"  \"855\"  \"845\"  \"859\"  \"856\"  \"825\" \n [41] \"828\"  \"854\"  \"847\"  \"840\"  \"873\"  \"822\"  \"818\"  \"838\"  \"815\"  \"813\" \n [51] \"816\"  \"849\"  \"802\"  \"805\"  \"792\"  \"823\"  \"808\"  \"798\"  \"800\"  \"842\" \n [61] \"809\"  \"807\"  \"826\"  \"810\"  \"801\"  \"794\"  \"771\"  \"796\"  \"790\"  \"787\" \n [71] \"775\"  \"751\"  \"783\"  \"811\"  \"768\"  \"779\"  \"795\"  \"770\"  \"821\"  \"830\" \n [81] \"767\"  \"772\"  \"791\"  \"781\"  \"773\"  \"777\"  \"814\"  \"778\"  \"782\"  \"837\" \n [91] \"759\"  \"846\"  \"797\"  \"835\"  \"832\"  \"793\"  \"803\"  \"834\"  \"785\"  \"831\" \n[101] \"820\"  \"812\"  \"824\"  \"728\"  \"760\"  \"762\"  \"753\"  \"758\"  \"764\"  \"741\" \n[111] \"709\"  \"735\"  \"749\"  \"752\"  \"761\"  \"750\"  \"776\"  \"766\"  \"789\"  \"763\" \n[121] \"864\"  \"858\"  \"869\"  \"886\"  \"844\"  \"863\"  \"916\"  \"890\"  \"872\"  \"907\" \n[131] \"926\"  \"935\"  \"933\"  \"906\"  \"905\"  \"912\"  \"972\"  \"996\"  \"1009\" \"961\" \n[141] \"952\"  \"981\"  \"917\"  \"1011\" \"1071\" \"1920\" \"3245\" \"3805\" \"3926\" \"3284\"\n[151] \"2700\" \"2347\" \"2078\" \"2935\" \"3040\" \"1860\" \"1437\" \"1512\" \"1720\" \"1493\"\n[161] \"1026\" \"928\"  \"874\"  \"833\"  \"850\"  \"\"     \"x\"   \n```\n:::\n\n```{.r .cell-code}\n## let's treat 'x' as a missing value indicator\ndat2 <- read.table(file.path('..', 'data', 'RTADataSub.csv'),\n                   sep = ',', header = TRUE,\n                   na.strings = c(\"NA\", \"x\"))\nunique(dat2[ , 2])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  [1] 2124 1830 1833 1600 1578 1187 1005  918  865  871  860  883  897  898  893\n [16]  913  870  962  880  875  884  894  836  848  885  851  900  861  866  867\n [31]  829  853  920  877  908  855  845  859  856  825  828  854  847  840  873\n [46]  822  818  838  815  813  816  849  802  805  792  823  808  798  800  842\n [61]  809  807  826  810  801  794  771  796  790  787  775  751  783  811  768\n [76]  779  795  770  821  830  767  772  791  781  773  777  814  778  782  837\n [91]  759  846  797  835  832  793  803  834  785  831  820  812  824  728  760\n[106]  762  753  758  764  741  709  735  749  752  761  750  776  766  789  763\n[121]  864  858  869  886  844  863  916  890  872  907  926  935  933  906  905\n[136]  912  972  996 1009  961  952  981  917 1011 1071 1920 3245 3805 3926 3284\n[151] 2700 2347 2078 2935 3040 1860 1437 1512 1720 1493 1026  928  874  833  850\n[166]   NA\n```\n:::\n\n```{.r .cell-code}\n## Let's check that the empty strings from 'dat' are now NAs in 'dat2'\nwhich(dat[ , 2] == \"\")[1:10]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] 312 313 314 315 317 318 319 320 322 323\n```\n:::\n\n```{.r .cell-code}\ndat2[which(dat[, 2] == \"\")[1], ] # pull out a line with a missing string\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    X2010.08.02.18.55 X2336 X549 X2086 X666 X481 X298 X1624 X1732 X593 X222\n312  2010-08-03 10:31    NA   NA    NA   NA   NA   NA    NA    NA   NA   NA\n    X911 X261 X1730 X211 X365 X216 X438 X596 X206 X204 X270 X176 X1159 X1137\n312   NA   NA    NA   NA   NA   NA   NA   NA   NA   NA   NA   NA    NA    NA\n    X135 X2036 X138 X1038 X201 X610 X627 X195 X976 X151 X1830 X421 X1087 X1157\n312   NA    NA   NA    NA   NA   NA   NA   NA   NA   NA    NA   NA    NA    NA\n    X181 X267 X193 X391 X208 X614 X546 X186 X1391 X217 X230 X625 X376 X164 X329\n312   NA   NA   NA   NA   NA   NA   NA   NA    NA   NA   NA   NA   NA   NA   NA\n    X1043 X497 X440 X197 X287 X837 X226 X973\n312    NA   NA   NA   NA   NA   NA   NA   NA\n```\n:::\n:::\n\n\n\nUsing `colClasses` is a good way to control how data are read in.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsequ <- read.table(file.path('..', 'data', 'hivSequ.csv'),\n  sep = ',', header = TRUE,\n  colClasses = c('integer','integer','character',\n    'character','numeric','integer'))\n## let's make sure the coercion worked - sometimes R is obstinant\nsapply(sequ, class)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  PatientID        Resp      PR.Seq      RT.Seq       VL.t0      CD4.t0 \n  \"integer\"   \"integer\" \"character\" \"character\"   \"numeric\"   \"integer\" \n```\n:::\n\n```{.r .cell-code}\n## that made use of the fact that a data frame is a list\n```\n:::\n\n\n\nNote that you can avoid reading in one or more columns by specifying\n*NULL* as the column class for those columns to be omitted. Also,\nspecifying the *colClasses* argument explicitly should make for faster\nfile reading. Finally, setting `stringsAsFactors=FALSE` is standard\npractice and is the default in R as of version 4.0. (*readr::read_csv()*\nhas always set `stringsAsFactors=FALSE`).\n\nIf possible, it's a good idea to look through the input file in the\nshell or in an editor before reading into R to catch such issues in\nadvance. Using the UNIX command *less* on *RTADataSub.csv* would have revealed these\nvarious issues, but note that *RTADataSub.csv* is a 1000-line subset of\na much larger file of data available from the kaggle.com website. So\nmore sophisticated use of UNIX utilities (as we will see in Unit 3) is often\nuseful before trying to read something into a program.\n\nThe basic function *scan()* simply reads everything in, ignoring lines,\nwhich works well and very quickly if you are reading in a numeric vector\nor matrix. *scan()* is also useful if your file is free format - i.e.,\nif it's not one line per observation, but just all the data one value\nafter another; in this case you can use *scan()* to read it in and then\nformat the resulting character or numeric vector as a matrix with as\nmany columns as fields in the dataset. Remember that the default is to\nfill the matrix by column.\n\nIf the file is not nicely arranged by field (e.g., if it has ragged\nlines), we'll need to do some more work. *readLines()* will read in each\nline into a separate character vector, after which we can process the\nlines using text manipulation. Here's an example from some US\nmeteorological data where I know from metadata (not provided here) that\nthe 4-11th values are an identifier, the 17-20th are the year, the\n22-23rd the month, etc.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat <- readLines(file.path('..', 'data', 'precip.txt'))\nid <- as.factor(substring(dat, 4, 11) )\nyear <- substring(dat, 18, 21)\nyear[1:5]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"2010\" \"2010\" \"2010\" \"2010\" \"2010\"\n```\n:::\n\n```{.r .cell-code}\nclass(year)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"character\"\n```\n:::\n\n```{.r .cell-code}\nyear <- as.integer(substring(dat, 18, 21))\nmonth <- as.integer(substring(dat, 22, 23))\nnvalues <- as.integer(substring(dat, 28, 30))\n```\n:::\n\n\n\n\nActually, that file, *precip.txt*, is in a fixed-width format (i.e.,\nevery element in a given column has the exact same number of\ncharacters),so reading in using *read.fwf()* would be a good strategy.\n\n## Connections\n\nR allows you to read in not just from a file but from a more general\nconstruct called a *connection*. This can include reading in text from the output of running a shell command and from unzipping a file on the fly.\n\nHere are some examples of connections:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat <- readLines(pipe(\"ls -al\"))\ndat <- read.table(pipe(\"unzip dat.zip\"))\ndat <- read.csv(gzfile(\"dat.csv.gz\"))\ndat <- readLines(\"http://www.stat.berkeley.edu/~paciorek/index.html\")\n```\n:::\n\n\n\n\nIn some cases, you might need to create the connection using *url()* or\nusing the *curl()* function from the *curl* package. Though for the\nexample here, simply passing the URL to *readLines()* does work. (In\ngeneral, *curl::curl()* provides some nice features for reading off the\ninternet.)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwikip1 <- readLines(\"https://wikipedia.org\")\nwikip2 <- readLines(url(\"https://wikipedia.org\"))\nlibrary(curl)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nUsing libcurl 7.68.0 with GnuTLS/3.6.13\n```\n:::\n\n```{.r .cell-code}\nwikip3 <- readLines(curl(\"https://wikipedia.org\"))\n```\n:::\n\n\n\nIf a file is large, we may want to read it in in chunks (of lines), do\nsome computations to reduce the size of things, and iterate. This is referred\nto as online processing.\n*read.table()*, *read.fwf()* and *readLines()* all have the arguments\nthat let you read in a fixed number of lines. To read-on-the-fly in\nblocks, we need to first establish the connection and then read from it\nsequentially. (If you don't, you'll read from the start of the file\nevery time you read from the file.)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncon <- file(file.path(\"..\", \"data\", \"precip.txt\"), \"r\")\n## \"r\" for 'read' - you can also open files for writing with \"w\"\n## (or \"a\" for appending)\nclass(con)\nblockSize <- 1000 # obviously this would be large in any real application\nnLines <- 300000\nfor(i in 1:ceiling(nLines / blockSize)){\n    lines <- readLines(con, n = blockSize)\n    # manipulate the lines and store the key stuff\n}\nclose(con)\n```\n:::\n\n\n \n\n\nHere's an example of using *curl()* to do this for a file on the web.\n\n\n\n::: {.cell hash='unit2-dataTech_cache/pdf/stream-curl_4a03b92515f73c5208a081745868f680'}\n\n```{.r .cell-code}\nURL <- \"https://www.stat.berkeley.edu/share/paciorek/2008.csv.gz\"\ncon <- gzcon(curl(URL, open = \"r\"))\n## url() in place of curl() works too\nfor(i in 1:4) {   ## Read in first four chunks as an example\n\tprint(i)\n\tprint(system.time(tmp <- readLines(con, n = 100000)))\n\tprint(tmp[1])\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1\n   user  system elapsed \n  0.449   0.000   0.450 \n[1] \"Year,Month,DayofMonth,DayOfWeek,DepTime,CRSDepTime,ArrTime,CRSArrTime,UniqueCarrier,FlightNum,TailNum,ActualElapsedTime,CRSElapsedTime,AirTime,ArrDelay,DepDelay,Origin,Dest,Distance,TaxiIn,TaxiOut,Cancelled,CancellationCode,Diverted,CarrierDelay,WeatherDelay,NASDelay,SecurityDelay,LateAircraftDelay\"\n[1] 2\n   user  system elapsed \n  0.436   0.000   0.437 \n[1] \"2008,1,29,2,1938,1935,2308,2257,XE,7676,N11176,150,142,104,11,3,SLC,OKC,866,5,41,0,,0,NA,NA,NA,NA,NA\"\n[1] 3\n   user  system elapsed \n  0.439   0.000   0.439 \n[1] \"2008,1,20,7,1540,1525,1651,1637,OO,5703,N227SW,71,72,58,14,15,SBA,SJC,234,5,8,0,,0,NA,NA,NA,NA,NA\"\n[1] 4\n   user  system elapsed \n  0.424   0.000   0.424 \n[1] \"2008,1,2,3,1313,1250,1443,1425,WN,440,N461WN,150,155,138,18,23,MCO,STL,880,3,9,0,,0,2,0,0,0,16\"\n```\n:::\n\n```{.r .cell-code}\nclose(con)\n```\n:::\n\n\n\nMore details on sequential (on-line) processing of large files can be\nfound in the tutorial on large datasets mentioned in the reference list\nabove.\n\nOne cool trick that can come in handy is to create a *text connection*.\nThis lets you 'read' from an R character vector as if it were a text\nfile and could be handy for processing text. For example, you could then\nuse *read.fwf()* applied to *con*.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat <- readLines('../data/precip.txt')\ncon <- textConnection(dat[1], \"r\")\nread.fwf(con, c(3,8,4,2,4,2))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   V1      V2   V3 V4   V5 V6\n1 DLY 1000807 PRCP HI 2010  2\n```\n:::\n:::\n\n\n\n\n\nWe can create connections for writing output too. Just make sure to open\nthe connection first.\n\n## File paths\n\nA few notes on file paths, related to ideas of reproducibility.\n\n1.  In general, you don't want to hard-code absolute paths into your\n    code files because those absolute paths won't be available on the\n    machines of anyone you share the code with. Instead, use paths\n    relative to the directory the code file is in, or relative to a\n    baseline directory for the project, e.g.:\\\n    \n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    dat <- read.csv('../data/cpds.csv')\n    ```\n    :::\n\n\n\n2.  Be careful with the directory separator in Windows files: you can\n    either do `C:\\\\mydir\\\\file.txt` or `C:/mydir/file.txt`, but\n    not `C:\\mydir\\file.txt`, and note the next comment about\n    avoiding use of '\\\\' for portability.\n\n3.  Using UNIX style directory separators will work in Windows, Mac or\n    Linux, but using Windows style separators is not portable across\n    operating systems.\\\n\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    ## good: will work on Windows\n    dat <- read.csv('../data/cpds.csv')\n    ## bad: won't work on Mac or Linux\n    dat <- read.csv('..\\\\data\\\\cpds.csv')  \n    ```\n    :::\n\n\n\n4.  Even better, use *file.path()* so that paths are constructed\n    specifically for the operating system the user is using:\\\n\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    ## good: operating-system independent\n    dat <- read.csv(file.path('..', 'data', 'cpds.csv'))  \n    ```\n    :::\n\n\n\n## The *readr* package\n\n*readr* is intended to deal with some of the shortcomings of the base R\nfunctions, such as leaving column names unmodified, and recognizing\ndates/times. It reads data in much more quickly than the base R\nequivalents. See [this blog\npost](http://blog.rstudio.org/2015/04/09/readr-0-1-0/). Some of the\nreadr functions that are analogs to the comparably-named base R\nfunctions are *read_csv()*, *read_fwf()*, *read_lines()*, and\n*read_table()*.\n\nLet's try out *read_csv()* on the airline dataset used in the R\nbootcamp.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(readr)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'readr'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:curl':\n\n    parse_date\n```\n:::\n\n```{.r .cell-code}\n## I'm violating the rule about absolute paths here!!\n## (airline.csv is big enough that I don't want to put it in the\n##    course repository)\ndir <- \"../data\"\nsystem.time(dat <- read.csv(file.path(dir, 'airline.csv'), stringsAsFactors = FALSE)) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   user  system elapsed \n  3.134   0.140   3.314 \n```\n:::\n\n```{.r .cell-code}\nsystem.time(dat2 <- read_csv(file.path(dir, 'airline.csv')))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nRows: 539895 Columns: 29\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n-- Column specification --------------------------------------------------------\nDelimiter: \",\"\nchr  (5): UniqueCarrier, TailNum, Origin, Dest, CancellationCode\ndbl (24): Year, Month, DayOfMonth, DayOfWeek, DepTime, CRSDepTime, ArrTime, ...\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n   user  system elapsed \n  4.195   0.171   1.856 \n```\n:::\n:::\n\n\n\n\n\n## Reading data quickly\n\nIn addition to the tips above, there are a number of packages that allow\none to read large data files quickly, in particular *data.table*, *arrow*,\nand *fst*. In general, these provide the ability to load datasets\ninto R without having them in memory, but rather stored in clever ways\non disk that allow for fast access. Metadata is stored in R. More on\nthis in the unit on big data and in the tutorial on large datasets\nmentioned in the reference list above.\n\n# 3. Output from R\n\n## Writing output to files\n\nFunctions for text output are generally analogous to those for input.\n*write.table()*, *write.csv()*, and *writeLines()* are analogs of\n*read.table()*, *read.csv()*, and *readLines()*. *write_csv()* is the\n*readr* version of write.csv. *write()* can be used to write a matrix to\na file, specifying the number of columns desired. *cat()* can be used\nwhen you want fine control of the format of what is written out and\nallows for outputting to a connection (e.g., a file).\n\n*toJSON()* in the *jsonlite* package will output R objects as JSON. One\nuse of JSON as output from R would be to *serialize* the information in\nan R object such that it could be read into another program.\n\nAnd of course you can always save to an R data file (a binary file format) using *save.image()*\n(to save all the objects in the workspace or *save()* to save only some\nobjects. Happily this is platform-independent so can be used to transfer\nR objects between different OS.\n\n## Formatting output\n\n*cat()* is a good choice for printing a message to the screen, often\nbetter than *print()*, which is an object-oriented method. You generally\nwon't have control over how the output of a *print()* statement is\nactually printed.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nval <- 1.5\ncat('My value is ', val, '.\\n', sep = '')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMy value is 1.5.\n```\n:::\n\n```{.r .cell-code}\nprint(paste('My value is ', val, '.', sep = ''))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"My value is 1.5.\"\n```\n:::\n:::\n\n\n\n\nWe can do more to control formatting with *cat()*:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## input\nx <- 7\nn <- 5\n## display powers\ncat(\"Powers of\", x, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPowers of 7 \n```\n:::\n\n```{.r .cell-code}\ncat(\"exponent   result\\n\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nexponent   result\n```\n:::\n\n```{.r .cell-code}\nresult <- 1\nfor (i in 1:n) {\n    result <- result * x\n    cat(format(i, width = 8), format(result, width = 10),\n            \"\\n\", sep = \"\")\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       1         7\n       2        49\n       3       343\n       4      2401\n       5     16807\n```\n:::\n:::\n\n\n\n\nOne thing to be aware of when writing out numerical data is how many\ndigits are included. For example, the default with `write` and\n`cat` is the number of digits that R displays to the screen,\ncontrolled by `options()$digits`. But note that `options()$digits` seems\nto have some variability in behavior across operating systems. If you\nwant finer control, use `sprintf`. For example, here we print out\ntemperatures as reals (\"f\"=floating point) with four decimal places\nand nine total character positions, followed by a C for Celsius:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntemps <- c(12.5, 37.234324, 1342434324.79997234, 2.3456e-6, 1e10)\nsprintf(\"%9.4f C\", temps)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"  12.5000 C\"        \"  37.2343 C\"        \"1342434324.8000 C\" \n[4] \"   0.0000 C\"        \"10000000000.0000 C\"\n```\n:::\n\n```{.r .cell-code}\ncity <- \"Boston\"\nsprintf(\"The temperature in %s was %.4f C.\", city, temps[1])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"The temperature in Boston was 12.5000 C.\"\n```\n:::\n\n```{.r .cell-code}\nsprintf(\"The temperature in %s was %9.4f C.\", city, temps[1])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"The temperature in Boston was   12.5000 C.\"\n```\n:::\n:::\n\n\n\nTo change the number of digits printed to the screen,\ndo `options(digits = 5)` or specify as an argument to `print` or\nuse `sprintf`.\n\n# 4. Webscraping and working with HTML, XML, and JSON\n\nThe book *XML and Web Technologies for Data Sciences with R* by Deb\nNolan (UCB Stats faculty) and Duncan Temple Lang (UCB Stats PhD alumnus\nand UC Davis Stats faculty) provides extensive information about getting\nand processing data off of the web, including interacting with web\nservices such as REST and SOAP and programmatically handling\nauthentication.\n\nHere are some UNIX command-line tools to help in webscraping and working\nwith files in formats such as JSON, XML, and HTML:\n<http://jeroenjanssens.com/2013/09/19/seven-command-line-tools-for-data-science.html>.\n\nWe'll cover a few basic examples in this section, but HTML and XML\nformatting and navigating the structure of such pages in great detail is\nbeyond the scope of what we can cover. The key thing is to see the main\nconcepts and know that the tools exist so that you can learn how to use\nthem if faced with such formats.\n\n## Reading HTML\n\nHTML (Hypertext Markup Language) is the standard markup language used\nfor displaying content in a web browser. In simple webpages (ignoring\nthe more complicated pages that involve Javascript), what you see in\nyour browser is simply a *rendering* (by the browser) of a text file containing HTML.\n\nHowever, instead of rendering the HTML in a browser, we might want to\nuse code to extract information from the HTML.\n\nLet's see a brief example of reading in HTML tables.\n\nNote that before doing any coding, it can be helpful to look at the raw\nHTML source code for a given page. We can explore the underlying HTML\nsource in advance of writing our code by looking at the page source\ndirectly in the browser (e.g., in Firefox under the 3-lines \"open menu\"\nsymbol, see `Web Developer (or More Tools) -> Page Source` and in Chrome\n`View -> Developer -> View Source`), or by downloading the webpage and\nlooking at it in an editor, although in some cases (such as the\nnytimes.com case), what we might see is a lot of JavaScript.\n\nOne lesson here is not to write a lot of your own code to do something\nthat someone else has probably already written a package for. We'll use\nthe *rvest* package.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(rvest, quietly = TRUE)  # uses xml2\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'rvest'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:readr':\n\n    guess_encoding\n```\n:::\n\n```{.r .cell-code}\nURL <- \"https://en.wikipedia.org/wiki/List_of_countries_and_dependencies_by_population\"\nhtml <- read_html(URL)\ntbls <- html_table(html_elements(html, \"table\"))\nsapply(tbls, nrow)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 242  12\n```\n:::\n\n```{.r .cell-code}\npop <- tbls[[1]]\nhead(pop)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 x 9\n  Rank  `Country / Dependency` Conti~1 Popul~2 Perce~3 Date  Sourc~4 Notes ``   \n  <chr> <chr>                  <chr>   <chr>   <chr>   <chr> <chr>   <chr> <lgl>\n1 –     World                  All     7,976,~ 100%    6 Se~ UN pro~ \"\"    NA   \n2 1     China                  Asia    1,412,~ 17.7%   31 D~ Offici~ \"The~ NA   \n3 2     India                  Asia    1,375,~ 17.2%   1 Ma~ Offici~ \"The~ NA   \n4 3     United States          North ~ 333,06~ 4.18%   6 Se~ Nation~ \"The~ NA   \n5 4     Indonesia              Asia[b] 275,77~ 3.46%   1 Ju~ Offici~ \"\"    NA   \n6 5     Pakistan               Asia    235,82~ 2.96%   1 Ju~ UN pro~ \"The~ NA   \n# ... with abbreviated variable names 1: Continent, 2: Population,\n#   3: `Percentage of the world`,\n#   4: `Source (official or from the United Nations)`\n```\n:::\n:::\n\n\n\n(Caution here -- notice what format the columns are stored in...)\n\n*read_html()* works by reading in the HTML as text and then parsing it\nto build up a tree containing the HTML elements. Then *html_elements()*\nfinds the HTML tables and *html_table()* converts them to data frames.\nrvest is part of the tidyverse, so it's often used with piping, e.g.,\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Turns out that html_table can take the entire html doc as input\ntbls <- URL |> read_html() |> html_table()\n```\n:::\n\n\n\nIt's often useful to be able to extract the hyperlinks in an HTML\ndocument. We'll find the link using [CSS\nselectors](https://www.w3schools.com/cssref/css_selectors.asp), which\nallow you to search for elements within HTML:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nURL <- \"http://www1.ncdc.noaa.gov/pub/data/ghcn/daily/by_year\"\n## approach 1: search for elements with 'href' attribute\nlinks <- read_html(URL) %>% html_elements(\"[href]\") %>% html_attr('href')\n## approach 2: search for HTML 'a' tags\nlinks <- read_html(URL) %>% html_elements(\"a\") %>% html_attr('href')\nhead(links, n = 10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] \"?C=N;O=D\"              \"?C=M;O=A\"              \"?C=S;O=A\"             \n [4] \"?C=D;O=A\"              \"/pub/data/ghcn/daily/\" \"1750.csv.gz\"          \n [7] \"1763.csv.gz\"           \"1764.csv.gz\"           \"1765.csv.gz\"          \n[10] \"1766.csv.gz\"          \n```\n:::\n:::\n\n\n\nMore generally, we may want to read an HTML document, parse it into its\ncomponents (i.e., the HTML elements), and navigate through the tree\nstructure of the HTML. Here we use the *XPath* language to specify\nelements rather than CSS selectors. XPath can also be used for\nnavigating through XML documents.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## find all 'a' elements that have attribute 'href'; then\n## extract the 'href' attribute\nlinks <- read_html(URL) %>% html_elements(xpath = \"//a[@href]\") %>%\n    html_attr('href')\nhead(links)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"?C=N;O=D\"              \"?C=M;O=A\"              \"?C=S;O=A\"             \n[4] \"?C=D;O=A\"              \"/pub/data/ghcn/daily/\" \"1750.csv.gz\"          \n```\n:::\n\n```{.r .cell-code}\n## we can extract various information\nlistOfANodes <- read_html(URL) %>% html_elements(xpath = \"//a[@href]\")\nlistOfANodes %>% html_attr('href') %>% head(n = 10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] \"?C=N;O=D\"              \"?C=M;O=A\"              \"?C=S;O=A\"             \n [4] \"?C=D;O=A\"              \"/pub/data/ghcn/daily/\" \"1750.csv.gz\"          \n [7] \"1763.csv.gz\"           \"1764.csv.gz\"           \"1765.csv.gz\"          \n[10] \"1766.csv.gz\"          \n```\n:::\n\n```{.r .cell-code}\nlistOfANodes %>% html_name() %>% head(n = 10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] \"a\" \"a\" \"a\" \"a\" \"a\" \"a\" \"a\" \"a\" \"a\" \"a\"\n```\n:::\n\n```{.r .cell-code}\nlistOfANodes %>% html_text()  %>% head(n = 10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] \"Name\"             \"Last modified\"    \"Size\"             \"Description\"     \n [5] \"Parent Directory\" \"1750.csv.gz\"      \"1763.csv.gz\"      \"1764.csv.gz\"     \n [9] \"1765.csv.gz\"      \"1766.csv.gz\"     \n```\n:::\n:::\n\n\n\nHere's another example of extracting specific components of information\nfrom a webpage (results not shown, since headlines will vary from day to\nday).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nURL <- \"https://www.nytimes.com\"\nheadlines2 <- read_html(URL) %>% html_elements(\"h2\") %>% html_text()\nhead(headlines2)\nheadlines3 <- read_html(URL) %>% html_elements(\"h3\") %>% html_text()\nhead(headlines3)\n```\n:::\n\n\n\n## XML\n\nXML is a markup language used to store data in self-describing (no\nmetadata needed) format, often with a hierarchical structure. It\nconsists of sets of elements (also known as nodes because they generally\noccur in a hierarchical structure and therefore have parents, children,\netc.) with tags that identify/name the elements, with some similarity to\nHTML. Some examples of the use of XML include serving as the underlying\nformat for Microsoft Office and Google Docs documents and for the KML\nlanguage used for spatial information in Google Earth.\n\nHere's a brief example. The book with id attribute *bk101* is an\nelement; the author of the book is also an element that is a child\nelement of the book. The id attribute allows us to uniquely identify the\nelement.\n\n```\n    <?xml version=\"1.0\"?>\n    <catalog>\n       <book id=\"bk101\">\n          <author>Gambardella, Matthew</author>\n          <title>XML Developer's Guide</title>\n          <genre>Computer</genre>\n          <price>44.95</price>\n          <publish_date>2000-10-01</publish_date>\n          <description>An in-depth look at creating applications with XML.</description>\n       </book>\n       <book id=\"bk102\">\n          <author>Ralls, Kim</author>\n          <title>Midnight Rain</title>\n          <genre>Fantasy</genre>\n          <price>5.95</price>\n          <publish_date>2000-12-16</publish_date>\n         <description>A former architect battles corporate zombies, an evil sorceress, and her own childhood to become queen of the world.</description>\n       </book>\n    </catalog>\n```\n\nWe can read XML documents into R using `xml2::read_xml()` and then\nmanipulate it using other functions from the *xml2* package. Here's an\nexample of working with lending data from the Kiva lending non-profit.\nYou can see the XML format in a browser at\n\n<http://api.kivaws.org/v1/loans/newest.xml>.\n\nXML documents have a tree structure with information at nodes. As above\nwith HTML, one can use the *XPath* language for navigating the tree and\nfinding and extracting information from the node(s) of interest.\n\nHere is some example code for extracting loan info from the Kiva data.\nWe'll first show the 'brute force' approach of working with the data as a\nlist and then the better approach of using XPath.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(xml2)\ndoc <- read_xml(\"https://api.kivaws.org/v1/loans/newest.xml\")\ndata <- as_list(doc)\nnames(data)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"response\"\n```\n:::\n\n```{.r .cell-code}\nnames(data$response)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"paging\" \"loans\" \n```\n:::\n\n```{.r .cell-code}\nlength(data$response$loans)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 20\n```\n:::\n\n```{.r .cell-code}\ndata$response$loans[[2]][c('name', 'activity',\n                           'sector', 'location', 'loan_amount')]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$name\n$name[[1]]\n[1] \"Piseth\"\n\n\n$activity\n$activity[[1]]\n[1] \"Home Energy\"\n\n\n$sector\n$sector[[1]]\n[1] \"Personal Use\"\n\n\n$location\n$location$country_code\n$location$country_code[[1]]\n[1] \"KH\"\n\n\n$location$country\n$location$country[[1]]\n[1] \"Cambodia\"\n\n\n$location$town\n$location$town[[1]]\n[1] \"Siem Reap\"\n\n\n$location$geo\n$location$geo$level\n$location$geo$level[[1]]\n[1] \"town\"\n\n\n$location$geo$pairs\n$location$geo$pairs[[1]]\n[1] \"13.366667 103.85\"\n\n\n$location$geo$type\n$location$geo$type[[1]]\n[1] \"point\"\n\n\n\n\n$loan_amount\n$loan_amount[[1]]\n[1] \"800\"\n```\n:::\n\n```{.r .cell-code}\n## alternatively, extract only the 'loans' info (and use pipes)\nloansNode <- doc %>% html_elements('loans')\nloanInfo <- loansNode %>% xml_children() %>% as_list()\nlength(loanInfo)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 20\n```\n:::\n\n```{.r .cell-code}\nnames(loanInfo[[1]])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] \"id\"                       \"name\"                    \n [3] \"description\"              \"status\"                  \n [5] \"funded_amount\"            \"basket_amount\"           \n [7] \"image\"                    \"activity\"                \n [9] \"sector\"                   \"themes\"                  \n[11] \"use\"                      \"location\"                \n[13] \"partner_id\"               \"posted_date\"             \n[15] \"planned_expiration_date\"  \"loan_amount\"             \n[17] \"borrower_count\"           \"lender_count\"            \n[19] \"bonus_credit_eligibility\" \"tags\"                    \n```\n:::\n\n```{.r .cell-code}\nnames(loanInfo[[1]]$location)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"country_code\" \"country\"      \"town\"         \"geo\"         \n```\n:::\n\n```{.r .cell-code}\n## suppose we only want the country locations of the loans (using XPath)\nxml_find_all(loansNode, '//location//country')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n{xml_nodeset (20)}\n [1] <country>Cambodia</country>\n [2] <country>Cambodia</country>\n [3] <country>Cambodia</country>\n [4] <country>Cambodia</country>\n [5] <country>Ecuador</country>\n [6] <country>Cambodia</country>\n [7] <country>Vietnam</country>\n [8] <country>Vietnam</country>\n [9] <country>Guatemala</country>\n[10] <country>Timor-Leste</country>\n[11] <country>El Salvador</country>\n[12] <country>Colombia</country>\n[13] <country>Colombia</country>\n[14] <country>Kenya</country>\n[15] <country>Kenya</country>\n[16] <country>Kenya</country>\n[17] <country>Kenya</country>\n[18] <country>Colombia</country>\n[19] <country>Colombia</country>\n[20] <country>Honduras</country>\n```\n:::\n\n```{.r .cell-code}\nxml_find_all(loansNode, '//location//country') %>% xml_text()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] \"Cambodia\"    \"Cambodia\"    \"Cambodia\"    \"Cambodia\"    \"Ecuador\"    \n [6] \"Cambodia\"    \"Vietnam\"     \"Vietnam\"     \"Guatemala\"   \"Timor-Leste\"\n[11] \"El Salvador\" \"Colombia\"    \"Colombia\"    \"Kenya\"       \"Kenya\"      \n[16] \"Kenya\"       \"Kenya\"       \"Colombia\"    \"Colombia\"    \"Honduras\"   \n```\n:::\n\n```{.r .cell-code}\n## or extract the geographic coordinates\nxml_find_all(loansNode, '//location//geo/pairs')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n{xml_nodeset (20)}\n [1] <pairs>13.09573 103.202206</pairs>\n [2] <pairs>13.366667 103.85</pairs>\n [3] <pairs>13.366667 103.85</pairs>\n [4] <pairs>13.366667 103.85</pairs>\n [5] <pairs>0.339176 -78.122234</pairs>\n [6] <pairs>12.5 104</pairs>\n [7] <pairs>21.40639 103.032155</pairs>\n [8] <pairs>21.40639 103.032155</pairs>\n [9] <pairs>14.421598 -91.404825</pairs>\n[10] <pairs>-8.930576 125.396797</pairs>\n[11] <pairs>13.833333 -88.916667</pairs>\n[12] <pairs>4 -72</pairs>\n[13] <pairs>5.472709 -74.667984</pairs>\n[14] <pairs>0.516667 35.283333</pairs>\n[15] <pairs>-0.785561 35.33914</pairs>\n[16] <pairs>0.035164 36.364292</pairs>\n[17] <pairs>-0.777114 34.945839</pairs>\n[18] <pairs>11.004107 -74.806981</pairs>\n[19] <pairs>10.391049 -75.479426</pairs>\n[20] <pairs>15.047154 -85.125776</pairs>\n```\n:::\n:::\n\n\n\n## JSON\n\nJSON files are structured as \"attribute-value\" pairs (aka \"key-value\"\npairs), often with a hierarchical structure. Here's a brief example:\n\n```\n    {\n      \"firstName\": \"John\",\n      \"lastName\": \"Smith\",\n      \"isAlive\": true,\n      \"age\": 25,\n      \"address\": {\n        \"streetAddress\": \"21 2nd Street\",\n        \"city\": \"New York\",\n        \"state\": \"NY\",\n        \"postalCode\": \"10021-3100\"\n      },\n      \"phoneNumbers\": [\n        {\n          \"type\": \"home\",\n          \"number\": \"212 555-1234\"\n        },\n        {\n          \"type\": \"office\",\n          \"number\": \"646 555-4567\"\n        }\n      ],\n      \"children\": [],\n      \"spouse\": null\n    }\n```\n\nA set of key-value pairs is a named array and is placed inside braces\n(squiggly brackets). Note the nestedness of arrays within arrays (e.g.,\naddress within the overarching person array and the use of square\nbrackets for unnamed arrays (i.e., vectors of information), as well as\nthe use of different types: character strings, numbers, null, and (not\nshown) boolean/logical values. JSON and XML can be used in similar ways,\nbut JSON is less *verbose* than XML.\n\nWe can read JSON into R using *fromJSON()* in the *jsonlite* package.\nLet's play again with the Kiva data. The same data that we had worked\nwith in XML format is also available in JSON format:\n<http://api.kivaws.org/v1/loans/newest.json>.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(jsonlite)\ndata <- fromJSON(\"http://api.kivaws.org/v1/loans/newest.json\")\nclass(data)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"list\"\n```\n:::\n\n```{.r .cell-code}\nnames(data)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"paging\" \"loans\" \n```\n:::\n\n```{.r .cell-code}\nclass(data$loans) # nice!\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"data.frame\"\n```\n:::\n\n```{.r .cell-code}\nhead(data$loans)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       id           name languages      status funded_amount basket_amount\n1 2426966            Hon        en fundraising            25            25\n2 2426991         Piseth        en fundraising             0             0\n3 2444685         Sareth        en fundraising             0             0\n4 2444689           Kheu        en fundraising             0             0\n5 2425164 Maria Consuelo    es, en fundraising             0             0\n6 2444680         Sophat        en fundraising             0             0\n  image.id image.template_id    activity       sector\n1  4946099                 1 Home Energy Personal Use\n2  4934240                 1 Home Energy Personal Use\n3  4947188                 1      Crafts         Arts\n4  4947194                 1      Crafts         Arts\n5  4914714                 1  Restaurant         Food\n6  4947060                 1     Farming  Agriculture\n                                  themes\n1 Social Enterprise, Clean Energy, Green\n2 Social Enterprise, Clean Energy, Green\n3                      Underfunded Areas\n4                      Underfunded Areas\n5                                   NULL\n6                                   NULL\n                                                                                            use\n1          to buy a biodigester that reduces kitchen smoke with biogas replacing the wood fuel.\n2          to buy a biodigester that reduces kitchen smoke with biogas replacing the wood fuel.\n3    to purchase more rattan and tropical fruit trees for making baskets to sell in the market.\n4    to purchase more rattan and tropical fruit trees for making baskets to sell in the market.\n5 to acquire cooking utensils and an industrial stove, as well as make renovations to her shop.\n6                             to buy rice seeds and fertilizer to support her farming business.\n  location.country_code location.country location.town location.geo.level\n1                    KH         Cambodia    Battambang               town\n2                    KH         Cambodia     Siem Reap               town\n3                    KH         Cambodia     Siem Reap               town\n4                    KH         Cambodia     Siem Reap               town\n5                    EC          Ecuador        Ibarra               town\n6                    KH         Cambodia        Pursat               town\n   location.geo.pairs location.geo.type partner_id          posted_date\n1 13.09573 103.202206             point        636 2022-09-07T15:40:51Z\n2    13.366667 103.85             point        636 2022-09-07T15:40:51Z\n3    13.366667 103.85             point        499 2022-09-07T15:40:51Z\n4    13.366667 103.85             point        499 2022-09-07T15:40:51Z\n5 0.339176 -78.122234             point        457 2022-09-07T15:40:50Z\n6            12.5 104             point        499 2022-09-07T15:40:50Z\n  planned_expiration_date loan_amount borrower_count lender_count\n1    2022-10-12T15:40:51Z         800              1            1\n2    2022-10-12T15:40:51Z         800              1            0\n3    2022-10-12T15:40:51Z         800              1            0\n4    2022-10-12T15:40:51Z         525              1            0\n5    2022-10-12T15:40:50Z        1600              1            0\n6    2022-10-12T15:40:50Z         500              1            0\n  bonus_credit_eligibility\n1                    FALSE\n2                    FALSE\n3                     TRUE\n4                     TRUE\n5                    FALSE\n6                     TRUE\n                                                                         tags\n1                                                            #Eco-friendly, 9\n2                                                                        NULL\n3                            #Woman-Owned Business, #Supporting Family, 6, 31\n4                            #Woman-Owned Business, #Supporting Family, 6, 31\n5 #Woman-Owned Business, #Biz Durable Asset, #Repair Renew Replace, 6, 35, 39\n6                          #Woman-Owned Business, #Vegan, #Elderly, 6, 10, 13\n```\n:::\n\n```{.r .cell-code}\ndata$loans[1, 'location.geo.pairs'] # hmmm...\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNULL\n```\n:::\n\n```{.r .cell-code}\ndata$loans[1, 'location']\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  country_code  country       town geo.level           geo.pairs geo.type\n1           KH Cambodia Battambang      town 13.09573 103.202206    point\n```\n:::\n:::\n\n\n\nOne disadvantage of JSON is that it is not set up to deal with missing\nvalues, infinity, etc.\n\n## Webscraping and web APIs\n\nHere we'll see some examples of making requests over the Web to get\ndata. We'll use APIs to systematically query a website for information.\nIdeally, but not always, the API will be documented. In many cases that\nsimply amounts to making an HTTP GET request, which is done by\nconstructing a URL.\n\nThe packages *RCurl* and *httr* are useful for a wide variety of such\nfunctionality. Note that much of the functionality I describe below is\nalso possible within the shell using either *wget* or *curl*.\n\n### Webscraping ethics and best practices\n\nWebscraping is the process of extracting data from the web, either\ndirectly from a website or using a web API (application programming\ninterface).\n\n1.  **Should you webscrape?** In general, if we can avoid webscraping\n    (particularly if there is not an API) and instead directly download\n    a data file from a website, that is greatly preferred.\n\n2.  **May you webscrape?** Before you set up any automated downloading\n    of materials/data from the web you should make sure that what you\n    are about to do is consistent with the rules provided by the\n    website.\n\nSome places to look for information on what the website allows are:\n\n-   legal pages such as Terms of Service or Terms and Conditions on the\n    website.\n\n-   check the robots.txt file (e.g.,\n    <https://scholar.google.com/robots.txt>) to see what a web crawler\n    is allowed to do, and whether the site requires a particular delay\n    between requests to the sites\n\n-   potentially contact the site owner if you plan to scrape a large\n    amount of data\n\nHere are some links with useful information:\n\n-   [A blog post overview on webscraping and\n    robots.txt](http://feedproxy.google.com/~r/RBloggers/~3/RZGJ8Trv5Xw/?utm_source=feedburner&utm_medium=email)\n\n-   [Blog post on webscraping\n    ethics](https://towardsdatascience.com/ethics-in-web-scraping-b96b18136f01)\n\n-   [Some information on how to understand a robots.txt\n    file](https://www.promptcloud.com/blog/how-to-read-and-respect-robots-file)\n\nTips for when you make automated requests:\n\n- When debugging code that processes the result of such a request, just run the request once, save (i.e., cache) the result, and then work on the processing code applied to the result. Don't make the same request over and over again.\n- In many cases you will want to include a time delay between your\nautomated requests to a site, including if you are not actually crawling\na site but just want to automate a small number of queries.\n\n### What is HTTP?\n\nHTTP (hypertext transfer protocol) is a system for communicating\ninformation from a server (i.e., the website of interest) to a client\n(e.g., your laptop). The client sends a request and the server sends a\nresponse.\n\nWhen you go to a website in a browser, your browser makes an HTTP GET\nrequest to the website. Similarly, when we did some downloading of html\nfrom webpages above, we used an HTTP GET request.\n\nAnytime the URL you enter includes parameter information after a question mark\n(`www.somewebsite.com?param1=arg1&param2=arg2`), you are using an API.\n\nThe response to an HTTP request will include a status code, which can be\ninterpreted based on [this\ninformation](https://developer.mozilla.org/en-US/docs/Web/HTTP/Status).\n\nThe response will generally contain content in the form of text (e.g.,\nHTML, XML, JSON) or raw bytes.\n\n### APIs: REST- and SOAP-based web services\n\nIdeally a web service documents their API (Applications Programming\nInterface) that serves data or allows other interactions. REST and SOAP\nare popular API standards/styles. Both REST and SOAP use HTTP requests;\nwe'll focus on REST as it is more common and simpler. When using REST, we access *resources*, which might be a Facebook\naccount or a database of stock quotes. The API will\n(hopefully) document what information it expects from the user and will\nreturn the result in a standard format (often a particular file format\nrather than producing a webpage).\n\nOften the format of the request is a URL (aka an endpoint) plus a query\nstring, passed as a GET request. Let's search for plumbers near\nBerkeley, and we'll see the GET request, in the form:\n\n<https://www.yelp.com/search?find_desc=plumbers&find_loc=Berkeley+CA&ns=1>\n\n-   the query string begins with ?\n\n-   there are one or more `Parameter=Argument` pairs\n\n-   pairs are separated by &\n\n-   \\+ is used in place of each space\n\n\nLet's see an example of accessing economic data from the\nWorld Bank, using the [documentation for their API](https://datahelpdesk.worldbank.org/knowledgebase/topics/125589-developer-information). Following the [API call structure](https://datahelpdesk.worldbank.org/knowledgebase/articles/898581-api-basic-call-structures), we can download (for example), data on various countries. The documentation indicates that our REST-based query can use either a URL structure or an argument-based structure.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Queries based on the documentation\napi_url <- \"http://api.worldbank.org/V2/incomeLevel/LIC/country\"\napi_args <- \"http://api.worldbank.org/V2/country?incomeLevel=LIC\"\n\n## Generalizing a bit\nreq <- \"http://api.worldbank.org/V2/country?incomeLevel=MIC&format=json\"\ndata <- fromJSON(req)\n## Be careful of data truncation/pagination\nreq <- \"http://api.worldbank.org/V2/country?incomeLevel=MIC&format=json&per_page=1000\"\ndata <- fromJSON(req)\n\n## Programmatic control\nbaseURL <- \"http://api.worldbank.org/V2/country\"\ngroup <- 'MIC'\nformat <- 'json'\nargs <- c(incomeLevel = group, format = format, per_page = 1000)\nurl <- paste0(baseURL, \"?\",\n    paste(  paste(names(args), args, sep = \"=\"),  collapse = \"&\"))\n   \ndata <- fromJSON(url)\nclass(data)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"list\"\n```\n:::\n\n```{.r .cell-code}\nlength(data)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2\n```\n:::\n\n```{.r .cell-code}\nnames(data)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNULL\n```\n:::\n\n```{.r .cell-code}\nhead(data[[2]])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   id iso2Code           name region.id region.iso2code\n1 AGO       AO         Angola       SSF              ZG\n2 ALB       AL        Albania       ECS              Z7\n3 ARG       AR      Argentina       LCN              ZJ\n4 ARM       AM        Armenia       ECS              Z7\n5 ASM       AS American Samoa       EAS              Z4\n6 AZE       AZ     Azerbaijan       ECS              Z7\n                region.value adminregion.id adminregion.iso2code\n1        Sub-Saharan Africa             SSA                   ZF\n2      Europe & Central Asia            ECA                   7E\n3 Latin America & Caribbean             LAC                   XJ\n4      Europe & Central Asia            ECA                   7E\n5        East Asia & Pacific            EAP                   4E\n6      Europe & Central Asia            ECA                   7E\n                                  adminregion.value incomeLevel.id\n1        Sub-Saharan Africa (excluding high income)            LMC\n2     Europe & Central Asia (excluding high income)            UMC\n3 Latin America & Caribbean (excluding high income)            UMC\n4     Europe & Central Asia (excluding high income)            UMC\n5       East Asia & Pacific (excluding high income)            UMC\n6     Europe & Central Asia (excluding high income)            UMC\n  incomeLevel.iso2code   incomeLevel.value lendingType.id lendingType.iso2code\n1                   XN Lower middle income            IBD                   XF\n2                   XT Upper middle income            IBD                   XF\n3                   XT Upper middle income            IBD                   XF\n4                   XT Upper middle income            IBD                   XF\n5                   XT Upper middle income            LNX                   XX\n6                   XT Upper middle income            IBD                   XF\n  lendingType.value  capitalCity longitude latitude\n1              IBRD       Luanda    13.242 -8.81155\n2              IBRD       Tirane   19.8172  41.3317\n3              IBRD Buenos Aires  -58.4173 -34.6118\n4              IBRD      Yerevan    44.509  40.1596\n5    Not classified    Pago Pago  -170.691 -14.2846\n6              IBRD         Baku   49.8932  40.3834\n```\n:::\n:::\n\n\n\nAPIs can change and disappear. Last year the example above involved the World Bank's Climate Data API, which I can no longer find!\n\n\nAs another example, here we can see the [US Treasury Department API](https://fiscaldata.treasury.gov/api-documentation/), which allows us to construct queries for federal financial data.\n\nThe Nolan and Temple Lang book provides a number of examples of\ndifferent ways of authenticating with web services that control access\nto the service.\n\nFinally, some web services allow us to pass information to the service\nin addition to just getting data or information. E.g., you can\nprogrammatically interact with your Facebook, Dropbox, and Google Drive\naccounts using REST based on HTTP POST, PUT, and DELETE requests.\nAuthentication is of course important in these contexts and some times\nyou would first authenticate with your login and password and receive a\n\"token\". This token would then be used in subsequent interactions in the\nsame session.\n\nI created your *github.berkeley.edu* accounts from Python by interacting\nwith the [GitHub API](https://docs.github.com/en/rest/reference/repos)\nusing the *requests* package.\n\n### HTTP requests by deconstructing an (undocumented) API\n\nIn some cases an API may not be documented or we might be lazy and not\nuse the documentation. Instead we might deconstruct the queries a\nbrowser makes and then mimic that behavior, in some cases having to\nparse HTML output to get at data. Note that if the webpage changes even\na little bit, our carefully constructed query syntax may fail.\n\nLet's look at some UN data (agricultural crop data). By going to\\\n<http://data.un.org/Explorer.aspx?d=FAO>, and clicking on \"Crops\", we'll\nsee a bunch of agricultural products with \"View data\" links. Click on\n\"apricots\" as an example and you'll see a \"Download\" button that allows\nyou to download a CSV of the data. Let's select a range of years and\nthen try to download \"by hand\". Sometimes we can right-click on the link\nthat will download the data and directly see the URL that is being\naccessed and then one can deconstruct it so that you can create URLs\nprogrammatically to download the data you want.\n\nIn this case, we can't see the full URL that is being used because\nthere's some Javascript involved. Therefore, rather than looking at the\nURL associated with a link we need to view the actual HTTP request sent\nby our browser to the server. We can do this using features of the\nbrowser (e.g., in Firefox see `Web Developer -> Network` and in Chrome\n`View -> Developer -> Developer tools` and choose the `Network` tab) (or right-click on the\nwebpage and select `Inspect` and then `Network`). Based on this we can\nsee that an HTTP GET request is being used with a URL such as:\\\n<http://data.un.org/Handlers/DownloadHandler.ashx?DataFilter=itemCode:526;year:2012,2013,2014,2015,2016,2017&DataMartId=FAO&Format=csv&c=2,4,5,6,7&s=countryName:asc,elementCode:asc,year:desc>.\n\nWe'e now able to easily download the data using that URL, which we can\nfairly easily construct using string processing in bash, R, or Python,\nsuch as this (here I just paste it together directly, but using more structured syntax\nsuch as I used for the World Bank example would be better):\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## example URL:\n## http://data.un.org/Handlers/DownloadHandler.ashx?DataFilter=itemCode:526;\n##year:2012,2013,2014,2015,2016,2017&DataMartId=FAO&Format=csv&c=2,4,5,6,7&\n##s=countryName:asc,elementCode:asc,year:desc\nitemCode <- 526\nbaseURL <- \"http://data.un.org/Handlers/DownloadHandler.ashx\"\nyrs <- paste(as.character(2012:2017), collapse = \",\")\nfilter <- paste0(\"?DataFilter=itemCode:\", itemCode, \";year:\", yrs)\nargs1 <- \"&DataMartId=FAO&Format=csv&c=2,3,4,5,6,7&\"\nargs2 <- \"s=countryName:asc,elementCode:asc,year:desc\"\nurl <- paste0(baseURL, filter, args1, args2)\n## if the website provided a CSV we could just do this:\n## apricots <- read.csv(url)\n## but it zips the file\ntemp <- tempfile()  ## give name for a temporary file\ndownload.file(url, temp)\ndat <- read.csv(unzip(temp))  ## using a connection (see Section 2)\n\nhead(dat)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  Country.or.Area Element.Code                                         Element\n1     Afghanistan          432 Gross Production Index Number (2014-2016 = 100)\n2     Afghanistan          432 Gross Production Index Number (2014-2016 = 100)\n3     Afghanistan          432 Gross Production Index Number (2014-2016 = 100)\n4     Afghanistan          432 Gross Production Index Number (2014-2016 = 100)\n5     Afghanistan          432 Gross Production Index Number (2014-2016 = 100)\n6     Afghanistan          432 Gross Production Index Number (2014-2016 = 100)\n  Year  Unit  Value Value.Footnotes\n1 2017 index 202.19              Fc\n2 2016 index  27.45              Fc\n3 2015 index 134.50              Fc\n4 2014 index 138.05              Fc\n5 2013 index 138.05              Fc\n6 2012 index 128.08              Fc\n```\n:::\n:::\n\n\n\nSo, what have we achieved?\n\n1. We have a reproducible workflow we can share with others (perhaps ourself in the future).\n\n2. We can automate the process of downloading many such files.\n\n\n### More details on HTTP requests\n\nA more sophisticated way to do the download is to pass the request in a structured way with named input parameters. This request is easier to construct programmatically. Here what is returned is a zip file, which is represented in R as a sequence of “raw” bytes. We can use httr's GET(), followed by writing to disk and reading back in, as follows (for some reason knitr won't print the output...):\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(httr)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'httr'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:curl':\n\n    handle_reset\n```\n:::\n\n```{.r .cell-code}\noutput2 <- GET(baseURL, query = list(\n               DataFilter = paste0(\"itemCode:\", itemCode, \";year:\", yrs),\n               DataMartID = \"FAO\", Format = \"csv\", c = \"2,3,4,5,6,7\",\n               s = \"countryName:asc,elementCode:asc,year:desc\"))\ntemp <- tempfile()  ## give name for a temporary file\nwriteBin(content(output2, 'raw'), temp)  ## write out as zip file\ndat <- read.csv(unzip(temp))\nhead(dat)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  Country.or.Area Element.Code                                         Element\n1     Afghanistan          432 Gross Production Index Number (2014-2016 = 100)\n2     Afghanistan          432 Gross Production Index Number (2014-2016 = 100)\n3     Afghanistan          432 Gross Production Index Number (2014-2016 = 100)\n4     Afghanistan          432 Gross Production Index Number (2014-2016 = 100)\n5     Afghanistan          432 Gross Production Index Number (2014-2016 = 100)\n6     Afghanistan          432 Gross Production Index Number (2014-2016 = 100)\n  Year  Unit  Value Value.Footnotes\n1 2017 index 202.19              Fc\n2 2016 index  27.45              Fc\n3 2015 index 134.50              Fc\n4 2014 index 138.05              Fc\n5 2013 index 138.05              Fc\n6 2012 index 128.08              Fc\n```\n:::\n:::\n\n\n\nIn some cases we may need to send a lot of information as part of the\nURL in a GET request. If it gets to be too long (e.g,, more than 2048\ncharacters) many web servers will reject the request. Instead we may\nneed to use an HTTP POST request (POST requests are often used for\nsubmitting web forms). A typical request would have syntax like this\nsearch (using *RCurl*):\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nif(url.exists('http://www.wormbase.org/db/searches/advanced/dumper')) {\n      x = postForm('http://www.wormbase.org/db/searches/advanced/dumper',\n              species=\"briggsae\",\n              list=\"\",\n              flank3=\"0\",\n              flank5=\"0\",\n              feature=\"Gene Models\",\n              dump = \"Plain TEXT\",\n              orientation = \"Relative to feature\",\n              relative = \"Chromsome\",\n              DNA =\"flanking sequences only\",\n              .cgifields = paste(c(\"feature\", \"orientation\", \"DNA\",\n                                   \"dump\",\"relative\"), collapse=\", \"))\n}\n```\n:::\n\n\n\nUnfortunately that specific search doesn't work because the server URL\nand/or API seem to have changed. But it gives you an idea of what the\nformat would look like.\n\n*httr* and *RCurl* can handle other kinds of HTTP requests such as PUT\nand DELETE. Finally, some websites use cookies to keep track of users\nand you may need to download a cookie in the first interaction with the\nHTTP server and then send that cookie with later interactions. More\ndetails are available in the Nolan and Temple Lang book.\n\n### Packaged access to an API\n\nFor popular websites/data sources, a developer may have packaged up the\nAPI calls in a user-friendly fashion for use from R, Python or other\nsoftware. For example there are Python (twitter) and R (twitteR)\npackages for interfacing with Twitter via its API.\n\nHere's some example code for Python (the Python package seems to be more\nfully-featured than the R package). This looks up the US senators'\nTwitter names and then downloads a portion of each of their timelines,\ni.e., the time series of their tweets. Note that Twitter has limits on\nhow much one can download at once.\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport json\nimport twitter\n\n# You will need to set the following variables with your\n# personal information.  To do this you will need to create\n# a personal account on Twitter (if you don't already have\n# one).  Once you've created an account, create a new\n# application here:\n#    https://dev.twitter.com/apps\n#\n# You can manage your applications here:\n#    https://apps.twitter.com/\n#\n# Select your application and then under the section labeled\n# \"Key and Access Tokens\", you will find the information needed\n# below.  Keep this information private.\nCONSUMER_KEY       = \"\"\nCONSUMER_SECRET    = \"\"\nOAUTH_TOKEN        = \"\"\nOAUTH_TOKEN_SECRET = \"\"\n\nauth = twitter.oauth.OAuth(OAUTH_TOKEN, OAUTH_TOKEN_SECRET,\n                           CONSUMER_KEY, CONSUMER_SECRET)\napi = twitter.Twitter(auth=auth)\n\n# get the list of senators\nsenators = api.lists.members(owner_screen_name=\"gov\", slug=\"us-senate\", count=100)\n\n# get all the senators' timelines\nnames = [d[\"screen_name\"] for d in senators[\"users\"]]\ntimelines = [api.statuses.user_timeline(screen_name=name, count = 500) \n             for name in names]\n\n# save information out to JSON\nwith open(\"senators-list.json\", \"w\") as f:\n    json.dump(senators, f, indent=4, sort_keys=True)\nwith open(\"timelines.json\", \"w\") as f:\n    json.dump(timelines, f, indent=4, sort_keys=True)\n```\n:::\n\n\n\n\n### Accessing dynamic pages\n\nSome websites dynamically change in reaction to the user behavior. In\nthese cases you need a tool that can mimic the behavior of a human\ninteracting with a site. Some options are:\n\n-   *selenium* (and the *RSelenium* wrapper for R) is a popular tool for\n    doing this.\n\n-   *splash* (and the *splashr* wrapper for R) is another approach.\n\n-   *htmlunit* is another tool for this.\n\n# 5. File and string encodings\n\nText (either in the form of a file with regular language in it or a data\nfile with fields of character strings) will often contain characters\nthat are not part of the [limited ASCII set of\ncharacters](http://en.wikipedia.org/wiki/ASCII), which has $2^{7}=128$\ncharacters and control codes; basically what you see on a standard US\nkeyboard. Each character takes up one byte (8 bits) of space (there is\nan unused bit that comes in handy in the UTF-8 context). We can actually\nhand-generate an ASCII file using the binary representation of each\ncharacter in R as an illustration.\n\nThe letter \"M\" is encoded based on the ASCII standard in bits as\n\"01001101\" as seen in the link above. For convenience, this is often\nwritten as two base-16 numbers (i.e., hexadecimal), where \"0100\"=\"4\" and\n\"1101\"=\"d\", hence we have \"4d\" in hexadecimal.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## 4d in hexadecimal is 'M'\n## 0a is a newline (at least in Linux/Mac)\n## \"0x\" is how we tell R we are using hexadecimal\nx <- as.raw(c('0x4d','0x6f', '0x6d','0x0a'))  ## i.e., \"Mom\\n\" in ascii\nx\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 4d 6f 6d 0a\n```\n:::\n\n```{.r .cell-code}\ncharToRaw('Mom\\n')      \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 4d 6f 6d 0a\n```\n:::\n\n```{.r .cell-code}\nwriteBin(x, 'tmp.txt')\nreadLines('tmp.txt')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Mom\"\n```\n:::\n\n```{.r .cell-code}\nsystem('ls -l tmp.txt', intern = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"-rw-r--r-- 1 paciorek scfstaff 4 Sep  7 08:50 tmp.txt\"\n```\n:::\n\n```{.r .cell-code}\nsystem('cat tmp.txt')\n```\n:::\n\n\n\n\nWhen encountering non-ASCII files, in some cases you may need to deal\nwith the text encoding (the mapping of individual characters (including\ntabs, returns, etc.) to a set of numeric codes). There are a variety of\ndifferent encodings for text files, with different ones common on\ndifferent operating systems.\n[UTF-8](https://en.wikipedia.org/wiki/UTF-8) is an encoding for the\nUnicode characters that includes more than 110,000 characters from 100\ndifferent alphabets/scripts. It's widely used on the web. Latin-1\nencodes a small subset of Unicode and contains the characters used in\nmany European languages (e.g., letters with accents). Here's an example\nof using a non-ASCII Unicode character:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## n-tilde and division symbol as Unicode 'code points'\nx2_unicode <- 'Pe\\u00f1a 3\\u00f72' \nEncoding(x2_unicode) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"UTF-8\"\n```\n:::\n\n```{.r .cell-code}\nx2_unicode\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Peña 3÷2\"\n```\n:::\n\n```{.r .cell-code}\ncharToRaw(x2_unicode)        \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] 50 65 c3 b1 61 20 33 c3 b7 32\n```\n:::\n\n```{.r .cell-code}\ncharToRaw('\\u00f1')    # indeed - two bytes, not one\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] c3 b1\n```\n:::\n\n```{.r .cell-code}\n## specified directly as hexadecimal in UTF-8 encoding \nx2_utf8 <- 'Pe\\xc3\\xb1a 3\\xc3\\xb72' \nx2_utf8\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Peña 3÷2\"\n```\n:::\n\n```{.r .cell-code}\nwriteBin(x2_unicode, 'tmp2.txt')\n## Here n-tilde and division symbol take up two bytes\n## but there is an extraneous null byte in there; not sure why.\nsystem('ls -l tmp2.txt') \n## The system knows how to interpret the UTF-8 encoded file\n## and represent the Unicode character on the screen:\nsystem('cat tmp2.txt')\n```\n:::\n\n\n\nUTF-8 is cleverly designed in terms of the bit-wise representation of\ncharacters such that ASCII characters still take up one byte, and most\nother characters take two bytes, but some take four bytes. In fact it is\neven more clever than that - the representation is such that the bits of\na one-byte character never appear within the representation of a two-\nor three- or four-byte character (and similarly for two-byte characters\nin three- or four-byte characters, etc.).\n\nThe UNIX utility *file*, e.g. `file tmp.txt` can help provide some\ninformation. *read.table()* in R takes arguments *fileEncoding* and\n*encoding* that allow one to specify the encoding as one reads text in.\nThe UNIX utility *iconv* and the R function *iconv()* can help with\nconversions.\n\nIn US installations of R, the default encoding is UTF-8; note below that\nvarious types of information are interpreted in US English with the\nencoding UTF-8:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nSys.getlocale()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"LC_CTYPE=en_US.UTF-8;LC_NUMERIC=C;LC_TIME=en_US.UTF-8;LC_COLLATE=en_US.UTF-8;LC_MONETARY=en_US.UTF-8;LC_MESSAGES=en_US.UTF-8;LC_PAPER=en_US.UTF-8;LC_NAME=C;LC_ADDRESS=C;LC_TELEPHONE=C;LC_MEASUREMENT=en_US.UTF-8;LC_IDENTIFICATION=C\"\n```\n:::\n:::\n\n\n\n\nWith strings already in R, you can convert between encodings with\n*iconv()*:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntext <- \"Melhore sua seguran\\xe7a\"\nEncoding(text)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"unknown\"\n```\n:::\n\n```{.r .cell-code}\nEncoding(text) <- \"latin1\"\ntext  ## this prints out correctly in R, but is not correct in the PDF\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Melhore sua segurança\"\n```\n:::\n\n```{.r .cell-code}\ntext <- \"Melhore sua seguran\\xe7a\"\ntextUTF8 <- iconv(text, from = \"latin1\", to = \"UTF-8\")\nEncoding(textUTF8)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"UTF-8\"\n```\n:::\n\n```{.r .cell-code}\ntextUTF8\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Melhore sua segurança\"\n```\n:::\n\n```{.r .cell-code}\niconv(text, from = \"latin1\", to = \"ASCII\", sub = \"???\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Melhore sua seguran???a\"\n```\n:::\n:::\n\n\n\n\nYou can also mark a string with an encoding, so R knows how to display\nit correctly (again, this prints out incorrectly in the PDF):\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- \"fa\\xE7ile\" \nEncoding(x) <- \"latin1\" \nx\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"façile\"\n```\n:::\n\n```{.r .cell-code}\n## playing around... \nx <- \"\\xa1 \\xa2 \\xa3 \\xf1 \\xf2\" \nEncoding(x) <- \"latin1\" \nx \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"¡ ¢ £ ñ ò\"\n```\n:::\n:::\n\n\n\n\nAn R error message with multi-byte string in the message often indicates\nan encoding issue. In particular errors often arise when trying to do\nstring manipulations in R on character vectors for which the encoding is\nnot properly set. Here's an example with some Internet logging data that\nwe used a few years ago in class in a problem set and which caused some\nproblems.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nload('../data/IPs.RData') # loads in an object named 'text'\ntmp <- try(substring(text, 1, 15))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nError in substring(text, 1, 15) : \n  invalid multibyte string at '<bf>a7lw8<6c>z2nX,%@ [128.32.244.179] by ncpc-email with ESMTP\n(SMTPD32-7.04) id A06E24A0116; Mon, 10 Jun 2002 11:43:42 +0800'\n```\n:::\n\n```{.r .cell-code}\n## the issue occurs with the 6402th element (found by trial and error):\ntmp <- try(substring(text[1:6401],1,15))\ntmp <- try(substring(text[1:6402],1,15))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nError in substring(text[1:6402], 1, 15) : \n  invalid multibyte string at '<bf>a7lw8<6c>z2nX,%@ [128.32.244.179] by ncpc-email with ESMTP\n(SMTPD32-7.04) id A06E24A0116; Mon, 10 Jun 2002 11:43:42 +0800'\n```\n:::\n\n```{.r .cell-code}\ntext[6402] # note the Latin-1 character\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"from 5#c\\xbfa7lw8lz2nX,%@ [128.32.244.179] by ncpc-email with ESMTP\\n(SMTPD32-7.04) id A06E24A0116; Mon, 10 Jun 2002 11:43:42 +0800\"\n```\n:::\n\n```{.r .cell-code}\ntable(Encoding(text))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nunknown \n   6936 \n```\n:::\n\n```{.r .cell-code}\n## Option 1\nEncoding(text) <- \"latin1\"\ntmp <- try(substring(text, 1, 15))\ntmp[6402]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"from 5#c¿a7lw8l\"\n```\n:::\n\n```{.r .cell-code}\n## Option 2\nload('../data/IPs.RData') # loads in an object named 'text'\ntmp <- try(substring(text, 1, 15))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nError in substring(text, 1, 15) : \n  invalid multibyte string at '<bf>a7lw8<6c>z2nX,%@ [128.32.244.179] by ncpc-email with ESMTP\n(SMTPD32-7.04) id A06E24A0116; Mon, 10 Jun 2002 11:43:42 +0800'\n```\n:::\n\n```{.r .cell-code}\ntext <- iconv(text, from = \"latin1\", to = \"UTF-8\")\ntmp <- try(substring(text, 1, 15))\n```\n:::\n\n\n\n# 6. Data structures\n\nAs we're reading data into R or other languages, it's important to think about the data structures we'll use to store the information. The data structure we choose can affect:\n\n- the amount of memory we need, \n- how quickly we can access the information in the data structure,\n- how much copying needs to be done to add information to or remove information from the data structure,\n- how efficiently we can use the data in subsequent computations.\n\nThis means that what you plan to do with the data should guide what kind of structure you store the data in.\n\n## Standard data structures in R and Python\n\n- In R and Python, one often ends up working with dataframes, lists, and vectors/matrices/arrays/tensors.\n- In R, if we are not working with rectangular datasets or standard numerical objects, we often end up using lists or enhanced versions of lists, sometimes with deeply nested structures.\n- In Python we commonly work with data structures that are part of additional packages, in particular numpy arrays and pandas dataframes.\n- Dictionaries in Python allow for easy use of key-value pairs where one can access values based on their key/label. In R one can do something similar with named vectors or named lists or (more efficiently) by using environments.\n\nIn Unit 7, we'll talk about *distributed* data structures that allow one to easily work with data distributed across multiple computers.\n\n## Other kinds of data structures\n\nYou may have heard of various other kinds of data structures, such as linked lists, trees, graphs, queues, and stacks. One of the key aspects that differentiate such data structures is how one navigates through the elements.\n\n*Sets* are collections of elements that don't have any duplicates (like a mathematical set).\n\nWith a *linked list*, with each element (or node) has a value and a pointer (reference) to the location of the next element. (With a doubly-linked list, there is also a pointer back to the previous element.) One big advantage of this is that one can insert an element by simply modifying the pointers involved at the site of the insertion, without copying any of the other elements in the list. A big disadvantage is that to get to an element you have to navigate through the list. \n\n![Linked list (courtesy of computersciencewiki.org)](linked-list.png)\n\n\nBoth *trees* and *graphs* are collections of nodes (vertices) and links (edges). A tree involves a set of nodes and links to child nodes (also possibly containing information linking the child nodes to their parent nodes). With a graph, the links might not be directional, and there can be cycles.\n\n![Tree (courtesy of computersciencewiki.org)](tree.png)\n\n\n![Graph (courtesy of computersciencewiki.org)](graph.png)\n\n\nA *stack* is a collection of elements that behave like a stack of lunch trays. You can only access the top element directly(\"last in, first out\"), so the operations are that you can push a new element onto the stack or pop the top element off the stack. In fact, nested function calls behave as stacks, and the memory used in the process of evaluating the function calls is called the 'stack'.\n\nA *queue* is like the line at a grocery store, behaving as \"first in, first out\".\n\nOne can use such data structures either directly or via add-on packages in R and Python, though I don't think they're all that commonly used in R. This is probably because statistical/data science/machine learning workflows often involve either 'rectangular' data (i.e., dataframe-style data) and/or mathematical computations with arrays. That said, trees and graphs are widely used.\n\nSome related concepts that we'll discuss further in Unit 5 include:\n\n - types: this refers to how a given piece of information is stored and what operations can be done with the information.\n    - 'primitive' types are the most basic types that often relate directly to how data are stored in memory or on disk (e.g., booleans, integers, numeric (real-valued), character, pointer (address, reference).\n - pointers: references to other locations (addresses) in memory. One often uses pointers to avoid unnecessary copying of data. \n - hashes: hashing involves fast lookup of the value associated with a key (a label), using a hash function, which allows one to convert the key to an address. This avoids having to find the value associated with a specific key by looking through all the keys until the key of interest is found (an O(n) operation).\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}