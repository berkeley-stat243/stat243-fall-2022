[
  {
    "objectID": "ps/ps2.html",
    "href": "ps/ps2.html",
    "title": "Problem Set 2",
    "section": "",
    "text": "This covers material in Units 3 and 4.\nIt’s due at 10 am (Pacific) on September 16, both submitted as a PDF to Gradescope as well as committed to your GitHub repository.\nPlease see PS1 for formatting and attribution requirements.\nNote that using chunks of bash code in Rmd/Rtex/Rnw can sometimes be troublesome, particularly on Windows machine. Some things to try if you are having trouble:\n\nset the terminal to be the Ubuntu subsystem if you are on Windows;\nif using RStudio, you may only be able to run bash chunks if you have the notebook mode on;\nusing single versus double quotes in your Rmd/Rtex document may make a difference.\n\nIf you can’t produce a PDF that includes the bash chunk output, feel free to not run those chunks and just paste in the output you get manually. Please post on the Ed forum if you have trouble.\nYou will probably need to use sed in a basic way as we have used it so far in class and in the bash tutorial. You should not need to use more advanced functionality nor should you need to use awk, but you may if you want to."
  },
  {
    "objectID": "ps/ps2.html#problems",
    "href": "ps/ps2.html#problems",
    "title": "Problem Set 2",
    "section": "Problems",
    "text": "Problems\n\nAdd assertions and testing for your code from Problem 4 of PS1. You may use a modified version of your PS1 solution, perhaps because you found errors in what you did or wanted to make changes based on Chris’ solutions or your discussions with other students.\n\nAdd formal assertions using the assertthat package. You should try to catch the various incorrect inputs a user could provide and anything else that could go wrong (e.g., what happens if one is not online?).\nUse the testthat package to set up a small but thoughtful set of tests of your functions. In deciding on your tests, try to think about tricky cases that might cause problems.\n\nA friend of mine is planning to get married in Death Valley National Park in March (this problem is based on real events…). She wants to hold it as late in March as possible but without having a high chance of a very hot day. This problem will automate the task of generating information about what day to hold the wedding on using data from the Global Historical Climatology Network. All of your operations should be done using the bash shell except part (c). Also, ALL of your work should be done using shell commands that you save in your solution file. So you can’t say “I downloaded the data from such-and-such website” or “I unzipped the file”; you need to give us the bash code that we could run to repeat what you did. This is partly for practice in writing shell code and partly to enforce the idea that your work should be reproducible and documented.\n\nDownload yearly climate data for a set of years of interest into a temporary directory. Do not download all the years and feel free to focus on a small number of years to reduce the amount of data you need to download. Note that data for Death Valley is only present in the last few decades. As you are processing the files, report the number of observations in each year by printing the information to the screen, including if there are no observations for that year.\nSubset to the station corresponding to Death Valley, to TMAX (maximum daily temperature), and to March, and put all the data into a single file. In subsetting to Death Valley, get the information programmatically from the ghcnd-stations.txt file one level up in the website. Do NOT type in the station ID code when you retrieve the Death Valley data from the yearly files.\nCreate an R or Python chunk that takes as input your single file from (b) and makes a single plot of side-by-side boxplots containing the maximum daily temperatures on each calendar day in March.\nNow generalize your code from parts (a) and (b). Write a shell function that takes as arguments a string for identifying the location, the weather variable of interest, and the time period (i.e., the years of interest and the month of interest), and returns the results. Your function should detect if the user provides the wrong number of arguments or a string that doesn’t allow one to identify a single weather station and return a useful error message. It should also give useful help information if the user invokes the function as: get_weather -h. Finally the function should remove the raw downloaded data files (or you should download into your operating system’s temporary file location). Hint: to check for equality in an if statement, you generally need syntax like: if [ \"${var}\" == \"7\" ].\n\nOn Friday, September 16, Section will consist of a discussion of good practices in reproducible research and computing. In preparation, please do the following:\n\nRead one of these five items (you can read more if you want of course!):\n\nThe Preface, the Basic Reproducible Workflow Template chapter and Lessons Learned chapters of this Berkeley-produced book on reproducible research, found online via UC Library search: The Practice of Reproducible Research. The book is written from the perspective of learning about reproducibility practices using case studies\nThis article is written from the perspective of scientific research: Millman and Perez.\nThis article is written from the perspective of scientific software development: Wilson et.al.\nThis article is written from the perspective of social science research: Chapter 11 of Christensen et al. Transparent and Reproducible Social Science Research. (You can also see the entire book online via UC Library search\nThis article is also written from the perspective of social science research: Gentzkow and Shapiro.\n\nWhen reading, please think about the following questions:\n\nAre there practices suggested that seem particularly compelling to you? What about ones that don’t seem compelling to you?\nDo you currently use any of the practices described? Which ones, and why? Which ones do you not use and why (apart from just not being aware of them)?\nWhy don’t researchers consistently utilize these principles/tools? Which ones might be the most/least used? Which ones might be the easiest/most difficult to implement?\nWhat principles and practices described apply more to analyses of data, and which apply more to software engineering? Which principles and practices apply to both?\n\nAs your answer to this problem, please write a paragraph or two where you discuss one or more of these questions. I’m not looking for more than 10-15 sentences, just evidence that you’ve read one of the items and considered it thoughtfully.\nPlease skim through this economics paper, focusing on the Method section, but note that the idea is just to get the main idea of the analysis steps, not to understand the context fully or see all the details. Then look at the code the authors provide and think about whether that code makes it easy to reproduce what they did in the paper. Based on the Unit 4 class notes and the item you read above in (a), make a short list of strengths and weaknesses of the reproducibility of the authors’ materials and provide that as your solution to this problem. Your task in Section on Friday, September 16 will be to discuss with your group and in Section as a whole and come up with a consensus answer. Some things to think about when you look at their materials:\n\nFrom the information above and the documentation provided, can you quickly identify where in the code (if present at all) the authors:\n\ncollected their data,\ncleaned/processed their data,\ncalculated various statistics, and\nproduced the plots in their paper.\n\nIn terms of coding what elements of their project do you like? Consider: Documentation, comments, organization, naming, workflow, data provenance, etc.\nSimilarly, what do you think could be improved?\nWithout examining the code itself, can you quickly discern the purpose of each file?\nWithout examining the code in detail, can you quickly tell what each block of code does?\nAre there exceptions to your answers above?\nIn what way do the authors document their workflow? Do you think this method is effective?"
  },
  {
    "objectID": "ps/ps3.html",
    "href": "ps/ps3.html",
    "title": "Problem Set 3",
    "section": "",
    "text": "This covers material in Units 4 and 5.\nIt’s due at 10 am (Pacific) on September 28, both submitted as a PDF to Gradescope as well as committed to your GitHub repository.\nPlease see PS1 and the grading rubric for formatting and attribution requirements."
  },
  {
    "objectID": "ps/ps3.html#problems",
    "href": "ps/ps3.html#problems",
    "title": "Problem Set 3",
    "section": "Problems",
    "text": "Problems\n\nThe goal of Problem 1 is two-fold: first to give you practice with regular expressions and string processing and the second (more important) to have you thinking about writing well-structured, readable code, using a functional programming perspective. Regarding the latter, please focus your attention on writing short, modular functions that allow effective use of map operations (or operate in a vectorized manner where possible). Think carefully about how to structure your objects to store the debate information. You might have each candidate’s response to a question be an element in a character vector or in a list.\nThe website Commission on Presidential Debates has the text from recent debates between the candidates for President of the United States. (As a bit of background for those of you not familiar with the US political system, there are usually three debates between the Republican and Democratic candidates at which they are asked questions so that US voters can determine which candidate they would like to vote for.) Your task is to process the information and produce data on the debates. Note that while I present the problem below as subparts (a)-(f), your solution does not need to be divided into subparts in the same way, but you do need to make clear in your solution where and how you are doing what. Your solution should do all of the downloading and processing from within R so that your operations are self-contained and reproducible. For the purposes of this problem, please work on the the debates I’ve selected (see code below) for the years 2000, 2004, 2008, 2012, 2016, and 2020. (I’ve tried to select debates that cover domestic policy in whole or in part to control one source of variation, namely the topic of the debate.) I’ll call each individual response by a candidate to a question a “chunk”. A chunk might just be a few words or might be multiple paragraphs. The result of all of this activity in parts (a)-(d) should be well-structured data object(s) containing the information about the debates and candidates.\nGiven that in earlier problem sets, you already worked on downloading and processing HTML, I’m giving you the code (in the file ps/ps3prob1.R in the class repository) to download the HTML and do some initial processing, so you can dive right into processing the actual debate text.\n\nConvert the text so that for each debate, the spoken words are split up into individual chunks of text spoken by each speaker (including the moderator). If there are two chunks in a row spoken by a candidate, combine them into a single chunk. Make sure that any formatting and non-spoken text (e.g., the tags for ‘Laughter’ and ‘Applause’) is stripped out. You should create some sort of metadata or attributes so that you can easily extract only the chunks for one candidate in later processing. You may need to do some looping as you manipulate the text to get the chunks, but try to do as much as possible in a vectorized way. Please print out or plot the number of chunks for the candidates.\nUse regular expression processing to extract the sentences and individual words as character vectors, one element per sentence and one element per word. Make some effort to figure out if there is any strangely-formatted text (e.g., using regular expressions) - there may be strange formatting from transcription errors in some cases.\nFor each candidate, for each debate, count the number of words and characters and compute the average word length for each candidate. Store this information in an R data structure and make a plot of the word length for the candidates. Comment briefly on the results.\nFor each candidate, count the following words or word stems and store in an R data structure: I, we, America{,n}, democra{cy,tic}, republic, Democrat{,ic}, Republican, free{,dom}, terror{,ism}, safe{,r,st,ty}, God [not including God bless], God Bless, {Jesus, Christ, Christian}. Make a plot or two and comment briefly on the results.\nPlease include unit tests for the functions that do your regular expression processing (i.e., the functions that extract sentences, words, and interesting content words. For the sake of time, you can keep this to a small number of tests for each function.\n(Extra credit) We may give extra credit for particularly nice solutions.\n\nHint: Depending on how your process the text, you may end up with lists for which the name of a list element is very long. Syntax such as names(myObj) <- NULL may be helpful.\nThis problem asks you to design an object-oriented programming (OOP) approach to the debate text analysis of problem 2. You don’t need to code anything up, but rather to decide what fields and methods you would create for a class that represents a debate, with additional class(es) as needed to represent the spoken chunks and metadata on the chunks for the candidates. The methods should include methods that take input text and create the fields in the classes. You can think of this in the context of R6 classes, or in the context of classes in an object-oriented language like Python or C++. To be clear, you do not have to write any of the code for the methods nor even formal code for the class structure; the idea is to design the formats of the classes. Basically if you were to redesign your functional programming code from problem 2 to use OOP, how would you design it? As your response, for each class, please provide a bulleted list of methods and bulleted list of fields and for each item briefly comment what the purpose is. Also note how each method uses other fields or methods."
  },
  {
    "objectID": "ps/ps1.html",
    "href": "ps/ps1.html",
    "title": "Problem Set 1",
    "section": "",
    "text": "This covers material in Units 2 and 4 as well as practice with some of the tools we’ll use in the course (R Markdown/knitr).\nIt’s due at 10 am (Pacific) on September 7, both submitted as a PDF to Gradescope as well as committed to your GitHub repository.\nPlease note my comments in the syllabus about when to ask for help and about working together. In particular, please give the names of any other students that you worked with on the problem set and indicate in the text or in code comments any specific ideas or code you borrowed from another student or any online reference."
  },
  {
    "objectID": "ps/ps1.html#formatting-requirements",
    "href": "ps/ps1.html#formatting-requirements",
    "title": "Problem Set 1",
    "section": "Formatting requirements",
    "text": "Formatting requirements\n\nYour electronic solution should be in the form of an R markdown file named ps1.Rmd, a Quarto markdown document named ps1.qmd, or a LaTeX+knitr file named ps1.Rtex, with R code chunks included in the file (or read in from a separate code file). Please see the dynamic documents tutorial for more information on how to do this.\nYour PDF submission should be the PDF produced from your Rmd/qmd/Rtex. Your GitHub submission should include the Rmd/qmd/Rtex file, any R code files containing chunks that you read into your Rmd/qmd/Rtex file, and the final PDF, all named according to the submission guidelines.\nYour solution should not just be code - you should have text describing how you approached the problem and what the various steps were. Your code should have comments indicating what each function or block of code does, and for any lines of code or code constructs that may be hard to understand, a comment indicating what that code does. You do not need to (and should not) show exhaustive output, but in general you should show short examples of what your code does to demonstrate its functionality. Please see the grading rubric."
  },
  {
    "objectID": "ps/ps1.html#problems",
    "href": "ps/ps1.html#problems",
    "title": "Problem Set 1",
    "section": "Problems",
    "text": "Problems\n\nPlease read these lecture notes about how computers work, used in a class on statistical computing at CMU. In particular, make sure you know the difference between disk and memory. You don’t need to turn anything in for this problem.\nThis problem explores file sizes in light of understanding storage in ASCII plain text versus binary formats and the fact that numbers are (generally) stored as 8 bytes per number in binary format.\n\nExplain the sizes of the two files created below. In discussing the CSV text file, how many characters do you expect to be in the file (i.e., you should be able to estimate this very accurately from first principles without using wc or any explicit program that counts characters). Hint: what do we know about numbers drawn from a standard normal distribution?\n\nn <-  1e7\na <- matrix(rnorm(n), ncol = 100)\na <- round(a, 10)\n\nfn_csv <- tempfile(fileext = '.csv')\nwrite.table(a, file = fn_csv, quote=FALSE, row.names=FALSE,\n               col.names = FALSE, sep=',')\nfn_rda <- tempfile(fileext = '.Rda')\nsave(a, file = fn_rda, compress = FALSE)\n\nfile.size(fn_csv)\n\n[1] 133890941\n\nfile.size(fn_rda)\n\n[1] 80000096\n\n\nNow consider saving out the numbers one row per number. Given we no longer have to save all the commas, why is the file size unchanged?\n\nb <- a\ndim(b) <- c(1e7, 1)  ## change to one column by adjusting attribute\nfn_csv_onecol <- tempfile(fileext = '.csv')\nwrite.table(b, file = fn_csv_onecol, quote=FALSE,\n     row.names=FALSE, col.names = FALSE, sep=',')\nfile.size(fn_csv_onecol)\n\n[1] 133890941\n\n\nConsider the following ways of reading the data into R (though similar results would be obtained in other languages). Explain the difference in speed between the three situations. Side note: in this case readr::read_csv() is rather faster than read.csv().\n\nsystem.time(a1 <- read.csv(fn_csv, header = FALSE))\n\n   user  system elapsed \n 24.837   0.232  25.080 \n\nsystem.time(a2 <- read.csv(fn_csv, header = FALSE,\n                    colClasses = 'numeric'))\n\n   user  system elapsed \n  2.316   0.071   2.388 \n\nsystem.time(a3 <- scan(fn_csv, sep = ','))\n\n   user  system elapsed \n  2.332   0.024   2.356 \n\n\nExplain why tmp.Rda is so much bigger than tmp2.Rda given they both contain the same number of numeric values.\n\nfn1_rda <- tempfile(fileext = '.Rda')\nsave(a, file = fn1_rda)\nfile.size(fn1_rda)\n\n[1] 76778550\n\nb <- rep(rnorm(1), 1e7)\nfn2_rda <- tempfile(fileext = '.Rda')\nsave(b, file = fn2_rda)\nfile.size(fn2_rda)\n\n[1] 116501\n\n\n\nPlease read Unit 4 on good programming/project practices and incorporate what you’ve learned from that reading into your solution for Problem 4. As your response to this question, briefly (a few sentences) note what you did in your code for Problem 4 that reflects the material in Sections 1.2 and 1.3 of Unit 4. Please also note anything in Unit 4 that you disagree with, if you have a different stylistic perspective.\nGo to Google Scholar and enter the name (including first name to help with disambiguation) for a researcher whose work interests you. (If you want to do the one that will match the problem set solutions, you can use “Jennifer Chayes”, who is Berkeley’s dean for data science.) If you’ve entered the name of a researcher that Google Scholar recognizes as having a Google Scholar profile, you should see that the first item returned is a “User profile”. Next, if you click on the hyperlink for the researcher under “User profiles for <researcher name>”, you’ll see that brings you to a page that provides the citations for all of the researcher’s papers. IMPORTANT: if you repeatedly query the Google Scholar site too quickly, Google will start returning “503” errors because it detects automated usage (see problem 5 below). So, if you are going to run code from a script such that multiple queries would get done in quick succession, please put something like Sys.sleep(2) in between the calls that do the HTTP requests. Also when developing your code, once you have the code in part (a) working to download the HTML, use the downloaded HTML to develop the remainder of your code and don’t keep re-downloading the HTML as you work on the remainder of the code.\n\nNow, based on the information returned by your work above, including the various URLs that your searching and clicking generated, write R code that will programmatically return the citation page for your researcher. Specifically, write a function whose input is the character string of the name of the researcher and whose output is the HTML (as an object of class xml_document) corresponding to the researcher’s citation page as well as the researcher’s Google Scholar ID.\nHint: you will need to use some string processing functions to extract the Scholar ID (and possibly for other things) from the output of the various rvest functions. I recommend functions from the stringr package (we’ll see these in Unit 5 and you can find information in the string processing tutorial), in particular: str_detect(), str_extract(), str_split(), and str_replace(). You should NOT need to use regular expressions (which we’ll cover in Unit 5), but you can if you want/know how to. Finally if you get an error message like this “Syntax error in regexp pattern. (U_REGEX_RULE_SYNTAX)”, it means the string processing function is interpreting the characters you are looking for as a regular expression. Simply wrap the string you are looking for in the fixed() function, e.g., fixed('sdf?=sd34') so the characters are simply interpreted as regular characters (i.e., interpreted literally).\nCreate a second function to process the resulting HTML to create an R data frame that contains the article title, authors, journal information, year of publication, and number of citations as five columns of information. Try your function on a second researcher to provide more confidence that your function is working properly.\nInclude checks in your code so that it fails gracefully if the user provides invalid input or Google Scholar doesn’t return a result. You don’t have to use the assertthat package as we won’t cover that until Section on Sep. 9, but you can if you want.\n(Extra credit) Fix your function so that you get all of the results for a researcher and not just the first 20.\n\nHint: as you are trying to understand the structure of the HTML, one option is to use write_xml() on the result of read_html() and then view the file that is produced in a text editor.\nNote: For simplicity you can either assume only one User Profile will be returned by your initial search or that you can just use the first of the profiles.\nLook at the robots.txt for Google Scholar (scholar.google.com) and the references in Unit 2 on the ethics of webscraping. Does it seem like it’s ok to scrape data from Google Scholar?\n(Extra Credit) The reticulate package and R Markdown allow you to have Python and R chunks in a document that interact with each other. Demonstrate the ability to use this functionality, in particular sending data from R to Python and back to R, with some processing done in Python (it doesn’t have to be complicated processing)."
  },
  {
    "objectID": "rubric.html",
    "href": "rubric.html",
    "title": "Statistics 243 Fall 2022",
    "section": "",
    "text": "This document provides guidance for submitting high-quality problem set (and project) solutions. This guidance is based on general good practices for scientific communication, reproducible research, and software development."
  },
  {
    "objectID": "rubric.html#general-presentation",
    "href": "rubric.html#general-presentation",
    "title": "Statistics 243 Fall 2022",
    "section": "General presentation",
    "text": "General presentation\n\nSimply presenting code or derivations is not sufficient.\nBriefly describe the overall goal or strategy before providing code/derivations.\nAs needed describe what the pieces of your code/derivation are doing to make it easier for a reader to follow the steps.\nKeep your output focused on illustrating what you did, without distracting from the flow of your solution by showing voluminous output. The output should illustrate and demonstrate, not overwhelm. If you need to show longer output, you can add it at the end as supplemental material."
  },
  {
    "objectID": "rubric.html#coding-practice",
    "href": "rubric.html#coding-practice",
    "title": "Statistics 243 Fall 2022",
    "section": "Coding practice",
    "text": "Coding practice\n\nMinimize (or eliminate) use of global variables.\nBreak down work into core tasks and develop small, modular, self-contained functions (or class methods) to carry out those tasks.\nDon’t repeat code. As needed refactor code to create new functions (or class methods).\nFunctions and classes should be “weakly coupled”, interacting via their interfaces and not by having to know the internals of how they work.\nUse data structures appropriate for the computations that need to be done.\nDon’t hard code ‘magic’ numbers. Assign such numbers to variables with clear names, e.g., speedOfLight = 3e8.\nProvide reasonable default arguments to functions (or class methods) when possible.\nProvide tests (including unit tests) when requested (this is good general practice but we won’t require it in all cases).\nAvoid overly complicated syntax – try to use the clearest syntax you can to solve the problem.\nIn terms of speed, don’t worry about it too much so long as the code finishes real-world tasks in a reasonable amount of time. When optimizing, focus on the parts of the code that are the bottlenecks.\nUse functions already available in the language rather than recreating yourself."
  },
  {
    "objectID": "rubric.html#code-style",
    "href": "rubric.html#code-style",
    "title": "Statistics 243 Fall 2022",
    "section": "Code style",
    "text": "Code style\n\nFollow a consistent style. For example (not required) the tidyverse style guide or its offshoot Google R style.\nUse informative variable and function names and have a consistent naming style.\nUse whitespace (spaces, newlines) and parentheses to make the structure of the code easy to understand and the individual syntax pieces clear.\nUse consistent indentation to make the structure of the code easy to understand.\nProvide comments that give the goal of a given piece of code and why it does things, but don’t use comments to restate what the code does when it should be obvious from reading the code.\n\nProvide summaries for blocks of code.\nFor particularly complicated syntax, say what a given piece of code does."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistics 243 Fall 2022",
    "section": "",
    "text": "See the links above for the key resources for the course."
  },
  {
    "objectID": "index.html#questions-about-taking-the-class",
    "href": "index.html#questions-about-taking-the-class",
    "title": "Statistics 243 Fall 2022",
    "section": "Questions about taking the class",
    "text": "Questions about taking the class\nIf you would like to audit the class, enroll as a UC Berkeley undergraduate, or enroll as a concurrent enrollment student (i.e., for visiting students), or for some other reason are not enrolled, please fill out this survey as soon as possible. All those enrolled or wishing to take the class should have filled it out by Friday August 26 at noon.\nUndergraduates can only take the course with my permission and once all graduate students have an opportunity to register. I expect there will be space, and will likely start admitting undergraduates early the week of August 29 or late the previous week.\nConcurrent enrollment students (e.g., exchange students from other universities/countries) can take the course with my permission and once all UC Berkeley students have had an opportunity to register. I expect there will be space, and will likely start admitting concurrent enrollment students early the week of August 29.\nPlease see the syllabus for the math and statistics background I expect, as well as the need to be familiar with R or to get up to speed in R during the first few weeks of the semester.\nThe first three weeks involve a lot of moving pieces, in part related to trying to get everyone up to speed with the bash shell and R. Please use the schedule to keep on top of what you need to do."
  },
  {
    "objectID": "index.html#course-content",
    "href": "index.html#course-content",
    "title": "Statistics 243 Fall 2022",
    "section": "Course content",
    "text": "Course content\n\nThis site: most non-recorded course material\n\nFor html versions of the Units, see the navigation bar above.\nFor PDF versions of the Units, clone the GitHub repository, or if you’re not yet familiar with Git, download a zip file.\n\nVarious SCF tutorials: These include the various tutorials referred to in the class materials (e.g., the UNIX and bash shell tutorials, the dynamic documents tutorial, the Git tutorial, the string processing tutorial, etc.).\nbCourses: links to class course captures and any pre-recorded material.\n\nIf you’re not yet familiar with Git, go to the upper right of this page and click on ‘Clone or download’ and then ‘Download ZIP’."
  },
  {
    "objectID": "office_hours.html",
    "href": "office_hours.html",
    "title": "Office hours",
    "section": "",
    "text": "Chris (Evans 495 or Zoom (see Ed Discussion post for link))\n\nMonday 11-11:30 am\nMonday 3:30-5 pm\nTuesday 4:30-5:30 pm\nThursday 11-11:30 am\nfeel free to schedule an appointment or to drop by if my door is open\n\nJames:\n\nWednesday 4:30-5:30 pm (Evans 444)\nFridays during unused section time, generally 2-3 pm and 4-4:30 pm (Evans 344)"
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Statistics 243 Fall 2022",
    "section": "",
    "text": "Statistics 243 is an introduction to statistical computing, taught using R. The course will cover both programming concepts and statistical computing concepts. Programming concepts will include data and text manipulation, data structures, functions and variable scope, regular expressions, debugging/testing, and parallel processing. Statistical computing topics will include working with large datasets, numerical linear algebra, computer arithmetic/precision, simulation studies and Monte Carlo methods, numerical optimization, and numerical integration/differentiation. A goal is that coverage of these topics complement the models/methods discussed in the rest of the statistics/biostatistics graduate curriculum. We will also cover the basics of UNIX/Linux, in particular some basic shell scripting and operating on remote servers, as well as a moderate amount of Python.\n\n\n\nWhile the course is taught using R and you will learn a lot about using R at an advanced level, this is not a course about learning R. Rather the focus of the course is computing for statistics and data science more generally, using R to illustrate the concepts.\nThis is not a course that will cover specific statistical/machine learning/data analysis methods.\n\n\n\n\nInformal prerequisites: If you are not a statistics or biostatistics graduate student, please chat with me if you’re not sure if this course makes sense for you. A background in calculus, linear algebra, probability and statistics is expected, as well as a basic ability to operate on a computer (but I do not assume familiarity with the UNIX-style command line/terminal/shell). Furthermore, I’m expecting you will know the basics of R, at the level of the Modules 1-5 in the R bootcamp offered Aug. 20-21, 2022. If you don’t have that background you’ll need to spend time in the initial couple weeks getting up to speed. In addition, we’ll have an optional hands-on practice session during the second or third week of class, and the GSI can also provide assistance."
  },
  {
    "objectID": "syllabus.html#covid-considerations",
    "href": "syllabus.html#covid-considerations",
    "title": "Statistics 243 Fall 2022",
    "section": "Covid considerations",
    "text": "Covid considerations\nWe’ll be following university policy through the semester.\n\nClass and section are in person. I will be recording class (but not section) via the room’s course capture capabilities (find the recordings in the bCourses Media Gallery) so if anyone needs to miss a class they should be able to catch up. There is in-class discussion and problem-solving so I expect students to attend class in general. However, please do not come to class if you feel any symptoms – you can watch the recording and it will not affect your grade in any way.\nMasks are not required at the moment, but you are welcome to wear one. For the sake of communicating most clearly (and because I had Covid in June) I won’t wear one while teaching."
  },
  {
    "objectID": "syllabus.html#objectives-of-the-course",
    "href": "syllabus.html#objectives-of-the-course",
    "title": "Statistics 243 Fall 2022",
    "section": "Objectives of the course",
    "text": "Objectives of the course\nThe goals of the course are that, by the end of the course, students be able to:\n\noperate effectively in a UNIX environment and on remote servers and compute clusters;\nhave a solid understanding of general programming concepts and principles, and be able to program effectively (including having an advanced knowledge of R functionality);\nbe familiar with concepts and tools for reproducible research and good scientific computing practices; and\nunderstand in depth and be able to make use of principles of numerical linear algebra, optimization, and simulation for statistics- and data science-related analyses and research."
  },
  {
    "objectID": "syllabus.html#topics-in-order-with-rough-timing",
    "href": "syllabus.html#topics-in-order-with-rough-timing",
    "title": "Statistics 243 Fall 2022",
    "section": "Topics (in order with rough timing)",
    "text": "Topics (in order with rough timing)\nThe ‘days’ here are (roughly) class sessions, as general guidance.\n\nIntroduction to UNIX, operating on a compute server (1 day)\nData formats, data access, webscraping, data structures (2 days)\nDebugging, good programming practices, reproducible research (1 day)\nThe bash shell and shell scripting, version control (3 days)\nProgramming concepts and advanced R programming: text processing and regular expressions, object-oriented programming, functions and variable scope, efficient programming, memory use (9 days)\nParallel processing (2 days)\nWorking with databases, hashing, and big data (3 days)\nComputer arithmetic/representation of numbers on a computer (3 days)\nNumerical linear algebra (5 days)\nSimulation studies and Monte Carlo (2 days)\nOptimization (6 days)\nNumerical integration and differentiation (1 day)\nGraphics (1 day)\n\nIf you want to get a sense of what material we will cover in more detail, in advance, you can take a look at the materials in the units directory of GitHub repository from when I taught the class in 2021."
  },
  {
    "objectID": "syllabus.html#personnel",
    "href": "syllabus.html#personnel",
    "title": "Statistics 243 Fall 2022",
    "section": "Personnel",
    "text": "Personnel\n\nInstructor:\n\nChris Paciorek (paciorek@stat.berkeley.edu)\n\nGSI\n\nJames Duncan\n\nOffice hours can be found here.\nWhen to see us about an assignment: We’re here to help, including providing guidance on assignments. You don’t want to be futilely spinning your wheels for a long time getting nowhere. That said, before coming to see us about a difficulty, you should try something a few different ways and define/summarize for yourself what is going wrong or where you are getting stuck."
  },
  {
    "objectID": "syllabus.html#course-websites-github-piazza-gradescope-and-bcourses",
    "href": "syllabus.html#course-websites-github-piazza-gradescope-and-bcourses",
    "title": "Statistics 243 Fall 2022",
    "section": "Course websites: GitHub, Ed Discussion, Gradescope, and bCourses",
    "text": "Course websites: GitHub, Ed Discussion, Gradescope, and bCourses\nKey websites for the course are:\n\nThis course website, which is hosted on GitHub pages, and the GitHub repository containing the source materials: https://github.com/berkeley-stat243/stat243-fall-2022\nSCF tutorials for additional content: https://statistics.berkeley.edu/computing/training/tutorials\nEd Discussion site for discussions/Q&A: https://edstem.org/us/courses/25090/discussion/\nbCourses site for course capture recordings (see Media Gallery) and possibly some other materials: https://bcourses.berkeley.edu/courses/1507757.\nGradescope for assignments (also linked from bCourses): https://www.gradescope.com/courses/425343\n\nAll course materials will be posted on here on the website (and on GitHub) except for video content, which will be in bCourses.\n\nCourse discussion\nWe will use the course Ed Discussion site for communication (announcements, questions, and discussion). You should ask questions about class material and problem sets through the site. Please use this site for your questions so that either James or I can respond and so that everyone can benefit from the discussion. I suggest you to modify your settings on Ed Discussion so you are informed by email of postings. In particular you are responsible for keeping track of all course announcements, which we’ll make on the Discussion forum. I strongly encourage you to respond to or comment on each other’s questions as well (this will help your class participation grade), although of course you should not provide a solution to a problem set problem. If you have a specific administrative question you need to direct just to me, it’s fine to email me directly or post privately on the Discussion site. But if you simply want to privately ask a question about content, then just come to an office hour or see me after class or James during/after section.\nIf you’re enrolled in the class you should be a member of the group and be able to access it. If you’re auditing or not yet enrolled and would like access, make sure to fill out the course survey and I will add you. In addition, we will use Gradescope for viewing grades."
  },
  {
    "objectID": "syllabus.html#course-material",
    "href": "syllabus.html#course-material",
    "title": "Statistics 243 Fall 2022",
    "section": "Course material",
    "text": "Course material\n\nPrimary materials: Course notes on course webpage/GitHub, SCF tutorials, and potentially pre-recorded videos on bCourses.\nBack-up textbooks (generally available via UC Library via links below):\n\nFor bash: Newham, Cameron and Rosenblatt, Bill. Learning the bash Shell available electronically through UC Library\nFor R:\n\nAdler, Joseph; R in a Nutshell available electronically through UC Library\nWickham, Hadley: Advanced R\n\nFor statistical computing topics:\n\nGentle, James. Computational Statistics\nGentle, James. Matrix Algebra or Numerical Linear Algebra with Applications in Statistics\n\nOther resources with more details on particular aspects of R:\n\nChambers, John; Software for Data Analysis: Programming with R\nXie, Yihui; Dynamic documents with R and knitr\nThe Quarto reference guide\nNolan, Deborah and Temple Lang, Duncan. XML and Web Technologies for Data Sciences with R\nThe R-intro and R-lang documentation\nMurrell, Paul; R Graphics, 2nd ed. http://www.stat.auckland.ac.nz/\\(\\sim\\)paul/RG2e/\nMurrell, Paul; Introduction to Data Technologies. http://www.stat.auckland.ac.nz/\\(\\sim\\)paul/ItDT/\n\nOther resources with more detail on particular aspects of statistical computing concepts:\n\nLange, Kenneth; Numerical Analysis for Statisticians, 2nd ed. First edition available through UC library\nMonahan, John; Numerical Methods of Statistics"
  },
  {
    "objectID": "syllabus.html#section",
    "href": "syllabus.html#section",
    "title": "Statistics 243 Fall 2022",
    "section": "Section",
    "text": "Section\nThe GSI will lead a two-hour discussion section each week (there are two sections). By and large, these will only last for about one hour of actual content, but the second hour may be used as an office hour with the GSI or for troubleshooting software during the early weeks. The discussion sections will vary in format and topic, but material will include demonstrations on various topics (version control, debugging, testing, etc.), group work on these topics, discussion of relevant papers, and discussion of problem set solutions. The first section (1-3 pm) generally has more demand, so to avoid having too many people in the room, you should go to your assigned section unless you talk to me first."
  },
  {
    "objectID": "syllabus.html#computing-resources",
    "href": "syllabus.html#computing-resources",
    "title": "Statistics 243 Fall 2022",
    "section": "Computing Resources",
    "text": "Computing Resources\nMost work for the course can be done on your laptop. Later in the course we’ll also use the Statistics Department cluster. You can also use the SCF JupyterHub or the campus DataHub to access a bash shell or run RStudio.\nThe software needed for the course is as follows:\n\nAccess to the UNIX command line (bash shell)\nGit\nR (RStudio is recommended but by no means required)\nPython (later in the course)\n\nWe have some tips for software installation (and access to DataHub), including suggestions for how to access a UNIX shell, which you’ll need to be able to do by the second week of class."
  },
  {
    "objectID": "syllabus.html#class-time",
    "href": "syllabus.html#class-time",
    "title": "Statistics 243 Fall 2022",
    "section": "Class time",
    "text": "Class time\nMy goal is to have classes be an interactive environment. This is both more interesting for all of us and more effective in learning the material. I encourage you to ask questions and will pose questions to the class to think about, respond to via Google forms, and discuss. To increase time for discussion and assimilation of the material in class, before some classes I may ask that you read material or work through tutorials in advance of class. Occasionally, I will ask you to submit answers to questions in advance of class as well.\nPlease do not use phones during class and limit laptop use to the material being covered.\nStudent backgrounds with computing will vary. For those of you with limited background on a topic, I encourage you to ask questions during class so I know what you find confusing. For those of you with extensive background on a topic (there will invariably be some topics where one of you will know more about it than I do or have more real-world experience), I encourage you to pitch in with your perspective. In general, there are many ways to do things on a computer, particularly in a UNIX environment and in R, so it will help everyone (including me) if we hear multiple perspectives/ideas.\nFinally, class recordings for review or to make up for absence will be available through the bCourses Media Gallery, available on the Media Gallery tab on the bCourses page for the class."
  },
  {
    "objectID": "syllabus.html#course-requirements-and-grading",
    "href": "syllabus.html#course-requirements-and-grading",
    "title": "Statistics 243 Fall 2022",
    "section": "Course requirements and grading",
    "text": "Course requirements and grading\n\nScheduling Conflicts\nCampus asks that I include this information about conflicts: Please notify me in writing by the second week of the term about any known or potential extracurricular conflicts (such as religious observances, graduate or medical school interviews, or team activities). I will try my best to help you with making accommodations, but I cannot promise them in all cases. In the event there is no mutually-workable solution, you may be dropped from the class.\nThe main conflict that would be a problem would be the quizzes, whose dates I will determine in late August / early September.\nQuizzes are in-person. There is no remote option, and the only make-up accommodations I will make are for illness or serious personal issues. Do not schedule any travel that may conflict with a quiz.\n\n\nCourse grades\nThe grade for this course is primarily based on assignments due every 1-2 weeks, two quizzes (likely in early-mid October and mid-late November), and a final group project. I will also provide extra credit questions on some problem sets. There is no final exam. 50% of the grade is based on the problem sets, 25% on the quizzes, 15% on the project, and 10% on your participation in discussions on Ed, your responses to the in-class Google forms questions, as well as occasional brief questions that I will ask you to answer in advance of the next class.\nGrades will generally be As and Bs. An A involves doing all the work, getting full credit on most of the problem sets, doing well on the quizzes, and doing a thorough job on the final project.\n\n\nProblem sets\nWe will be less willing to help you if you come to our office hours or post a question online at the last minute. Working with computers can be unpredictable, so give yourself plenty of time for the assignments.\nThere are several rules for submitting your assignments.\n\nYou should prepare your assignments using either R Markdown (or the new Quarto format) or LaTeX plus knitr.\nProblem set submission consists of both of the following:\n\nA PDF submitted electronically through Gradescope, by the start of class (10 am) on the due date, and\nAn electronic copy of the PDF, code file, and R Markdown/Quarto/knitr document pushed to your class GitHub repository, following the instructions to be provided by the GSI.\n\nOn-time submission will be determined based on the time stamp of when the PDF is submitted to Gradescope.\nAnswers should consist of textual response or mathematical expressions as appropriate, with key chunks of code embedded within the document. Extensive additional code can be provided as an appendix. Before diving into the code for a problem, you should say what the goal of the code is and your strategy for solving the problem. Raw code without explanation is not an appropriate solution. Please see our qualitative grading rubric for guidance. In general the rubric is meant to reinforce good coding practices and high-quality scientific communication.\nAny mathematical derivations may be done by hand and scanned with your phone if you prefer that to writing up LaTeX equations.\n\nNote: R Markdown is an extension to the Markdown markup language that allows one to embed R code within an HTML document. Quarto is a new tool that generalizes R Markdown and provides the compatible qmd format. knitr is a tool that allows one to embed chunks of code within LaTeX documents, including with Overleaf and the LyX GUI front-end to LaTeX. Please see the SCF dynamics document tutorial; there will be additional information in the first section and on the first problem set.\n\n\nSubmitting assignments\nIn the first section (September 2), we’ll discuss how to submit your problem sets both on Gradescope and via your class GitHub repository, located at https://github.berkeley.edu/<your_calnet_username>.\n\n\nProblem set grading\nThe grading scheme for problem sets is as follows. Each problem set will receive a numeric score for (1) presentation and explanation of results, (2) technical accuracy of code or mathematical derivation, and (3) code quality/style and creativity. For each of these three components, the possible scores are:\n\n0 = no credit,\n1 = partial credit (you did some of the problems but not all),\n2 = satisfactory (you tried everything but there were pieces of what you did that didn’t solve or present/explain one or more problems in a complete way), and\n3 = full credit.\n\nAgain, the qualitative grading rubric provides guidance on what we want to see for full credit.\nFor components #1 and #3, many of you will get a score of 2 for some problem sets as you develop good coding practices. You can still get an A in the class despite this.\nYour total score for the PS is a weighted sum of the scores for the three components. If you turn in a PS late, I’ll bump you down by two points (out of the available). If you turn it in really late (e.g., after we start grading them), I will bump you down by four points. No credit after solutions are distributed.\n\n\nFinal project\nThe final project will be a joint coding project in groups of 3-4. I’ll assign an overall task, and you’ll be responsible for dividing up the work, coding, debugging, testing, and documentation. You’ll need to use the Git version control system for working in your group.\n\n\nRules for working together and the campus honor code\nI encourage you to work together and help each other out. However, the problem set solutions you submit must be your own. What do I mean by that?\n\nYou must first try to figure out a given problem on your own. After that, if you’re stuck or want to explore alternative approaches or check what you’ve done, feel free to consult with your fellow students and with the GSI and me.\nWhat does “consult with a fellow student mean”? You can discuss a problem with another student, brainstorm approaches, and share code syntax (generally not more than one line) on how to do small individual coding tasks within a problem.\n\nYou should not ask another student for complete code or solutions, or look at their code/solution.\nYou should not share complete code or solutions with another student or on Ed Discussion.\n\nYou must provide attribution for ideas obtained elsewhere, including other students.\n\nIf you got a specific idea for how to do part of a problem from a fellow student (or some other resource), you should note that in your solution in the appropriate place (for specific syntax ideas, note this in a code comment), just as you would cite a book or URL.\nYou MUST note on your problem set solution any fellow students who you worked/consulted with.\nYou do not need to cite any Ed Discussion posts nor any discussions with Chris or James.\n\nUltimately, your solution to a problem set (writeup and code) must be your own, and you’ll hear from me if either look too similar to someone else’s.\n\nPlease see the last section of this document for more information on the Campus Honor Code, which I expect you to follow."
  },
  {
    "objectID": "syllabus.html#feedback",
    "href": "syllabus.html#feedback",
    "title": "Statistics 243 Fall 2022",
    "section": "Feedback",
    "text": "Feedback\nI welcome comments and suggestions and concerns. Particularly good suggestions will count towards your class participation grade."
  },
  {
    "objectID": "syllabus.html#accomodations-for-students-with-disabilities",
    "href": "syllabus.html#accomodations-for-students-with-disabilities",
    "title": "Statistics 243 Fall 2022",
    "section": "Accomodations for Students with Disabilities",
    "text": "Accomodations for Students with Disabilities\nPlease see me as soon as possible if you need particular accommodations, and we will work out the necessary arrangements."
  },
  {
    "objectID": "syllabus.html#campus-honor-code",
    "href": "syllabus.html#campus-honor-code",
    "title": "Statistics 243 Fall 2022",
    "section": "Campus Honor Code",
    "text": "Campus Honor Code\nThe following is the Campus Honor Code. With regard to collaboration and independence, please see my rules regarding problem sets above – Chris.\nThe student community at UC Berkeley has adopted the following Honor Code: “As a member of the UC Berkeley community, I act with honesty, integrity, and respect for others.” The hope and expectation is that you will adhere to this code.\nCollaboration and Independence: Reviewing lecture and reading materials and studying for exams can be enjoyable and enriching things to do with fellow students. This is recommended. However, unless otherwise instructed, homework assignments are to be completed independently and materials submitted as homework should be the result of one’s own independent work.\nCheating: A good lifetime strategy is always to act in such a way that no one would ever imagine that you would even consider cheating. Anyone caught cheating on a quiz or exam in this course will receive a failing grade in the course and will also be reported to the University Center for Student Conduct. In order to guarantee that you are not suspected of cheating, please keep your eyes on your own materials and do not converse with others during the quizzes and exams.\nPlagiarism: To copy text or ideas from another source without appropriate reference is plagiarism and will result in a failing grade for your assignment and usually further disciplinary action. For additional information on plagiarism and how to avoid it, see, for example: http://gsi.berkeley.edu/teachingguide/misconduct/prevent-plag.html\nAcademic Integrity and Ethics: Cheating on exams and plagiarism are two common examples of dishonest, unethical behavior. Honesty and integrity are of great importance in all facets of life. They help to build a sense of self-confidence, and are key to building trust within relationships, whether personal or professional. There is no tolerance for dishonesty in the academic world, for it undermines what we are dedicated to doing – furthering knowledge for the benefit of humanity.\nYour experience as a student at UC Berkeley is hopefully fueled by passion for learning and replete with fulfilling activities. And we also appreciate that being a student may be stressful. There may be times when there is temptation to engage in some kind of cheating in order to improve a grade or otherwise advance your career. This could be as blatant as having someone else sit for you in an exam, or submitting a written assignment that has been copied from another source. And it could be as subtle as glancing at a fellow student’s exam when you are unsure of an answer to a question and are looking for some confirmation. One might do any of these things and potentially not get caught. However, if you cheat, no matter how much you may have learned in this class, you have failed to learn perhaps the most important lesson of all."
  },
  {
    "objectID": "labs/02/assertionsAndTesting.html",
    "href": "labs/02/assertionsAndTesting.html",
    "title": "Assertions and Testing",
    "section": "",
    "text": "PDF"
  },
  {
    "objectID": "labs/02/assertionsAndTesting.html#references-and-useful-links",
    "href": "labs/02/assertionsAndTesting.html#references-and-useful-links",
    "title": "Assertions and Testing",
    "section": "References and useful links",
    "text": "References and useful links\n\nTesting section of the R packages tutorial by Hadley Wickham\nGitHub for assertthat package by Hadley Wickham\nAssertions and testing tutorial in Python"
  },
  {
    "objectID": "labs/02/assertionsAndTesting.html#learning-objectives",
    "href": "labs/02/assertionsAndTesting.html#learning-objectives",
    "title": "Assertions and Testing",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nUnderstand the benefits of assertions and testing as well as the differences between the two.\nIntroduction to the R package assertthat.\nIntroduction to the R package testthat.\nPractice writing assertions and tests on your own."
  },
  {
    "objectID": "labs/02/assertionsAndTesting.html#purpose-of-assertions-and-testing",
    "href": "labs/02/assertionsAndTesting.html#purpose-of-assertions-and-testing",
    "title": "Assertions and Testing",
    "section": "Purpose of Assertions and Testing",
    "text": "Purpose of Assertions and Testing\nWe all want our code to be correct the first time we write it. The unfortunate reality is that we all make mistakes when coding, either because of “silly mistakes” (indexing errors, incorrect syntax, using a wrong variable name, etc.) or because of a fundamental misunderstanding of the problem we are trying to solve. While print statements and writing test cases can help reduce coding errors, it is desirable to have a formal, structured way to test our code to ensure that it is functioning how we want it to. It is here that the assertthat and testthat packages in R prove useful."
  },
  {
    "objectID": "labs/02/assertionsAndTesting.html#assertion-vs.-testing",
    "href": "labs/02/assertionsAndTesting.html#assertion-vs.-testing",
    "title": "Assertions and Testing",
    "section": "Assertion vs. Testing",
    "text": "Assertion vs. Testing\nAssertions check the internal state of a function. For example, consider a function add(x, y) which returns x + y. The function assumes x is numeric, and an assertion within the body of the function would confirm that this is in the case and return an error if not.\nOn the other hand, tests (sometimes referred to as “unit tests”) check that a function produces the expected output for various inputs. For example, ensuring that add(1, 2) returns the number 3. Tests may include checks that assertions are working properly, for example by confirming that an error is thrown when the user calls add(\"potato\", 2).\nTests and assertions are similar in that,\n\nBoth are part of ensuring programs run correctly and aspects of defensive programming.\nBoth should check small pieces of the code while providing useful error messages, so they tell you exactly where the issue arises.\n\nHere’s a summary table comparing the two:\n\n\n\n\n\n\n\nAssertions\nTests\n\n\n\n\nTake the perspective of the developer.\nTake the perspective of the user.\n\n\nAssert that the developer knows what they’re doing.\nTest what the user can do.\n\n\nCheck internal function states.\nCheck function results given a specific input.\n\n\nTypically found within a function alongside source code.\nTypically kept in a directory separate from source code.\n\n\nCan run every time the code is called.\nRun periodically during development at specific moments (e.g., before creating a git commit).\n\n\nAmount of computation should be limited to avoid slowing down source code.\nFor large packages with many functions, can take many minutes or even hours to run.\n\n\nShould depend only on local states (e.g., the function’s arguments and internal variables).\nCan depend on global state (e.g., global options).\n\n\nDouble as inline documentation of source code.\nCan be helpful for the code design process (i.e., test-driven development)."
  },
  {
    "objectID": "labs/02/assertionsAndTesting.html#three-main-functions-assert_that-see_if-and-validate_that",
    "href": "labs/02/assertionsAndTesting.html#three-main-functions-assert_that-see_if-and-validate_that",
    "title": "Assertions and Testing",
    "section": "Three main functions: assert_that, see_if, and validate_that",
    "text": "Three main functions: assert_that, see_if, and validate_that\nThese are the three primary functions from the package:\n\nassert_that() signals (i.e., throws) an error. This is primarily what you will use in your functions.\nsee_if() returns a logical value, with the error message as an attribute, but no error is thrown.\nvalidate_that() returns TRUE on success and otherwise returns the error as a string.\n\nHere is an example of the differences. When the assertion is TRUE they all return TRUE and continue with the execution of the function.\n\n# these functions will help us see the differences in assertthat's functions\nreturnStringAssert <- function(x){\n  assert_that(is.string(x))\n\n  return(x)\n}\nreturnStringSeeIf <- function(x){\n  see_if(is.string(x))\n\n  return(x)\n}\nreturnStringValidate <- function(x){\n  validate_that(is.string(x))\n\n  return(x)\n}\n\nreturnStringAssert(\"a\")\n\n[1] \"a\"\n\nreturnStringSeeIf(\"a\")\n\n[1] \"a\"\n\nreturnStringValidate(\"a\")\n\n[1] \"a\"\n\n\nWhen the assertion is FALSE the functions have different behaviors. assert_that() will throw an error, halting furthering execution of the function immediately. see_if() and validate_that() will not stop the execution, allowing the function to continue with bad state.\n\nreturnStringAssert(c(\"a\", \"b\"))\n\nError: x is not a string (a length one character vector).\n\nreturnStringSeeIf(c(\"a\", \"b\"))\n\n[1] \"a\" \"b\"\n\nreturnStringValidate(c(\"a\", \"b\"))\n\n[1] \"a\" \"b\"\n\n\nHowever, all three will give the error message\nassert_that signals an error:\n\nassert_that(is.string(c(\"a\", \"b\")))\n\nError: c(\"a\", \"b\") is not a string (a length one character vector).\n\n\nsee_if returns FALSE with the error message as an attribute:\n\nsee_if_result <- see_if(is.string(c(\"a\", \"b\")))\nsee_if_result\n\n[1] FALSE\nattr(,\"msg\")\n[1] \"c(\\\"a\\\", \\\"b\\\") is not a string (a length one character vector).\"\n\nattr(see_if_result, \"msg\")\n\n[1] \"c(\\\"a\\\", \\\"b\\\") is not a string (a length one character vector).\"\n\n\nvalidate_that returns the error message as a string:\n\nvalidate_that(is.string(c(\"a\", \"b\")))\n\n[1] \"c(\\\"a\\\", \\\"b\\\") is not a string (a length one character vector).\"\n\n\nWhile in general assert_that() is likely to be your go-to, you might prefer to use see_if() or validate_that() in cases where you first want to inspect the error message and perhaps check other aspects of your function’s state before eventually signaling an error (e.g., using stop() with a custom message) so that function execution does not continue with the bad state.\n\nerr_msg <- attr(see_if_result, \"msg\")\nstop(\"see_if() returned FALSE because \", err_msg, call. = FALSE)\n\nError: see_if() returned FALSE because c(\"a\", \"b\") is not a string (a length one character vector)."
  },
  {
    "objectID": "labs/02/assertionsAndTesting.html#writing-your-own-assertions",
    "href": "labs/02/assertionsAndTesting.html#writing-your-own-assertions",
    "title": "Assertions and Testing",
    "section": "Writing Your Own Assertions",
    "text": "Writing Your Own Assertions\nWhile you could use see_if() or validate_that() to create custom error messages as in the previous example, assertthat already provides a couple of ways to do so.\nThe first is by adding a new assertion that checks whether the number is odd and add a custom message directly to the assertion:\n\nis_odd <- function(x) {\n  # your custom assertion checking functions can have their own assertions!\n  # you can check multiple conditions by separating them with a ,\n  assert_that(is.numeric(x), length(x) == 1)\n\n  # here is the main assertion\n  assert_that(x %% 2 == 1, msg = paste(\"x =\", x, \"is even\"))\n}\n\nassert_that(is_odd(2))\n\nError: x = 2 is even\n\n\nThe second is to using the on_failure() function, which allows you to use more complex logic to create your assertion failure messages. Below is an example of how this works:\n\nis_odd2 <- function(x) {\n  assert_that(is.numeric(x), length(x) == 1)\n  x %% 2 == 1\n}\nattributes(is_odd2)\n\n$srcref\nfunction(x) {\n  assert_that(is.numeric(x), length(x) == 1)\n  x %% 2 == 1\n}\n\nassert_that(is_odd2(2))\n\nError: is_odd2(x = 2) is not TRUE\n\non_failure(is_odd2) <- function(call, env) {\n  paste(\"x =\", deparse(call$x), \" is even\")\n}\nattributes(is_odd2)\n\n$srcref\nfunction(x) {\n  assert_that(is.numeric(x), length(x) == 1)\n  x %% 2 == 1\n}\n\n$fail\nfunction(call, env) {\n  paste(\"x =\", deparse(call$x), \" is even\")\n}\n\nassert_that(is_odd2(2))\n\nError: x = 2  is even\n\n\nThe assertions from our original is_odd() function flow through the function we assigned to the fail attribute of is_odd() by using on_failure(), so we still get the appropriate error messages when we pass a non-numeric or vector value to is_odd()."
  },
  {
    "objectID": "labs/02/assertionsAndTesting.html#some-additional-useful-assertions",
    "href": "labs/02/assertionsAndTesting.html#some-additional-useful-assertions",
    "title": "Assertions and Testing",
    "section": "Some Additional Useful Assertions",
    "text": "Some Additional Useful Assertions\nassertthat provides a few additional assertions above and beyond what base R provides that can be quite useful:\n\nis.flag(x): is x TRUE or FALSE? (a boolean flag)\nis.string(x): is x a length 1 character vector?\nhas_name(x, nm), x %has_name% nm: does x have component nm?\nhas_attr(x, attr), x %has_attr% attr: does x have attribute attr?\nis.count(x): is x a single positive integer?\nare_equal(x, y): are x and y equal?\nnot_empty(x): are all dimensions of x greater than 0?\nnoNA(x): is x free from missing values?\nis.dir(path): is path a directory?\nis.writeable(path)/is.readable(path): is path writeable/readable?\nhas_extension(path, extension): does file have given extension?"
  },
  {
    "objectID": "labs/02/assertionsAndTesting.html#list-of-common-testthat-expectations",
    "href": "labs/02/assertionsAndTesting.html#list-of-common-testthat-expectations",
    "title": "Assertions and Testing",
    "section": "List of Common testthat Expectations",
    "text": "List of Common testthat Expectations\n\n\n\n\n\n\n\nFunction\nDescription\n\n\n\n\nexpect_true(x)\nexpects that x is TRUE\n\n\nexpect_false(x)\nexpects that x is FALSE\n\n\nexpect_null(x)\nexpects that x is NULL\n\n\nexpect_type(x)\nexpects that x is of type y\n\n\nexpect_is(x, y)\nexpects that x is of class y\n\n\nexpect_length(x, y)\nexpects that x is of length y\n\n\nexpect_equal(x, y)\nexpects that x is equal to y\n\n\nexpect_equivalent(x, y)\nexpects that x is equivalent to y\n\n\nexpect_identical(x, y)\nexpects that x is identical to y\n\n\nexpect_lt(x, y)\nexpects that x is less than y\n\n\nexpect_gt(x, y)\nexpects that x is greater than y\n\n\nexpect_lte(x, y)\nexpects that x is less than or equal to y\n\n\nexpect_gte(x, y)\nexpects that x is greater than or equal y\n\n\nexpect_named(x)\nexpects that x has names y\n\n\nexpect_matches(x, y)\nexpects that x matches y (regex)\n\n\nexpect_message(x, y)\nexpects that x gives message y\n\n\nexpect_warning(x, y)\nexpects that x gives warning y\n\n\nexpect_error(x, y)\nexpects that x throws error y"
  },
  {
    "objectID": "labs/02/assertionsAndTesting.html#motivating-example",
    "href": "labs/02/assertionsAndTesting.html#motivating-example",
    "title": "Assertions and Testing",
    "section": "Motivating Example",
    "text": "Motivating Example\nTo understand how testthat works, we will consider the standardize() function, which takes a vector x, subtracts the mean of the vector, and then divides by the standard deviation. Notice the assertions in the function to check pre-conditions!\n\nstandardize <- function(x, na.rm = FALSE) {\n  # assertions on input\n  assert_that(is.vector(x))\n  assert_that(is.flag(na.rm))\n\n  # do computation\n  z <- (x - mean(x, na.rm = na.rm)) / sd(x, na.rm = na.rm)\n  return(z)\n}\n\n\nInformal testing\nWhen writing a function, the informal process of testing usually looks something like this, executed line-by-line in the R console:\n\na <- c(2, 4, 7, 8, 9)\nz <- standardize(a)\nz\n\n[1] -1.3719887 -0.6859943  0.3429972  0.6859943  1.0289915\n\n\nThen you might look at the mean and standard deviation of z to see if standardize() appears to be working as expected:\n\n# zero mean\nmean(z)\n\n[1] 0\n\n# unit std-dev\nsd(z)\n\n[1] 1\n\n\nNext, you might keep testing the function with more extreme cases:\n\ny <- c(1, 2, 3, 4, NA)\nstandardize(y)\n\n[1] NA NA NA NA NA\n\nstandardize(y, na.rm = TRUE)\n\n[1] -1.1618950 -0.3872983  0.3872983  1.1618950         NA\n\n\nAnd so on for different types of inputs:\n\nalog <- c(TRUE, FALSE, FALSE, TRUE)\nstandardize(alog)\n\n[1]  0.8660254 -0.8660254 -0.8660254  0.8660254\n\n\nThis approach is fine and encouraged for interactive development, but don’t waste all this energy! Hold on to your testing code for a rainy day."
  },
  {
    "objectID": "labs/02/assertionsAndTesting.html#using-testthat-expectations",
    "href": "labs/02/assertionsAndTesting.html#using-testthat-expectations",
    "title": "Assertions and Testing",
    "section": "Using testthat expectations",
    "text": "Using testthat expectations\nInstead of just writing a list of more or less informal tests in the R console, we are going to use the functions provide by testthat.\nTo learn about the testing functions, we’ll consider the following test inputs:\n\nx <- c(1, 2, 3)\ny <- c(1, 2, NA)\nw <- c(TRUE, FALSE, TRUE)\nq <- letters[1:3]\n\n\nThe “happy path”: Testing with “normal” input\nThe core of testthat consists of expectations; to write expectations you use functions from the testthat package starting with expect_ such as expect_equal(), expect_integer() or expect_error().\n\nx <- c(1, 2, 3)\nz <- (x - mean(x)) / sd(x)\n\nexpect_equal(standardize(x), z)\nexpect_length(standardize(x), length(x))\nexpect_type(standardize(x), 'double')\n\nNotice that when an expectation runs successfully, nothing appears to happen. But that’s good news. If an expectation fails, you’ll typically get an error, here are some failed tests:\n\n# different expected output\nexpect_equal(standardize(x), x)\n\nError: standardize(x) not equal to `x`.\n3/3 mismatches (average diff: 2)\n[1] -1 - 1 == -2\n[2]  0 - 2 == -2\n[3]  1 - 3 == -2\n\n\n\n# different expected length\nexpect_length(standardize(x), 2)\n\nError: standardize(x) has length 3, not length 2.\n\n\n\n# different expected type\nexpect_type(standardize(x), 'character')\n\nError: standardize(x) has type 'double', not 'character'.\n\n\n\n\nEdge cases: Testing function robustness\nIt’s important to be creative when testing and get into the mindset of the user of your code. You might be the only user, but your perspective when developing code vs. when you use it later on are not one in the same. Think about the range of inputs the user might give your functions and how your function should behave in cases that don’t fall directly on the happy path.\n\nTesting inputs with NA\nLet’s include a vector with missing values, which we want to handle.\n\ny <- c(1, 2, NA)\nz1 <- (y - mean(y, na.rm = FALSE)) / sd(y, na.rm = FALSE)\nz2 <- (y - mean(y, na.rm = TRUE)) / sd(y, na.rm = TRUE)\n\nexpect_equal(standardize(y), z1)\nexpect_length(standardize(y), length(y))\nexpect_equal(standardize(y, na.rm = TRUE), z2)\nexpect_length(standardize(y, na.rm = TRUE), length(y))\nexpect_type(standardize(y), 'double')\n\n\n\nTesting with logical input\nLet’s now test standardize() with a logical vector:\n\nw <- c(TRUE, FALSE, TRUE)\nz <- (w - mean(w)) / sd(w)\n\nexpect_equal(standardize(w), z)\nexpect_length(standardize(w), length(w))\nexpect_type(standardize(w), 'double')\n\nYou may be able to think of other edge cases that would be helpful to test for this function. While it is not practical to attempt to write a test for every possible input, you should think carefully about how your users may reasonably interact with your code and what sorts of inputs may lead to unexpected bugs or shortcomings even when the inputs are reasonable.\n\n\n\nTesting for expected errors\nWhile we might assume that standardize() will already rightly throw an error when given the character vector q, it’s still a good idea to test for the failures that we expect. This is especially important for catching bad bugs where an error should be thrown but isn’t, allowing function execution to continue with incorrect state. Even if we try to guard against bad state with assertions, testing can still help catch any incorrect logic in those assertions.\nHere’s the current error we get when calling standardize() with character vector input. (Notice the warning signaled by using mean() on a character vector.)\n\nq <- letters[1:3]\nstandardize(q)\n\nWarning in mean.default(x, na.rm = na.rm): argument is not numeric or logical:\nreturning NA\n\n\nError in x - mean(x, na.rm = na.rm): non-numeric argument to binary operator\n\n\nWe can confirm that an error is signaled using expect_error():\n\nexpect_error(standardize(q))\n\nWarning in mean.default(x, na.rm = na.rm): argument is not numeric or logical:\nreturning NA\n\n\n\n\n\n\n\n\nAlways use the regexp argument when using expect_error()\n\n\n\nUsing expect_error() without the regexp argument, as in the above example, is almost never what you want!\n\n\nexpect_error() will pass if any error is signaled, including ones that we aren’t expecting! Therefore, it’s good practice to use the regexp argument in expect_error() to match the error message you expect to see. In this case, we’ll also use fixed = TRUE to exactly match the error message, rather than use a more general regular expression.\n\nexpected_msg <- \"non-numeric argument to binary operator\"\nexpect_error(standardize(q), regexp = expected_msg, fixed = TRUE)\n\nWarning in mean.default(x, na.rm = na.rm): argument is not numeric or logical:\nreturning NA\n\n\nTogether with the class and ... arguments (see ?testthat::expect_error), it’s possible to create very sophisticated logic for matching specific errors."
  },
  {
    "objectID": "labs/02/assertionsAndTesting.html#combining-multiple-expectations-into-a-test-with-test_that",
    "href": "labs/02/assertionsAndTesting.html#combining-multiple-expectations-into-a-test-with-test_that",
    "title": "Assertions and Testing",
    "section": "Combining multiple expectations into a test with test_that()",
    "text": "Combining multiple expectations into a test with test_that()\nNow that you’ve seen how the expectation functions work, the next thing to talk about is the function test_that() which you’ll use to group a set of expectations.\nLooking at the previous test examples with the normal input vector, all the expectations can be wrapped inside a call to test_that(). The first argument of test_that() is a string indicating what is being tested, followed by an R expression with the expectations.\n\ntest_that(\"standardize works with normal input\", {\n  x <- c(1, 2, 3)\n  z <- (x - mean(x)) / sd(x)\n\n  expect_equal(standardize(x), z)\n  expect_length(standardize(x), length(x))\n  expect_type(standardize(x), 'double')\n})\n\nTest passed 😀\n\n\nLikewise, all the expectations with the vector containing missing values can be wrapped inside another call to test_that() like this:\n\ntest_that(\"standardize works with missing values\", {\n  y <- c(1, 2, NA)\n  z1 <- (y - mean(y, na.rm = FALSE)) / sd(y, na.rm = FALSE)\n  z2 <- (y - mean(y, na.rm = TRUE)) / sd(y, na.rm = TRUE)\n\n  expect_equal(standardize(y), z1)\n  expect_length(standardize(y), length(y))\n  expect_equal(standardize(y, na.rm = TRUE), z2)\n  expect_length(standardize(y, na.rm = TRUE), length(y))\n  expect_type(standardize(y), 'double')\n})\n\nTest passed 🥇\n\n\nAnd last, but not least, the expectations with the logical vector can be grouped in a test_that() call:\n\ntest_that(\"standardize handles logical vector\", {\n  w <- c(TRUE, FALSE, TRUE)\n  z <- (w - mean(w)) / sd(w)\n\n  expect_equal(standardize(w), z)\n  expect_length(standardize(w), length(w))\n  expect_type(standardize(w), 'double')\n})\n\nTest passed 🎊"
  },
  {
    "objectID": "labs/02/assertionsAndTesting.html#running-tests",
    "href": "labs/02/assertionsAndTesting.html#running-tests",
    "title": "Assertions and Testing",
    "section": "Running tests",
    "text": "Running tests\nThe formal way to implement the tests is to include them in a separate R script file, e.g. tests-function-name.R. Then you\nIf your working directory is the sections/03/ directory, then you could run the tests in tests-standardize.R from the R console using the function test_file()\n\n# (assuming that your working directory is \"sections/03/\")\n# run from R console\ntest_file(\"tests/tests-standardize.R\")\n\n\n══ Testing tests-standardize.R ═════════════════════════════════════════════════\n\n[ FAIL 0 | WARN 0 | SKIP 0 | PASS 0 ]\n[ FAIL 0 | WARN 0 | SKIP 0 | PASS 1 ]\n[ FAIL 0 | WARN 0 | SKIP 0 | PASS 2 ]\n[ FAIL 0 | WARN 0 | SKIP 0 | PASS 3 ]\n[ FAIL 0 | WARN 0 | SKIP 0 | PASS 4 ]\n[ FAIL 0 | WARN 0 | SKIP 0 | PASS 5 ]\n[ FAIL 0 | WARN 0 | SKIP 0 | PASS 6 ]\n[ FAIL 0 | WARN 0 | SKIP 0 | PASS 7 ]\n[ FAIL 0 | WARN 0 | SKIP 0 | PASS 8 ]\n[ FAIL 0 | WARN 0 | SKIP 0 | PASS 9 ]\n[ FAIL 0 | WARN 0 | SKIP 0 | PASS 10 ]\n[ FAIL 0 | WARN 0 | SKIP 0 | PASS 11 ] Done!\n\n\nWe see that all 11 of the tests were passed, so it seems like our function is working as expected.\nTo see what the output of test_file() looks like when tests fail I included a version of standarize which adds a 1 to the end of function called standarizeWrong in the functions.R file. In this case we expect the tests to fail and that is what we see:\n\n# (assuming that your working directory is \"sections/03/\")\n# run from R console\ntest_file(\"tests/tests-standardize-wrong.R\")\n\n\n══ Testing tests-standardize-wrong.R ═══════════════════════════════════════════\n\n[ FAIL 0 | WARN 0 | SKIP 0 | PASS 0 ]\n[ FAIL 1 | WARN 0 | SKIP 0 | PASS 0 ]\n[ FAIL 1 | WARN 0 | SKIP 0 | PASS 1 ]\n[ FAIL 1 | WARN 0 | SKIP 0 | PASS 2 ]\n[ FAIL 1 | WARN 0 | SKIP 0 | PASS 3 ]\n[ FAIL 1 | WARN 0 | SKIP 0 | PASS 4 ]\n[ FAIL 2 | WARN 0 | SKIP 0 | PASS 4 ]\n[ FAIL 2 | WARN 0 | SKIP 0 | PASS 5 ]\n[ FAIL 2 | WARN 0 | SKIP 0 | PASS 6 ]\n[ FAIL 3 | WARN 0 | SKIP 0 | PASS 6 ]\n[ FAIL 3 | WARN 0 | SKIP 0 | PASS 7 ]\n[ FAIL 3 | WARN 0 | SKIP 0 | PASS 8 ]\n\n── Failure (tests-standardize-wrong.R:9:3): standardize works with normal input ──\nstandardizeWrong(x) not equal to `z`.\n3/3 mismatches (average diff: 1)\n[1] 0 - -1 == 1\n[2] 1 -  0 == 1\n[3] 2 -  1 == 1\n\n── Failure (tests-standardize-wrong.R:22:3): standardize works with missing values ──\nstandardizeWrong(y, na.rm = TRUE) not equal to `z2`.\n2/3 mismatches (average diff: 1)\n[1] 0.293 - -0.707 == 1\n[2] 1.707 -  0.707 == 1\n\n── Failure (tests-standardize-wrong.R:32:3): standardize handles logical vector ──\nstandardizeWrong(w) not equal to `z`.\n3/3 mismatches (average diff: 1)\n[1]  1.577 -  0.577 == 1\n[2] -0.155 - -1.155 == 1\n[3]  1.577 -  0.577 == 1\n\n\n[ FAIL 3 | WARN 0 | SKIP 0 | PASS 8 ]\n\n\nHere we see that 3 tests failed, namely that our output is not equal to the value that we expect it to be. This allows us to go back to the function and assess what may be going wrong."
  },
  {
    "objectID": "labs/01/intro_git_knitr.html",
    "href": "labs/01/intro_git_knitr.html",
    "title": "Introduction to git and knitr",
    "section": "",
    "text": "PDF\nIn this section we will learn and dicuss some tools that you will need to complete your first problem set, namely git and knitr (either Quarto, R Markdown, or Rtex). At the end there are also resources about code style and a couple notes to keep in mind when turning in your problem sets."
  },
  {
    "objectID": "labs/01/intro_git_knitr.html#getting-started-with-git-and-github",
    "href": "labs/01/intro_git_knitr.html#getting-started-with-git-and-github",
    "title": "Introduction to git and knitr",
    "section": "Getting started with Git and GitHub",
    "text": "Getting started with Git and GitHub\n\nLearning Objectives\n\nCreate a remote repository on GitHub\nCreate a local Git repository on your machine\nPractice adding, and committing changes to your (local) Git repo\nPractice pushing commited changes to a remote repo\n\n\n\nUseful Links\n\nA nice tutorial is available on the Berkeley SCF github repo\nUsing Git without having to enter your password over and over:\n\nOption 1: Use Git Credential Manager and HTTPS clone method, e.g.,\ngit clone https://github.com/berkeley-scf/tutorial-git-basics.git\nOption 2: Use SSH keys and SSH clone method, e.g.,\ngit clone git@github.com:berkeley-scf/tutorial-git-basics.git\nEither way, be sure to follow the instructions for your OS (Mac, Windows, or Linux) by selecting it at the top of the page.\n\ngit-scm.com has a number useful resources for learning Git, including:\n\nPro Git book\nCommand reference manual\n\n\n\n\nCreate a New GitHub Repository\nThere are two ways to start a repository:\n\nCreating the repository on GitHub using your browser and then use git clone.\nUsing git init on your machine and then link it to a remote server, e.g. GitHub.\n\n\n\n\n\n\n\nOption 1 is usually the right choice\n\n\n\n\n\nUse option 2. only if you already have a local repo and later decide you want to create a remote copy on a service such as GitHub. For this course, this won’t be necessary.\nBut if you do need to use git init, then when you first create a local repo on your machine using git init, the default branch name is master. However, the default branch when you create one on GitHub is called main, so if you later decide to add a remote copy of your local repo to GitHub, you will need to update the name of your default branch. After creating the blank repo on GitHub (no README.md or .gitignore), run the commands that GitHub provides in your local repo to change the default branch from master to main.\n\n\n\nToday, we’re going to cover option 1. by creating one online using Berkeley’s GitHub Enterprise instance:\n\nOpen your browser and Sign in to your github.berkeley.edu account.\nLocate the + button (next to your avatar).\nSelect the New repository option.\nChoose a name for your repository: e.g. demo-repo.\nIn the Description field add a brief description: e.g. “this is a demo repo”\nCheck “Add .gitignore” and type “R” to find one that is tailored to R projects. This stops items like .Rhistory files from being added to your repo.\nClick the green button Create repository.\n\n\n\nAdding a README file\nInitially, your repo is located on GitHub. To set it up locally, you must clone the repository from GitHub, e.g.:\n# replace `https://github.com/berkeley-scf/tutorial-git-basics` with\n# the URL / clone method (HTTPS or SSH) for your new repo\ngit clone https://github.com/berkeley-scf/tutorial-git-basics\nIt is customary to add a README.md file at the top level (note that a blank one was already added to your PS repo). This file must contain (at least) a description of what the repository is about. The following command will create a README.md file with some minimalist content:\necho \"# Demo Repo\" >> README.md\nNow you have a “new” file in your local repo, but this change has not been recorded by Git. You can confirm this by checking the status of the repo:\ngit status\nNotice that Git knows that README.md is untracked, so let’s add the changes to Git’s database:\ngit add README.md\nCheck the status of the repo again:\ngit status\nNow Git is tracking the file README.md. Next, the changes need to committed to the repository. You can use the -m option to write a message inline. It’s a good idea to keep your commit messages succinct but informative.\ngit commit -m \"Add README\"\nIn general, it is best to make frequent small commits rather than infrequent large commits. For example, you might want to make a single commit for each problem in a problem set, rather than a single commit for the full problem set.\n\n\n\n\n\n\nWarning\n\n\n\nIf you run git commit without -m, Git will launch a default text editor, usually vi or vim. If you this happens unintentionally, you can quit by typing ESC followed by : and then q! followed by Enter.\nYou can specify which editor Git should use using git config. Take a look at this documentation to learn how.\n\n\n\n\nExamining changes while editing\nOnce you create a commit, it becomes a part of your repo’s history. You can see the history using git log:\n# this usually opens in `less`\n# type `Q` to quit and return to the bash shell\ngit log\nLet’s make a change to README.md, which is now tracked by Git.\necho \"Nothing to see here.\" >> README.md\ngit status will show that README.md was modified, along with other useful information. But it won’t actually show the differences between your repo’s history and the current un-committed state. To see this, there is another extremely useful command:\n# this also typically opens in `less`\ngit diff\n\n# you can also check a specific file or directory\ngit diff README.md\n\n\nPushing changes to a remote repo\nNow that you have linked your local repo with your remote repo, you can start pushing (i.e., uploading) commits to GitHub. As part of the basic workflow with Git and GitHub, you want to frequently check the status of your repo:\ngit status\nNow let’s push your recent commit to the remote branch (origin) from the local branch (main):\ngit push origin main\nGo to your Github repository and refresh the browser. If everything went fine, you should be able to see the contents of your customized README.md file.\nIf you or a collaborator make changes on your remote repo you must pull the remote repo into your local repo before you attempt to push the changes you made locally.\ngit pull\nIn summary, if you want to use git add files to a GitHub repo, the steps are:\nStep 0: Pull the repo (This is necessary if your local repo is “out of date’’, meaning there are changes on the remote repo that are not present in your local repo. This is not necessary if your local repo contains the most recently made changes.)\nStep 1: Make edits to/create new files in your local repo. Check on status and _diff_erences frequently.\nStep 2: Add the files you edited/created.\nStep 3: Commit your update with a helpful message.\nStep 4: Push the update to the main branch.\n\n\nPractice exercises\n\n\n\n\n\n\nPractice exercise 0: Configure git and clone the course website repo\n\n\n\n\n\n\nUse git config to configure your identity, like the following example:\ngit config --global user.name \"Oski Bear\"\ngit config --global user.email oski@berkeley.edu\nFor more info on configuring Git, see here.\nIf you haven’t already, clone the GitHub repo for the course website:\n# HTTPS\ngit clone https://github.com/berkeley-stat243/stat243-fall-2022.git\n\n# SSH\ngit clone git@github.com:berkeley-stat243/stat243-fall-2022.git\nYou can find the source for this lab in labs/01/intro_git_knitr.Rmd\n\n\n\n\n\n\n\n\n\n\nPractice exercise 1: Add a .gitignore to your PS submission repo.\n\n\n\n\n\nNow that you know how to clone a repo, add files, commit, and push to GitHub, try the following on your laptop:\n\nClone your PS submission repo.\nCopy the R-tailored .gitignore file from the demo repo you created above to the root directory of your local copy of the repo.\ncd to your local repo and use git status to confirm that .gitignore is untracked, then add it and commit with an informative message.\nVisit GitHub’s gitignore repo, go to the “Global” subdirectory, and find the .gitignore template for your operating system (e.g., macOS.gitignore).\nAdd the contents of the OS-specific .gitignore template to the end of your PS submission repo’s .gitignore and save the file.\nUse a git command to check the changes you made.\nAdd the changes, commit with an informative message, and push them to the remote repo.\n\n\n\n\n\n\n\n\n\n\nPractice exercise 2: Clone your PS submission repo on the SCF.\n\n\n\n\n\nIn general, you can feel free to work with your repo on your laptop. But in case you need the code on the SCF, you can try the following steps:\n\nssh to one of the SCF machines.\n(Optional) Find / create an SSH key and add it to your account at github.berkeley.edu.\nClone your PS submission repo somewhere in your SCF home directory.\nOn your laptop, make some minor change to the README.md. For example, you can add a description line like “STAT 243 PS submission repo.”\nAdd, commit, and push the change.\nBack on the SCF, cd into your repo and run git pull to update that copy of your repo with the changes you just made on your laptop!"
  },
  {
    "objectID": "labs/01/intro_git_knitr.html#knitr-and-r-markdown-files",
    "href": "labs/01/intro_git_knitr.html#knitr-and-r-markdown-files",
    "title": "Introduction to git and knitr",
    "section": "knitr and R Markdown Files",
    "text": "knitr and R Markdown Files\n\nLearning Objectives:\n\nDifferentiate between .R and .Rmd files\nUnderstand dynamic documents\nGain familiarity with R Markdown .Rmd files\nGain familiarity with code chunks\n\n\n\nUseful Links\n\nSCF tutorial on dynamic documents\nknitr in a knutshell tutorial with information about R Markdown and knitr with LaTeX.\n\nR Markdown cheatsheet\nComplete R Markdown guide\nR Sweave tutorial\n\n\n\nOpening and knitting an Rmd file\nIn the menu bar of RStudio, click on File, then New File, and choose R Markdown. Select the default option (Document), and click OK. RStudio will open a new .Rmd file in the source pane. And you should be able to see a file with some default content.\nLocate the button Knit, the one with an icon of a ball of yarn and two needles. Click the button (knit to HTML) so you can see how Rmd files are rendered and displayed as HTML documents. Alternatively, you can use a keyboard shortcut: in Mac Command+Shift+K, in Windows and Linux Ctrl+Shift+K\n\n\nWhat is an Rmd file?\nRmd files are a special type of file, referred to as a dynamic document. This is the fancy term we use to describe a document that allows us to combine narrative (text) with R code in one single file.\nRmd files are plain text files. This means that you can open an Rmd file with any text editor (not just RStudio) to see and edit its contents.\nThe main idea behind dynamic documents is simple yet very powerful: instead of working with two separate files, one that contains the R code, and another one that contains the narrative, you use an .Rmd file to include both the commands and the narrative.\nOne of the main advantages of this paradigm is that you avoid having to copy results from your computations and paste them into a report file. In fact, there are more complex ways to work with dynamic documents and source files. But the core idea is the same: combine narrative and code in a way that lets the computer do the manual, repetitive, and time consuming job.\nRmd is just one type of dynamic document that you will find in RStudio. In fact, RStudio provides other file formats that can be used as dynamic documents: e.g., .Rnw (R Sweave), .Rhtml (R HTML), and .qmd (Quarto).\n\n\nAnatomy of an Rmd file\nThe structure of an .Rmd file can be divided in two parts: 1) a YAML header (also called YAML metadata), and 2) the body of the document. In addition to this structure, you should know that .Rmd files typically use three types of syntaxes: YAML, Markdown, and R.\nThe YAML header consists of the first few lines at the top of the file. This header is established by a set of three dashes --- as delimiters (one starting set, and one ending set). This part of the file requires you to use YAML syntax (Yet Another Markup Language.) Within the delimiter sets of dashes, you specify settings (or metadata) that will apply to the entire document. Some of the common options are things like:\n\ntitle\nauthor\ndate\noutput\n\nThe body of the document is everything below the YAML header. It consists of a mix of narrative and R code. All the text that is narrative is written in a markup syntax called Markdown (although you can also use LaTeX math notation). In turn, all the text that is code is written in R syntax inside blocks of code (see below).\n\n\nHow does an Rmd file work?\nRmd files are plain text files. All that matters is the syntax of its content. The content is basically divided in the header, and the body.\n\nThe header uses YAML syntax.\nThe narrative in the body uses Markdown syntax.\nThe code and commands use R syntax.\n\nThe process to generate a nice rendered document from an Rmd file is known as knitting. When you knit an Rmd file, various R packages and programs run behind the scenes. But the process can be broken down in three main phases: 1) Parsing, 2) Execution, and 3) Rendering.\nParsing refers to examining line by line to identified components as yaml header, or as markdown text, or as R code.\nEach component receives a special treatment and formatting.\nThe most interesting part is in the pieces of text that are R code. Those are separated and executed if necessary. The commands may be included in the final document. Also, the output may be included in the final document. Sometimes, nothing is executed nor included.\nDepending on the specified output format (e.g. HTML, pdf, word), all the components are assembled, and one single document is generated.\n\n\nYet Another Syntax to Learn\nR markdown (Rmd) files use markdown as the main syntax to write content.Markdown is a very lightweight type of markup language, and it is relatively easy to learn.\nOne of the most common sources of confusion when learning about R and Rmd files has to do with the hash symbol #. As you know, # is the character used by R to indicate comments. The issue is that the # character has a different meaning in markdown syntax. Hashes in markdown are used to define levels of headings.\nIn an Rmd file, a hash # that is inside a code chunk will be treated as an R comment. A hash outside a code chunk, will be treated as markdown syntax, making its associated text a given type of heading.\n\n\nBlocks of code: Code chunks vs. inline code\nThere are two types of blocks of code:\n\ncode chunks\ninline code.\n\n\nInline code\nInline code is code inserted within a line of narrative text between backticks (`). For example, here’s a small bit of raw Markdown:\n**Important**: Edit your `README.md`.\nAnd here’s what that looks like when rendered:\nImportant: Edit your README.md.\nOftentimes, inline code blocks are simply a way to improve the formatting of our document and aren’t used for computation, but they can run R code using the following syntax:\nI like to eat `r pi`.\nResult: I like to eat 3.1415927.\nThis is helpful when you’re describing results and don’t want to hard-code a number or the amount of repititions you ran. Instead, you can use variables from previous code chunks in the document.\n\n\nCode chunks\nThese are the most important blocks of code because they typically include the R computations that will run when you knit the document.\nCode chunks are lines of text (typically in the syntax of a specific programming language such as R) separated from any lines of narrative (in Markdown syntax) by the special Markdown syntax ``` (three backticks). For example:\n```{r my-chunk, echo = FALSE}\n\npar(mfrow = c(2, 3))\n\nfor (n in 10^seq(0, 5)) {\n    x <- rbinom(n, 100, 0.5)\n    hist(x)\n}\n\npar(mfrow = c(1, 1))\n\n```\nThe opening backticks are followed by curly braces, the language (r for R), an optional (but highly recommended) name for the chunk (my-chunk here), and then comma-separated chunk options (see below).\nNote the final three backticks at the bottom of the chunk – these close the chunk and allow you to return to the narrative in Markdown. By adding the above example to an .Rmd, you would see the following output:\n\n\n\n\n\nThe details aren’t super important, but for those that are curious here is what happens when we run Knit:\n\nknitr evaluates the chunks that start with ```{r} and inserts the results into a intermediary markdown file.\nIt then calls Pandoc which converts the intermediary markdown to the next format (possibly the output), repeating until the output format specified in the YAML header is reached.\nFor example, for PDF output, the sequence is: .Rmd -> .md -> .tex -> .pdf\n\n\n\nCode chunk options\nThere are dozens of options available to control the executation of the code, the formatting and display of both the commands and the output, the display of images, graphs, and tables, and other fancy things. Here’s a list of the basic options you should become familiar with:\n\ncache: whether to store the results from excecuted code, so that it does not need to be run on subsequent knits.\n\nTRUE\nFALSE\n\neval: whether the code should be evaluated\n\nTRUE\nFALSE\n\necho: whether the code should be displayed\n\nTRUE\nFALSE\nnumbers indicating lines in a chunk\n\nerror: whether to stop execution if there is an error\n\nTRUE\nFALSE\n\nresults: how to display the output\n\nmarkup\nasis\nhold\nhide\n\ncomment: character used to indicate output lines\n\nthe default is a double hash ##\n\"\" empty character (to have a cleaner display)\n\n\n\n\n\nLaTeX\nRmarkdown files render LaTeX through an external generator. This means that you can write any math equations or LaTeX syntax within a specific chunk, and install the required LaTeX libraries outside of R, and it will be rendered properly.\nInline code chunks are setoff with single dollar signs, ie $\\beta$ is rendered as \\(\\beta\\). This is great for small equations, Greek letters, and references to variables.\nLaTeX chunks can also be significantly more complicated. Independent chunks are setoff with double dollar signs, ie $$ Complex LaTeX Thing $$, such as the following equation: \\[\nD(\\theta_l,T_x) = \\left\\{\n         \\begin{array}{ll}\n             \\theta_{l[0]}^{'}=\\theta_l                                 & \\quad i = 0 \\\\\n             \\theta_{l[i+1]}^{'} = \\theta_{l[i]}^{'} *F(\\overline{L_{[t-i-T_x]}})   & \\quad i \\leq T_l\n         \\end{array}\n     \\right.\n\\]"
  },
  {
    "objectID": "labs/01/intro_git_knitr.html#r-sweave",
    "href": "labs/01/intro_git_knitr.html#r-sweave",
    "title": "Introduction to git and knitr",
    "section": "R Sweave",
    "text": "R Sweave\nAn alternative to R Markdown is R Sweave. R Sweave knits R code together in the form of a LaTeX document. There is an example R Sweave document example_sweave.Rnw included in this folder. This can serve as a template for you if you choose to use this format for your problem sets.\nThe options for code chunks listed in the Rmd section above are the same as the options used in Rnw code chunks. As you can see in the example_sweave.Rnw document the syntax to designate code chunks is different. In Sweave documents you will use <<>>= to start the chunk and @ to end the chunk.\nTo compile the PDF click the Compile PDF button.\n\nUpdating Preferences to knit using knitr\nSweave is older way to knit together code and text, while knitr is more updated and allows for better formatting. By default R Studio sets R Sweave documents to be run using Sweave to switch to knitr open the RStudio menu in the menu bar, and choose Preferences. In Preferences go to the Sweave tab and change the PDF generation to Weave Rnw files using knitr.\n\n\nOpening an R Sweave\nIn the menu bar of RStudio, click on File, then New File, and choose R Sweave. Select the default option (Document), and click Ok. RStudio will open a new .Rnw file in the source pane. And you should be able to see a file with some default content.\nIf you opened an Rnw file before changing the from Sweave to knitr, the default content will contain \\SweaveOpts{concordance=TRUE} you will need to remove this line of code if you have changed the preferences to generate the PDF using knitr."
  },
  {
    "objectID": "labs/01/intro_git_knitr.html#rtex",
    "href": "labs/01/intro_git_knitr.html#rtex",
    "title": "Introduction to git and knitr",
    "section": "Rtex",
    "text": "Rtex\nThe final option is to use the Rtex file format. This file format again uses knitr to combine code chunks with text in the form of a LaTeX document.\nFor instructions of how to use this see the dynamic documents tutorial. The integration with RStudio is not great, so there is no automated way to open a .Rtex file from RStudio’s menu. Also you will need to use the command line to compile."
  },
  {
    "objectID": "labs/01/intro_git_knitr.html#code-style",
    "href": "labs/01/intro_git_knitr.html#code-style",
    "title": "Introduction to git and knitr",
    "section": "Code style",
    "text": "Code style\n\nUseful Links\n\nHomework Submission\nHadley Wickham Style Guide\nGoogle’s R Style Guide\nWeird One with Links\n\n\n\nAdditional notes on style\nYou don’t need to follow the exact style of any of those - use your own judgment and figure out what style you like and be consistent in using that style. But you should do the following:\n\nuse white space to make it easier to read your code\nhave your code lines be no more than 80 characters\ngive your objects and functions meaningful (and not overly long) names\ncomment your code\nindent your code as needed so one can see what lines of code go together in a block\n\nYou should NOT include periods in names of objects (this contradicts Google’s style guide). The reason is that periods are used to mean something specific in R’s S3 object oriented programming syntax (e.g., predict.lm) and that periods are used in other languages specifically for object-oriented syntax. So I’d suggest either calculate_mle or calculateMLE, not calculate.mle."
  },
  {
    "objectID": "labs/01/intro_git_knitr.html#acknowledgements",
    "href": "labs/01/intro_git_knitr.html#acknowledgements",
    "title": "Introduction to git and knitr",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThis lab was originally authored by Jared Bennett and updated incrementally by Zoe Vernon, Andrew Vaughn, and James Duncan."
  },
  {
    "objectID": "labs/01/markdown_template.html",
    "href": "labs/01/markdown_template.html",
    "title": "R Markdown Template",
    "section": "",
    "text": "I am going to show some code in R that performs matrix multiplication of a \\(2 \\times 3\\) matrix \\(A\\) and a \\(3 \\times 4\\) matrix \\(B\\).\n\n#We first set our random seed\nset.seed(2021)\n\n#Let's create A as a matrix of standard normal random variates\nA = matrix(rnorm(6), nrow = 2, ncol = 3)\n\n#We'll create B as a matrix of variates from a normal distribution\n#with mean 0 and a standard deviation of 2\nB = matrix(rnorm(12, mean = 0, sd = 2), nrow = 3, ncol = 4)\n\n\n#Now, we will print A, B, and the matrix product AB\nA\n\n           [,1]      [,2]       [,3]\n[1,] -0.1224600 0.3486495  0.8980537\n[2,]  0.5524566 0.3596322 -1.9225695\n\nB\n\n           [,1]       [,2]      [,3]      [,4]\n[1,] 0.52348873  3.4599263 0.3639908 -3.682951\n[2,] 1.83113274 -2.1644097 3.0170836  3.246620\n[3,] 0.02754388 -0.5456504 3.2089402  0.262778\n\nA%*%B\n\n          [,1]      [,2]      [,3]      [,4]\n[1,] 0.5990530 -1.668346  3.889131  1.818936\n[2,] 0.8947842  2.182118 -4.883281 -1.372290\n\n\nWe can even put in some text here and refer to the matrices \\(A\\) and \\(B\\) from a subsequent code block. In other words, code in a certain block can refer to results from earlier code blocks.\n\nprint(A[1,1] + B[2,3])\n\n[1] 2.894624\n\n\n\n\n\nNote that we can also use LaTeX’s display mode:\n\\[e^{i\\pi}+1=0\\]"
  },
  {
    "objectID": "labs/01/markdown_template.html#problem-2",
    "href": "labs/01/markdown_template.html#problem-2",
    "title": "R Markdown Template",
    "section": "Problem 2",
    "text": "Problem 2\nFor calculations we do not actually want to run, we can set \\(eval=F\\) at the beginning of the R block.\n\nrnorm(10^(10^10))"
  },
  {
    "objectID": "labs/01/markdown_template.html#problem-3",
    "href": "labs/01/markdown_template.html#problem-3",
    "title": "R Markdown Template",
    "section": "Problem 3",
    "text": "Problem 3\nNote as a formatting guide that as in standard LaTeX multiple consecutive spaces are treated as only one space and that single newlines are treated as spaces. Multiple newlines\nare treated as one newline."
  },
  {
    "objectID": "labs/01/markdown_template.html#problem-4",
    "href": "labs/01/markdown_template.html#problem-4",
    "title": "R Markdown Template",
    "section": "Problem 4",
    "text": "Problem 4\nAlso note that you should not add spaces directly after the opening \\(\\$\\) of a mathematical expression or directly before the closing \\(\\$\\) of a mathematical expression. For example, \\(\\$1+2 \\$\\) will display correctly but both \\(\\$\\,\\, 1+2\\$\\) and \\(\\$1+2\\,\\,\\$\\) will display as text instead of math."
  },
  {
    "objectID": "labs/03/reproducibility.html",
    "href": "labs/03/reproducibility.html",
    "title": "Reproducibility discussion",
    "section": "",
    "text": "Today we’ll discuss reproducibility in small groups and as a class, using the paper “A Cohort Location Model of Household Sorting in US Metropolitan Regions” as a case study.\nPlease open up the paper and corresponding code."
  },
  {
    "objectID": "labs/03/reproducibility.html#make-small-groups",
    "href": "labs/03/reproducibility.html#make-small-groups",
    "title": "Reproducibility discussion",
    "section": "Make small groups",
    "text": "Make small groups\n< 5 minutes\nForm a group with the 3–4 people sitting nearest to you, turning your desks toward one another.\nA different person should take each of the following roles:\n\nGeneral discussion leader: Leads the first 15 minutes discussion. You’ll keep the discussion moving and try to encourage contributions from all members of your group.\nGroup consensus leader: You’ll keep track of the group discussion of strengths and weaknesses and break any ties in the final consensus.\nStrengths representative: During the class discussion at the end, you’ll give one of the strengths from your group’s list, and say any differences in your group’s ranking vs. what we currently have on the board.\nWeakness representative: During the class discussion at the end, you’ll give one of the weaknesses from your group’s list, and say any differences in your group’s ranking vs. what we currently have on the board.\n\nIf you only have 3 people, then one person should serve as the representative for both the strengths and weaknesses."
  },
  {
    "objectID": "labs/03/reproducibility.html#general-discussion-in-small-groups",
    "href": "labs/03/reproducibility.html#general-discussion-in-small-groups",
    "title": "Reproducibility discussion",
    "section": "General discussion in small groups",
    "text": "General discussion in small groups\n10–15 minutes\nPlease discuss each of the following together:\n\nFrom the paper and documentation on GitHub provided by the authors, try to find where the authors carried out each of the following in their code (or if they did at all):\n\ndata collection\ndata cleaning / processing\nstatistical calculations\nvisualizations from the paper\n\nIn terms of their coding, what elements of their project do you like. Consider:\n\ndocumentation\ncomments\norganization\nnaming\nworkflow\ndata provenance\netc.\n\nWhat aspects of their code do you think could be improved?\nWithout looking at the code itself, can you tell what the purpose of each file is?\nWithout looking at the code in detail, can you quickly tell what each block of code does?\nAre there any exceptions to your answers above?\nIn what way do the authors document their workflow? Is it effective?\nTake note of any judgment calls you noticed in their workflow. Are those judgment calls documented and/or justified in their code?"
  },
  {
    "objectID": "labs/03/reproducibility.html#group-consensus-of-strengths-weaknesses",
    "href": "labs/03/reproducibility.html#group-consensus-of-strengths-weaknesses",
    "title": "Reproducibility discussion",
    "section": "Group consensus of strengths & weaknesses",
    "text": "Group consensus of strengths & weaknesses\n10–15 minutes\nAs a group, come to a consensus on the most important strengths and weaknesses of the reproducibility aspects of the paper, and rank each category separately from most important to least.\nThese should be succinct enough to write on the board later, so please write them down and keep them short and specific. Try to come up with 4–5 for each."
  },
  {
    "objectID": "labs/03/reproducibility.html#class-discussion",
    "href": "labs/03/reproducibility.html#class-discussion",
    "title": "Reproducibility discussion",
    "section": "Class discussion",
    "text": "Class discussion\n10–15 minutes\nAs a class, we’ll discuss the strengths and weaknesses and find the overall consensus. Please pick two representatives from your group, one for strengths and one for weaknesses."
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Statistics 243 Fall 2022",
    "section": "",
    "text": "Submit your solutions on Gradescope and (for problem sets but not assignments) via your Git repository.\n\n\n\nAssignment or PS\nDate\nTime\nNotes\n\n\n\n\nbash shell problems (first 10)\nFriday Sep. 2\n10 am\nI recomend you do at least some of the tutorial reading before class on Aug. 31\n\n\nPS 1 (HTML) (PDF)\nWednesday Sep. 7\n10 am\n\n\n\nregular expression reading and problems\nFriday Sep. 9\n10 am\n\n\n\nPS 2 (HTML) (PDF)\nFriday Sep. 16\n10 am\n\n\n\nPS 3 (HTML) (PDF)\nWednesday Sep. 28\n10 am"
  },
  {
    "objectID": "schedule.html#quiz-schedule",
    "href": "schedule.html#quiz-schedule",
    "title": "Statistics 243 Fall 2022",
    "section": "Quiz schedule",
    "text": "Quiz schedule\nQuizzes are in-person only.\n\nQuiz 1: Monday October 24 in class.\n\nReview session Friday October 21 in Lab.\n\nQuiz 2: Monday November 21 in class.\n\nReview session Friday November 18 in Lab."
  },
  {
    "objectID": "schedule.html#project-schedule",
    "href": "schedule.html#project-schedule",
    "title": "Statistics 243 Fall 2022",
    "section": "Project schedule",
    "text": "Project schedule\nDue date: TBD, but due either the reading week or the exam week."
  },
  {
    "objectID": "schedule.html#first-few-week-activities-and-assignments",
    "href": "schedule.html#first-few-week-activities-and-assignments",
    "title": "Statistics 243 Fall 2022",
    "section": "First few week activities and assignments",
    "text": "First few week activities and assignments\n\n\n\nWeek\nDay\nDate\nTime\nActivity/Assignment\n\n\n\n\n1\nThursday\n2022-08-25\n4:00-5:30 pm\nOptional: Introduction to LaTeX session run by the library (see Details below)\n\n\n\nFriday\n2022-08-26\nnoon\nRequired (part of your class participation): class survey\n\n\n\n\n\nnoon\nRequired: office hour time survey\n\n\n\n\n\nnoon\nRequired: Go to github.berkeley.edu and login with your Calnet credentials (that’s all – it will allow me to create your repository)\n\n\n\n\n\nnoon\nOptional: survey for time for extra R help session\n\n\n\n\n\nlab (1:00-4:30 pm)\nOptional: Lab 0 help session for software installation/setup and UNIX command line basics (see Details below)\n\n\n2\nMonday\n2022-08-29\n10 am\nRequired: read first three sections of Unit 2 (sections before ‘Webscraping’)\n\n\n\n\n\nnone\nOptional: work through the UNIX basics tutorial and answer (for yourself) the questions at the end\n\n\n\nTuesday\n2022-08-30\n4-6pm\nOptional: R help session (see Details below)\n\n\n\nFriday\n2022-09-02\n10 am\nRequired: Bash shell tutorial and exercises (see Details below)\n\n\n\n\n\nlab\nRequired: Lab 1 on using course tools and problem set submission (see Details below)\n\n\n3\nWednesday\n2022-09-07\n4:00-5:30 pm\nOptional: Introduction to LaTeX session run by the library (see Details below)\n\n\n\nWednesday\n2022-09-07\n10 am\nPS1 due\n\n\n\nFriday\n2022-09-09\n10 am\nRegular expression tutorial and exercises (see Details below)\n\n\n\n\n\nlab\nRequired: Lab 2 on assertions and testing"
  },
  {
    "objectID": "schedule.html#notes-on-assignments-and-activities",
    "href": "schedule.html#notes-on-assignments-and-activities",
    "title": "Statistics 243 Fall 2022",
    "section": "Notes on assignments and activities",
    "text": "Notes on assignments and activities\n\nOptional library LaTeX sessions: I highly recommend (in particular if you are a Statistics graduate student) that you know how to create equations in LaTeX.\nOptional Lab 0 software/command line help session: (August 26 in Evans 344) Help session for installing software, accessing a UNIX-style command line, and basic UNIX usage (e.g., the UNIX basics tutorial). You should have software installed, be able to accesss the command line, and have started to become familiar with basic UNIX usage before class on Wednesday August 31.\nOptional R help session: (August 30 in Evans 334) If you are not familiar with R at the level of modules 1-5 of the R bootcamp, work through those modules and the breakout problems associated with the modules. If you’d like help and an opportunity for extra practice, please attend the special R catch-up session listed above. Some other resources for R are listed at the end of module 11 of the bootcamp materials, so you could also use those resources. You should do this by the end of the week of September 6.\nBash shell tutorial and exercises: (by September 2), read through this tutorial on using the bash shell. You can skip the pages on Regular Expressions and Managing Processes. Work through the first 10 problems in the exercises and submit your answers via Gradescope. This is not a formal problem set, so you don’t need to worry about formatting nor about explaining/commenting your answers, nor do you need to put your answers in your GitHub class repository. In fact it’s even fine with me if you hand-write the answers and scan them to an electronic document. I just want to make sure you’ve worked through the tutorial. I’ll be doing demonstrations on using the bash shell in class starting on Wednesday August 31, so that will be helpful as you work through the tutorial.\nLab 1: (September 2) First section/lab on using Git, setting up your GitHub repository for problem sets, and using R Markdown/Quarto/knitr to generate dynamic documents. Please come only to the section you are registered for given space limits in the room, unless you have talked with Chris and have his permission.\nRegular expression tutorial and exercises: (by September 9), read the regular expression material in the tutorial on using the bash shell. Also read Section 1 of the ‘basic text manipulation’ page of the string processing tutorial (you can focus on Section 1.2 on stringr if you wish). Then answer the regular expressions (regex) practice problems and submit your answers on Gradescope. This is not one of the graded problem sets but rather an ‘assignment’ that will simply be noted as being completed or not."
  },
  {
    "objectID": "howtos/ps-submission.html",
    "href": "howtos/ps-submission.html",
    "title": "Problem Set Submissions",
    "section": "",
    "text": "Steps:\n\nLog into github.berkeley.edu using your Berkeley credentials. Because of how the system works, you will need to log in before your account is created. Nothing else needs to be done, just log in and log out.\nAfter accounts are created (may take a couple days after first login), when you log in again, you should see one private repository listed on the left side (e.g., stat243-fall-2022/ahv36). This is your class repository. Do not change the repository settings! They are set up for this class.\nClone the repo to your home directory (I would clone it into a directory just for repositories (e.g., I use ~/repos). In the top-level of your working directory, you should create a file named (exactly) .gitignore.\n\nThe .gitignore file causes Git to ignore transient or computer-specific files that R/RStudio generates. (more info at https://github.com/github/gitignore) In it, put (again, don’t put dashed lines):\n# History files\n.Rhistory\n.Rapp.history\n\n# Session Data files\n.RData\n\n# Example code in package build process\n*-Ex.R\n\n# Output files from R CMD build\n/*.tar.gz\n\n# Output files from R CMD check\n/*.Rcheck/\n\n# RStudio files\n.Rproj.user/\n\n# produced vignettes\nvignettes/*.html\nvignettes/*.pdf\n\n# OAuth2 token, see https://github.com/hadley/httr/releases/tag/v0.3\n.httr-oauth\n\n# knitr and R markdown default cache directories\n/*_cache/\n/cache/\n\n# Temporary files created by R markdown\n*.utf8.md\n*.knit.md\n\n# Shiny token, see https://shiny.rstudio.com/articles/shinyapps.html\nrsconnect/\n\nRepository Organization\nThe problem sets in your repository should be organized into folders with specific filenames.\nWhen we pull from your repository, our code will be assuming the following structure:\nyour_repo/\n├── ps1/\n│   ├── ps1.pdf\n│   ├── ps1.Rmd # or .Rtex, .rnw, .lyx, .tex, .R\n├── ps2/\n│   ├── ...\n├── ...\n├── ps8/\n├── .gitignore\n└── info.json\nThe file names are case-sensitive, so please keep everything lowercase."
  },
  {
    "objectID": "howtos/windowsInstall.html",
    "href": "howtos/windowsInstall.html",
    "title": "R/Rstudio on Windows",
    "section": "",
    "text": "Note\n\n\n\nThis tutorial installs Windows-only versions of everything. Modern Windows systems have an Ubuntu subsystem available that we highly recommend. See the Installing the Linux Subsystem on Windows tutorial for setting up that configuration.\n\n\n\nInstalling R\nThe first step in installing the R language. This is available on CRAN (Comprehensive R Archive Network).\n\nGo to the CRAN webpage, www.r-project.org\nIn the first paragraph, click the link download R\nYou’re now on a page titled CRAN Mirrors, choose the mirror located closest to your geographic location\n\nMirrors are different servers that all host copies of the same website. You get best performance from the location closest to you.\n\nYou’re now on a paged titled The Comprehensive R Archive Network. The first box is labeled Downloand and Install R, click Download R for Windows\nClick base or install R for the first time, these take you to the same place\n\nFor more advanced things, you may need the Rtools download later. It isn’t necessary now, but remember that for the future.\n\nAt the top is a large-font link, Download R X.X.X for Windows, click this. It will begin downloading the Windows installer for R.\nFollow the instructions for setup. If you are unsure of anything, leave the default settings\n\n\n\nInstalling RStudio\nRStudio is one of the best text editors for coding in R. It is our recommended option for beginning. After you are comfortable with the language, or if you use other languages as well, you may want to explore Atom or Sublime. More advanced options include Emacs with [ESS package][https://ess.r-project.org/] and vim with the Nvim-R plugin.\nTo install RStudio:\n\nGo to the RStudio Desktop download page, rstudio.com/products/rstudio/download/#download\nChoose the download for your OS, most likely the Windows 10/8/7 one\nFollow the instructions for setup. If you are unsure of anything, leave the default settings\nOpen RStudio (R will run automatically in the background)\n\nYou may have to allow RStudio to run if prompted (depends on security settings and anti-virus software)\n\n\nOnce RStudio is installed, you can install or update packages in one of two ways:\n\nVia the console, using install.packages() or update.packages()\n\nVia the gui:\n\nIn the top bar, click on tools\nSelect Install Packages… to install packages\nSelect Check for Package Updates… to update packages\n\n\n\n\nCompiling PDF Documents\nFor the purposes of this class, you will be submitting homeworks as PDF documents that blend written text, code, and code-generated output. These documents are RMarkdown documents, and are dynamic documents that provide a convenient method for documenting your work (more on this in one of the lab sections). To do this, you need a LaTeX renderer. We recommend MiKTeX for Windows.\n\nGo to Getting MiKTeX to download MiKTeX for Windows, miktex.org/download\nThe first page should be Install on Windows, click Download at the bottom of the page\n\nClick the download to begin\n\n\n\n\n\n\n\nImportant\n\n\n\nFOLLOW THESE INSTALL INSTRUCTIONS.\nThe default options are fine in most places, but there is one that will cause problems.\n\n\n\nAccept the Copying Conditions, click next\nInstall only for you, click next\nUse the default directory, click next\nThis should be the Settings page. Under Install missing packages on-the-fly, change the setting to Yes, click next\n\n\n\nBecause we are using MiKTeX as an external renderer, it can’t ask you to install missing packages, and will then fail, so we have to set that installation as automatic.\n\n\nClick start (Optional, but highly recommended) Open RStudio, select a new .Rmd document, d then choose knit. This may take some time, because MiKTeX is installing new braries, but it ensures that your pipeline is setup correctly"
  },
  {
    "objectID": "howtos/accessingPython.html",
    "href": "howtos/accessingPython.html",
    "title": "Accessing Python",
    "section": "",
    "text": "We recommend using the Anaconda distribution of Python, though that is not required. You can download it here. Make sure to use Python 3.7, 3.8, or 3.9 and not Python 2.*.\nOnce you’ve installed Python, please install the following packages: - numpy - scipy - pandas - dask - dask.distributed - dask.bag - dask.array - dask.dataframe - dask.multiprocessing\nAssuming you installed the Anaconda Python, you should be able to do this:"
  },
  {
    "objectID": "howtos/accessingPython.html#python-from-the-command-line",
    "href": "howtos/accessingPython.html#python-from-the-command-line",
    "title": "Accessing Python",
    "section": "Python from the command line",
    "text": "Python from the command line\nOnce you get your SCF account (which you’ll need for our discussion of parallelization and big data and for PS6), you can access Python or IPython from the UNIX command line as soon as you login to an SCF server. Just SSH to an SCF Linux machine (e.g., arwen.berkeley.edu or radagast.berkeley.edu) and run ‘python’ or ‘ipython’ from the command line.\nMore details on using SSH are here. Note that if you have the Ubuntu subsystem for Windows, you can use SSH directly from the Ubuntu terminal."
  },
  {
    "objectID": "howtos/accessingPython.html#python-via-jupyter-notebook",
    "href": "howtos/accessingPython.html#python-via-jupyter-notebook",
    "title": "Accessing Python",
    "section": "Python via Jupyter notebook",
    "text": "Python via Jupyter notebook\nYou can use a Jupyter notebook to run Python code from the SCF JupyterHub or the Berkeley DataHub. Select Start My Server. Then, unless you are running long or parallelized code, just click Spawn (in other words, accept the default ‘standalone’ partition). On the next page select ‘New’ and ‘Python 3’.\nTo finish your session, click on Control Panel and Stop My Server. Do not click Logout."
  },
  {
    "objectID": "howtos/RandRStudioInstall.html",
    "href": "howtos/RandRStudioInstall.html",
    "title": "Installing R & RStudio",
    "section": "",
    "text": "Via DataHub\nSee the instructions in Accessing the Unix Command Line for how to login to Datahub. Then in the mid-upper right, click on New and RStudio. Alternatively, to go directly to RStudio, go to https://r.datahub.berkeley.edu."
  },
  {
    "objectID": "howtos/accessingUnixCommandLine.html",
    "href": "howtos/accessingUnixCommandLine.html",
    "title": "Accessing the Unix Command Line",
    "section": "",
    "text": "Mac OS (on your personal machine):\nOpen a Terminal by going to Applications -> Utilities -> Terminal\n\n\nWindows (on your personal machine):\n\nYou may be able to use the Ubuntu bash shell available in Windows.\nYour PC must be running a 64-bit version of Windows 10 Anniversary Update or later (build 1607+).\nPlease see these links for more information:\n\nhttp://blog.revolutionanalytics.com/2017/12/r-in-the-windows-subsystem-for-linux.html\nhttps://msdn.microsoft.com/en-us/commandline/wsl/install_guide\n\nFor more detailed instructions, see the Installing the Linux Subsystem on Windows tutorial.\n(Not recommended) There’s an older program called cygwin that provides a UNIX command-line interface.\n\nNote that when you install Git on Windows, you will get Git Bash. While you can use this to control Git, the functionality is limited so this will not be enough for general UNIX command-line access for the course.\n\n\nLinux (on your personal machine):\nIf you have access to a Linux machine, you very likely know how to access a terminal.\n\n\nAccess via DataHub (provided by UC Berkeley’s Data Science Education Program)\n\nGo to https://datahub.berkeley.edu\nClick on Sign in with bCourses, sign in via CalNet, and authorize DataHub to have access to your account.\nIn the mid-upper right, click on New and Terminal.\nTo end your session, click on Control Panel and Stop My Server. Note that Logout will not end your running session, it will just log you out of it.\n\n\n\nAccess via the Statistical Computing Facility (SCF)\nWith an SCF account (available here), you can access a bash shell in the ways listed below.\nThose of you in the Statistics Department should be in the process of getting an SCF account. Everyone else will need an SCF account when we get to the unit on parallel computing, but you can request an account now if you prefer.\n\nYou can login to our various Linux servers and access a bash shell that way. Please see http://statistics.berkeley.edu/computing/access.\nYou can also access a bash shell via the SCF JupyterHub interface; please see the Accessing Python instructions but when you click on New, choose Terminal. This is very similar to the DataHub functionality discussed above."
  },
  {
    "objectID": "howtos/windowsAndLinux.html",
    "href": "howtos/windowsAndLinux.html",
    "title": "Windows 10 and the Ubuntu Subsystem",
    "section": "",
    "text": "Windows 10 has a powerful new feature that allows a full Linux system to be installed and run from within Windows. This is incredibly useful for building/testing code in Linux, without having a dedicated Linux machine, but it poses strange new behaviors as two very different operating systems coexist in one place. Initially, this document mirrors the Windows Install tutorial, showing you how to install Ubuntu and setting up R, RStudio, and LaTex. Then, we cover some of the issues of running two systems together, starting with finding files, finding the Ubuntu subsystem, and file modifications."
  },
  {
    "objectID": "howtos/windowsAndLinux.html#installing-ubuntu",
    "href": "howtos/windowsAndLinux.html#installing-ubuntu",
    "title": "Windows 10 and the Ubuntu Subsystem",
    "section": "Installing Ubuntu",
    "text": "Installing Ubuntu\nThere are 2 parts to installing a Linux subsystem in Windows. I will write this using Ubuntu as the example, as it is my preferred Linux distro, but several others are provided by Windows.\nSources:\n\nOfficial Windows Instructions\nUbuntu Update Instructions\n\n\n1) Enable Linux Subsystem\nBy default, the Linux subsystem is an optional addition in Windows. This feature has to be enabled prior to installing Linux. There are two ways to do it.\n\nCMD Line\nThe simplest way to enable the Linux subsystem is through PowerShell.\n\nOpen PowerShell as Administrator\nRun the following (on one line):\nEnable-WindowsOptionalFeature -Online -FeatureName Microsoft-Windows-Subsystem-Linux\nRestart the computer\n\nGUI\nIf you don’t wish to use PowerShell, you can work your way through the control panel and turn on the Linux subsystem.\n\nOpen the settings page through the search bar\nGo to Programs and Features\nFind Turn Windows Features on or off on the right side\nEnable the Windows Subsystem for Linux option\nRestart the computer\n\n\n\n\n2) Install Linux Subsystem\nOnce the Linux subsystem feature has been enabled, there are multiple methods to download and install the Linux distro you want. I highly recommend installing Ubuntu from the Microsoft store. There are several other flavors available as well, but Ubuntu is generally the easiest to learn and the most well supported.\n\nOpen the Microsoft Store\nSearch for Ubuntu\n\nYou’re looking for the highest number followed by LTS, currently 20.04 LTS (or 18.04 LTS is fine too). This is the current long-term-release, meaning it will be supported for the next 5 years.\n\nClick on the tile, then click Get, and this should start the installation.\nFollow the prompts to install Ubuntu.\n\nAfter installing Ubuntu, it is advisable to update it. This is something you should do on a regular basis.\n\nOpen a Bash terminal.\nType sudo apt update to update your local package database.\nType sudo apt upgrade to upgrade your installed packages."
  },
  {
    "objectID": "howtos/windowsAndLinux.html#using-the-linux-terminal-from-r-in-windows",
    "href": "howtos/windowsAndLinux.html#using-the-linux-terminal-from-r-in-windows",
    "title": "Windows 10 and the Ubuntu Subsystem",
    "section": "Using the Linux Terminal from R in Windows",
    "text": "Using the Linux Terminal from R in Windows\nTo get all the functionality of a UNIX-style commandline from within R (e.g., for bash code chunks), you should set the terminal under R in Windows to be the Linux subsystem."
  },
  {
    "objectID": "howtos/windowsAndLinux.html#a-note-on-file-modification",
    "href": "howtos/windowsAndLinux.html#a-note-on-file-modification",
    "title": "Windows 10 and the Ubuntu Subsystem",
    "section": "A Note on File Modification",
    "text": "A Note on File Modification\nDO NOT MODIFY LINUX FILES FROM WINDOWS\nIt is highly recommended that you never modify Linux files from Windows because of metadata corruption issues. Any files created under the Linux subsystem, only modify them with Linux tools. In contrast, you can create files in the Windows system and modify them with both Windows or Linux tools. There could be file permission issues because Windows doesn’t have the same concept of file permissions as Linux. So, if you intend to work on files using both Linux and Windows, create the files in the C drive under Windows, and you should be safe to edit them with either OS."
  },
  {
    "objectID": "howtos/windowsAndLinux.html#finding-windows-from-linux",
    "href": "howtos/windowsAndLinux.html#finding-windows-from-linux",
    "title": "Windows 10 and the Ubuntu Subsystem",
    "section": "Finding Windows from Linux",
    "text": "Finding Windows from Linux\nOnce you have some flavor of Linux installed, you need to be able to navigate from your Linux home directory to wherever Windows stores files. This is relatively simple, as the Windows Subsystem shows Windows to Linux as a mounted drive.\n\nSource\n\n\nOpen a Bash terminal.\nType cd / to get to the root directory.\nIn root, type cd /mnt. This gets you to the mount point for your drives.\nType ls to see what drives are available (you should see a c, and maybe d as well).\nType cd c to get into the Windows C drive. This is the root of the C directory for Windows.\nTo find your files, change directy into the users folder, then into your username.\n\ncd Users/<your-user-name>\nThis is your home directory in Windows. If you type ls here, you should see things like\n\nDocuments\nDownloads\nPictures\nVideos\netc…"
  },
  {
    "objectID": "howtos/windowsAndLinux.html#finding-linux-from-windows",
    "href": "howtos/windowsAndLinux.html#finding-linux-from-windows",
    "title": "Windows 10 and the Ubuntu Subsystem",
    "section": "Finding Linux from Windows",
    "text": "Finding Linux from Windows\nThis is slightly more tricky than getting from Linux to Windows. Windows stores the Linux files in a hidden subfolder so that you don’t mess with them from Windows. However, you can find them, and then the easiest way (note, do not read as safest or smartest) to find those files in the future is by creating a desktop shortcut.\n\nSource\n\n\nOpen File Explorer\nIn the address bar, type %userprofile%\\AppData\\Local\\Packages\n\n%userprofile% will expand to something like C:\\Users\\<your-user-name>\n\nLook for a folder related to the Linux distro that you installed\n\nThese names will change slightly over time, but look for something similar-ish.\nFor Ubuntu, look for something with CanonicalGroupLimited.UbuntuonWindows in it.\n\nCanonical is the creator/distributor of Ubuntu.\n\n\nClick LocalState\nClick rootfs\n\nThis is the root of your Linux distro.\n\nClick into home and then into your user name.\n\nThis is your home directory under Linux.\n\nDO NOT MODIFY THESE FILES FROM WINDOWS\n\nData corruption is a possibility.\n\n\nSo, the final path to find your home directory from windows will look like:\n%userprofile%\\AppData\\Local\\Packages\\<Distro-Folder>\\LocalStat\\rootfs\\home\\<your-user-name>\\"
  },
  {
    "objectID": "howtos/windowsAndLinux.html#installing-r-on-the-linux-subsystem",
    "href": "howtos/windowsAndLinux.html#installing-r-on-the-linux-subsystem",
    "title": "Windows 10 and the Ubuntu Subsystem",
    "section": "Installing R on the Linux Subsystem",
    "text": "Installing R on the Linux Subsystem\nIMPORTANT: This section is only if you’d like to try using R under Linux. For class, using R under Windows should be fine.\nThe Linux Subsystem behaves exactly like a regular Linux installation, but for completeness, I will provide instructions here for people new to Linux. These instructions are written from the perspective of Ubuntu, but will be similar for other repos.\nR is not a part of the standard Ubuntu installation. So, we have to add the repository manually to our repository list. This is relatively straightforward, and R supports several versions of Ubuntu.\nSources:\n\nCRAN guide for Ubuntu\nDigital Ocean quick tutorial\n\n\nIn a bash window, type:\nsudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys E298A3A825C0D65DFD57CBB651716619E084DAB9\n\nThis adds the key to “sign”, or validate, the R repository\n\nThen, type:\nsudo add-apt-repository 'deb https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/'\n\ncloud.r-project.org is the default mirror, however, it is prudent to connect to the mirror closest to you geographically. Berkeley has it’s own mirror, so the command with the Berkeley mirror would look like\n\nsudo add-apt-repository 'deb https://cran.r-project.org/bin/linux/ubuntu/bionic-cran40/'\nFinally, type sudo apt install r-base, and press y to confirm installation\nTo test that it worked, type R into the console, and an R session should begin\n\nType q() to quit the R session"
  },
  {
    "objectID": "howtos/windowsAndLinux.html#installing-rstudio-on-the-linux-subsystem",
    "href": "howtos/windowsAndLinux.html#installing-rstudio-on-the-linux-subsystem",
    "title": "Windows 10 and the Ubuntu Subsystem",
    "section": "Installing Rstudio on the Linux Subsystem",
    "text": "Installing Rstudio on the Linux Subsystem\n\n\n\n\n\n\nWarning\n\n\n\nTHIS NO LONGER WORKS\n\n\nAs of Rstudio 1.5.x, it does not run on WSL. Link\nAlso possible issues, WSL has no GUI, and therefore can’t support anything that uses a GUI.\nThese instructions work, but Rstudio doesn’t run.\nSources:\n\nRstudio\nSource\n\n\nGo the the Rstudio website (link above) and download the appropriate Rstudio Desktop version.\n\nFor most people, this is the Ubuntu 18 (64-bit) installer.\nSave it somewhere that you can find it.\nYou should have a file similar to rstudio-<version number>-amd64.deb\n\nOpen a terminal window and navigate to wherever you saved the rstudio install file.\nType the command sudo dpkg -i ./rstudio-<version number>-amd64.deb\n\nThis tells the package installer (dpkg) to install (-i) the file specified (./thing.deb)\n\nType the command sudo apt-get install -f\n\nThis tells the package manager (apt-get) to fix (-f) any dependency issues that may have arisen when installing the package.\n\nType the command which rstudio to make sure the system can find it.\n\nOutput should be similar to /usr/bin/rstudio\n\nRun rstudio from linux by typing rstudio &\n\nThe & runs it in the background, allowing you to close the terminal window."
  },
  {
    "objectID": "howtos/windowsAndLinux.html#installing-latex-on-the-linux-subsystem",
    "href": "howtos/windowsAndLinux.html#installing-latex-on-the-linux-subsystem",
    "title": "Windows 10 and the Ubuntu Subsystem",
    "section": "Installing LaTeX on the Linux Subsystem",
    "text": "Installing LaTeX on the Linux Subsystem\n\n\n\n\n\n\nImportant\n\n\n\nThis section is only if you’d like to try using LaTeX under Linux. For class, using LaTeX (or R Markdown) under Windows should be fine.\n\n\nLaTeX is a text-markup language used when generating documents from .Rmd files.\nSource LaTeX\n\nType sudo apt-get install texlive-full, press y to confirm installation\n\nGenerally, if you want to create and edit R Markdown documents you will also need a text editor to go with your LaTeX installation, but we won’t go into that here."
  },
  {
    "objectID": "howtos/gitInstall.html",
    "href": "howtos/gitInstall.html",
    "title": "Installing Git",
    "section": "",
    "text": "You can install Git by downloading and installing the correct binary from here.\nGit comes installed on the SCF, so if you login to an SCF machine and want to use Git there, you don’t need to install Git.\n\nSidenotes on using Git with RStudio\nYou can work with Git through RStudio via RStudio projects.\nHere are some instructions. Here are some helpful guidelines from RStudio.\nYou may need to tell RStudio where the Git executable is located as follows.\n\nOn Windows, the git executable should be installed somewhere like: \"C:/Program Files (x86)/Git/bin/git.exe\"\nOn MacOS X, you can locate the executable by executing the following in Terminal: which git\nOnce you locate the executable, you may then need to confirm that RStudio is looking in the right place. Go to “Tools -> Options -> Git/SVN -> Git executable” and confirm it has the correct information about the location of the git executable."
  },
  {
    "objectID": "units/unit2-dataTech.html",
    "href": "units/unit2-dataTech.html",
    "title": "Data technologies, formats, and structures",
    "section": "",
    "text": "PDF\nReferences (see syllabus for links):\n(Optional) Videos\nThere are four videos from 2020 in the bCourses Media Gallery that you can use for reference if you want to:"
  },
  {
    "objectID": "units/unit2-dataTech.html#text-and-binary-files",
    "href": "units/unit2-dataTech.html#text-and-binary-files",
    "title": "Data technologies, formats, and structures",
    "section": "Text and binary files",
    "text": "Text and binary files\nIn general, files can be divided into text files and binary files. In both cases, information is stored as a series of bits. Recall that a bit is a single value in base 2 (i.e., a 0 or a 1), while a byte is 8 bits.\nA text file is one in which the bits in the file encode individual characters. Note that the characters can include the digit characters 0-9, so one can include numbers in a text file by writing down the digits needed for the number of interest. Examples of text file formats include CSV, XML, HTML, and JSON.\nText files may be simple ASCII files (i.e., files encoded using ASCII) or in other encodings such as UTF-8, both covered in Section 5. ASCII files have 8 bits (1 byte) per character and can represent 128 characters (the 52 lower and upper case letters in English, 10 digits, punctuation and a few other things – basically what you see on a standard US keyboard). UTF-8 files have between 1 and 4 bytes per character.\nA binary file is one in which the bits in the file encode the information in a custom format and not simply individual characters. Binary formats are not (easily) human readable but can be more space-efficient and faster to work with (because it can allow random access into the data rather than requiring sequential reading). The meaning of the bytes in such files depends on the specific binary format being used and a program that uses the file needs to know how the format represents information. Examples of binary files include netCDF files, R data (e.g., .Rda) files, Python pickle files, and compiled code files.\nNumbers in binary files are usually stored as 8 bytes per number. We’ll discuss this much more in Unit 8."
  },
  {
    "objectID": "units/unit2-dataTech.html#common-file-types",
    "href": "units/unit2-dataTech.html#common-file-types",
    "title": "Data technologies, formats, and structures",
    "section": "Common file types",
    "text": "Common file types\nHere are some of the common file types, some of which are text formats and some of which are binary formats.\n\n‘Flat’ text files: data are often provided as simple text files. Often one has one record or observation per row and each column or field is a different variable or type of information about the record. Such files can either have a fixed number of characters in each field (fixed width format) or a special character (a delimiter) that separates the fields in each row. Common delimiters are tabs, commas, one or more spaces, and the pipe (|). Common file extensions are .txt and .csv. Metadata (information about the data) are often stored in a separate file. CSV files are quite common, but if you have files where the data contain commas, other delimiters can be good. Text can be put in quotes in CSV files, and this can allow use of commas within the data. This is difficult to deal with from the command line, but read.table() in R handles this situation.\n\nOne occasionally tricky difficulty is as follows. If you have a text file created in Windows, the line endings are coded differently than in UNIX. Windows uses a newline (the ASCII character \\n) and a carriage return (the ASCII character \\r) whereas UNIX uses onlyl a newline in UNIX). There are UNIX utilities (fromdos in Ubuntu, including the SCF Linux machines and dos2unix in other Linux distributions) that can do the necessary conversion. If you see ^M at the end of the lines in a file, that’s the tool you need. Alternatively, if you open a UNIX file in Windows, it may treat all the lines as a single line. You can fix this with todos or unix2dos.\n\nIn some contexts, such as textual data and bioinformatics data, the data may be in a text file with one piece of information per row, but without meaningful columns/fields.\nIn scientific contexts, netCDF (.nc) (and the related HDF5) are popular format for gridded data that allows for highly-efficient storage and contains the metadata within the file. The basic structure of a netCDF file is that each variable is an array with multiple dimensions (e.g., latitude, longitude, and time), and one can also extract the values of and metadata about each dimension. The ncdf4 package in R nicely handles working with netCDF files.\nData may also be in text files in formats designed for data interchange between various languages, in particular XML or JSON. These formats are “self-describing”; namely the metadata is part of the file. The XML2, rvest, and jsonlite packages are useful for reading and writing from these formats. More in Section 4.\nYou may be scraping information on the web, so dealing with text files in various formats, including HTML. The XML2 and rvest packages are also useful for reading HTML.\nData may already be in a database or in the data storage format of another statistical package (Stata, SAS, SPSS, etc.). The foreign package in R has excellent capabilities for importing Stata (read.dta()), SPSS (read.spss()), and SAS (read.ssd() and, for XPORT files, read.xport()), among others.\nFor Excel, there are capabilities to read an Excel file (see the readxl and XLConnect package among others), but you can also just go into Excel and export as a CSV file or the like and then read that into R. In general, it’s best not to pass around data files as Excel or other spreadsheet format files because (1) Excel is proprietary, so someone may not have Excel and the format is subject to change, (2) Excel imposes limits on the number of rows, (3) one can easily manipulate text files such as CSV using UNIX tools, but this is not possible with an Excel file, (4) Excel files often have more than one sheet, graphs, macros, etc., so they’re not a data storage format per se.\nR can easily interact with databases (SQLite, PostgreSQL, MySQL, Oracle, etc.), querying the database using SQL and returning results to R. More in the big data unit and in the large datasets tutorial mentioned above."
  },
  {
    "objectID": "units/unit2-dataTech.html#core-r-functions",
    "href": "units/unit2-dataTech.html#core-r-functions",
    "title": "Data technologies, formats, and structures",
    "section": "Core R functions",
    "text": "Core R functions\nread.table() is probably the most commonly-used function for reading in data. It reads in delimited files (read.csv() and read.delim() are special cases of read.table()). The key arguments are the delimiter (the sep argument) and whether the file contains a header, a line with the variable names. We can use read.fwf() to read from a fixed width text file into a data frame.\nThe most difficult part of reading in such files can be dealing with how R determines the classes of the fields that are read in. There are a number of arguments to read.table() and read.fwf() that allow the user to control the classes. One difficulty in older versions of R was that character fields were read in as factors.\nLet’s work through a couple examples. Before we do that, let’s look at the arguments to read.table(). Note that sep='' separates on any amount of white space.\n\ndat <- read.table(file.path('..', 'data', 'RTADataSub.csv'),\n                  sep = ',', header = TRUE)\nsapply(dat, class)[1:10]  # What are the classes of the columns?\n\nX2010.08.02.18.55             X2336              X549             X2086 \n      \"character\"       \"character\"       \"character\"       \"character\" \n             X666              X481              X298             X1624 \n      \"character\"       \"character\"       \"character\"       \"character\" \n            X1732              X593 \n      \"character\"       \"character\" \n\n## whoops, there is an 'x', presumably indicating missingness:\nunique(dat[ , 2])\n\n  [1] \"2124\" \"1830\" \"1833\" \"1600\" \"1578\" \"1187\" \"1005\" \"918\"  \"865\"  \"871\" \n [11] \"860\"  \"883\"  \"897\"  \"898\"  \"893\"  \"913\"  \"870\"  \"962\"  \"880\"  \"875\" \n [21] \"884\"  \"894\"  \"836\"  \"848\"  \"885\"  \"851\"  \"900\"  \"861\"  \"866\"  \"867\" \n [31] \"829\"  \"853\"  \"920\"  \"877\"  \"908\"  \"855\"  \"845\"  \"859\"  \"856\"  \"825\" \n [41] \"828\"  \"854\"  \"847\"  \"840\"  \"873\"  \"822\"  \"818\"  \"838\"  \"815\"  \"813\" \n [51] \"816\"  \"849\"  \"802\"  \"805\"  \"792\"  \"823\"  \"808\"  \"798\"  \"800\"  \"842\" \n [61] \"809\"  \"807\"  \"826\"  \"810\"  \"801\"  \"794\"  \"771\"  \"796\"  \"790\"  \"787\" \n [71] \"775\"  \"751\"  \"783\"  \"811\"  \"768\"  \"779\"  \"795\"  \"770\"  \"821\"  \"830\" \n [81] \"767\"  \"772\"  \"791\"  \"781\"  \"773\"  \"777\"  \"814\"  \"778\"  \"782\"  \"837\" \n [91] \"759\"  \"846\"  \"797\"  \"835\"  \"832\"  \"793\"  \"803\"  \"834\"  \"785\"  \"831\" \n[101] \"820\"  \"812\"  \"824\"  \"728\"  \"760\"  \"762\"  \"753\"  \"758\"  \"764\"  \"741\" \n[111] \"709\"  \"735\"  \"749\"  \"752\"  \"761\"  \"750\"  \"776\"  \"766\"  \"789\"  \"763\" \n[121] \"864\"  \"858\"  \"869\"  \"886\"  \"844\"  \"863\"  \"916\"  \"890\"  \"872\"  \"907\" \n[131] \"926\"  \"935\"  \"933\"  \"906\"  \"905\"  \"912\"  \"972\"  \"996\"  \"1009\" \"961\" \n[141] \"952\"  \"981\"  \"917\"  \"1011\" \"1071\" \"1920\" \"3245\" \"3805\" \"3926\" \"3284\"\n[151] \"2700\" \"2347\" \"2078\" \"2935\" \"3040\" \"1860\" \"1437\" \"1512\" \"1720\" \"1493\"\n[161] \"1026\" \"928\"  \"874\"  \"833\"  \"850\"  \"\"     \"x\"   \n\n## let's treat 'x' as a missing value indicator\ndat2 <- read.table(file.path('..', 'data', 'RTADataSub.csv'),\n                   sep = ',', header = TRUE,\n                   na.strings = c(\"NA\", \"x\"))\nunique(dat2[ , 2])\n\n  [1] 2124 1830 1833 1600 1578 1187 1005  918  865  871  860  883  897  898  893\n [16]  913  870  962  880  875  884  894  836  848  885  851  900  861  866  867\n [31]  829  853  920  877  908  855  845  859  856  825  828  854  847  840  873\n [46]  822  818  838  815  813  816  849  802  805  792  823  808  798  800  842\n [61]  809  807  826  810  801  794  771  796  790  787  775  751  783  811  768\n [76]  779  795  770  821  830  767  772  791  781  773  777  814  778  782  837\n [91]  759  846  797  835  832  793  803  834  785  831  820  812  824  728  760\n[106]  762  753  758  764  741  709  735  749  752  761  750  776  766  789  763\n[121]  864  858  869  886  844  863  916  890  872  907  926  935  933  906  905\n[136]  912  972  996 1009  961  952  981  917 1011 1071 1920 3245 3805 3926 3284\n[151] 2700 2347 2078 2935 3040 1860 1437 1512 1720 1493 1026  928  874  833  850\n[166]   NA\n\n## Let's check that the empty strings from 'dat' are now NAs in 'dat2'\nwhich(dat[ , 2] == \"\")[1:10]\n\n [1] 312 313 314 315 317 318 319 320 322 323\n\ndat2[which(dat[, 2] == \"\")[1], ] # pull out a line with a missing string\n\n    X2010.08.02.18.55 X2336 X549 X2086 X666 X481 X298 X1624 X1732 X593 X222\n312  2010-08-03 10:31    NA   NA    NA   NA   NA   NA    NA    NA   NA   NA\n    X911 X261 X1730 X211 X365 X216 X438 X596 X206 X204 X270 X176 X1159 X1137\n312   NA   NA    NA   NA   NA   NA   NA   NA   NA   NA   NA   NA    NA    NA\n    X135 X2036 X138 X1038 X201 X610 X627 X195 X976 X151 X1830 X421 X1087 X1157\n312   NA    NA   NA    NA   NA   NA   NA   NA   NA   NA    NA   NA    NA    NA\n    X181 X267 X193 X391 X208 X614 X546 X186 X1391 X217 X230 X625 X376 X164 X329\n312   NA   NA   NA   NA   NA   NA   NA   NA    NA   NA   NA   NA   NA   NA   NA\n    X1043 X497 X440 X197 X287 X837 X226 X973\n312    NA   NA   NA   NA   NA   NA   NA   NA\n\n\nUsing colClasses is a good way to control how data are read in.\n\nsequ <- read.table(file.path('..', 'data', 'hivSequ.csv'),\n  sep = ',', header = TRUE,\n  colClasses = c('integer','integer','character',\n    'character','numeric','integer'))\n## let's make sure the coercion worked - sometimes R is obstinant\nsapply(sequ, class)\n\n  PatientID        Resp      PR.Seq      RT.Seq       VL.t0      CD4.t0 \n  \"integer\"   \"integer\" \"character\" \"character\"   \"numeric\"   \"integer\" \n\n## that made use of the fact that a data frame is a list\n\nNote that you can avoid reading in one or more columns by specifying NULL as the column class for those columns to be omitted. Also, specifying the colClasses argument explicitly should make for faster file reading. Finally, setting stringsAsFactors=FALSE is standard practice and is the default in R as of version 4.0. (readr::read_csv() has always set stringsAsFactors=FALSE).\nIf possible, it’s a good idea to look through the input file in the shell or in an editor before reading into R to catch such issues in advance. Using the UNIX command less on RTADataSub.csv would have revealed these various issues, but note that RTADataSub.csv is a 1000-line subset of a much larger file of data available from the kaggle.com website. So more sophisticated use of UNIX utilities (as we will see in Unit 3) is often useful before trying to read something into a program.\nThe basic function scan() simply reads everything in, ignoring lines, which works well and very quickly if you are reading in a numeric vector or matrix. scan() is also useful if your file is free format - i.e., if it’s not one line per observation, but just all the data one value after another; in this case you can use scan() to read it in and then format the resulting character or numeric vector as a matrix with as many columns as fields in the dataset. Remember that the default is to fill the matrix by column.\nIf the file is not nicely arranged by field (e.g., if it has ragged lines), we’ll need to do some more work. readLines() will read in each line into a separate character vector, after which we can process the lines using text manipulation. Here’s an example from some US meteorological data where I know from metadata (not provided here) that the 4-11th values are an identifier, the 17-20th are the year, the 22-23rd the month, etc.\n\ndat <- readLines(file.path('..', 'data', 'precip.txt'))\nid <- as.factor(substring(dat, 4, 11) )\nyear <- substring(dat, 18, 21)\nyear[1:5]\n\n[1] \"2010\" \"2010\" \"2010\" \"2010\" \"2010\"\n\nclass(year)\n\n[1] \"character\"\n\nyear <- as.integer(substring(dat, 18, 21))\nmonth <- as.integer(substring(dat, 22, 23))\nnvalues <- as.integer(substring(dat, 28, 30))\n\nActually, that file, precip.txt, is in a fixed-width format (i.e., every element in a given column has the exact same number of characters),so reading in using read.fwf() would be a good strategy."
  },
  {
    "objectID": "units/unit2-dataTech.html#connections",
    "href": "units/unit2-dataTech.html#connections",
    "title": "Data technologies, formats, and structures",
    "section": "Connections",
    "text": "Connections\nR allows you to read in not just from a file but from a more general construct called a connection. This can include reading in text from the output of running a shell command and from unzipping a file on the fly.\nHere are some examples of connections:\n\ndat <- readLines(pipe(\"ls -al\"))\ndat <- read.table(pipe(\"unzip dat.zip\"))\ndat <- read.csv(gzfile(\"dat.csv.gz\"))\ndat <- readLines(\"http://www.stat.berkeley.edu/~paciorek/index.html\")\n\nIn some cases, you might need to create the connection using url() or using the curl() function from the curl package. Though for the example here, simply passing the URL to readLines() does work. (In general, curl::curl() provides some nice features for reading off the internet.)\n\nwikip1 <- readLines(\"https://wikipedia.org\")\nwikip2 <- readLines(url(\"https://wikipedia.org\"))\nlibrary(curl)\n\nUsing libcurl 7.68.0 with GnuTLS/3.6.13\n\nwikip3 <- readLines(curl(\"https://wikipedia.org\"))\n\nIf a file is large, we may want to read it in in chunks (of lines), do some computations to reduce the size of things, and iterate. This is referred to as online processing. read.table(), read.fwf() and readLines() all have the arguments that let you read in a fixed number of lines. To read-on-the-fly in blocks, we need to first establish the connection and then read from it sequentially. (If you don’t, you’ll read from the start of the file every time you read from the file.)\n\ncon <- file(file.path(\"..\", \"data\", \"precip.txt\"), \"r\")\n## \"r\" for 'read' - you can also open files for writing with \"w\"\n## (or \"a\" for appending)\nclass(con)\nblockSize <- 1000 # obviously this would be large in any real application\nnLines <- 300000\nfor(i in 1:ceiling(nLines / blockSize)){\n    lines <- readLines(con, n = blockSize)\n    # manipulate the lines and store the key stuff\n}\nclose(con)\n\n \nHere’s an example of using curl() to do this for a file on the web.\n\nURL <- \"https://www.stat.berkeley.edu/share/paciorek/2008.csv.gz\"\ncon <- gzcon(curl(URL, open = \"r\"))\n## url() in place of curl() works too\nfor(i in 1:4) {   ## Read in first four chunks as an example\n    print(i)\n    print(system.time(tmp <- readLines(con, n = 100000)))\n    print(tmp[1])\n}\n\n[1] 1\n   user  system elapsed \n  0.467   0.005   0.472 \n[1] \"Year,Month,DayofMonth,DayOfWeek,DepTime,CRSDepTime,ArrTime,CRSArrTime,UniqueCarrier,FlightNum,TailNum,ActualElapsedTime,CRSElapsedTime,AirTime,ArrDelay,DepDelay,Origin,Dest,Distance,TaxiIn,TaxiOut,Cancelled,CancellationCode,Diverted,CarrierDelay,WeatherDelay,NASDelay,SecurityDelay,LateAircraftDelay\"\n[1] 2\n   user  system elapsed \n  0.424   0.008   0.433 \n[1] \"2008,1,29,2,1938,1935,2308,2257,XE,7676,N11176,150,142,104,11,3,SLC,OKC,866,5,41,0,,0,NA,NA,NA,NA,NA\"\n[1] 3\n   user  system elapsed \n  0.422   0.005   0.426 \n[1] \"2008,1,20,7,1540,1525,1651,1637,OO,5703,N227SW,71,72,58,14,15,SBA,SJC,234,5,8,0,,0,NA,NA,NA,NA,NA\"\n[1] 4\n   user  system elapsed \n  0.433   0.009   0.441 \n[1] \"2008,1,2,3,1313,1250,1443,1425,WN,440,N461WN,150,155,138,18,23,MCO,STL,880,3,9,0,,0,2,0,0,0,16\"\n\nclose(con)\n\nMore details on sequential (on-line) processing of large files can be found in the tutorial on large datasets mentioned in the reference list above.\nOne cool trick that can come in handy is to create a text connection. This lets you ‘read’ from an R character vector as if it were a text file and could be handy for processing text. For example, you could then use read.fwf() applied to con.\n\ndat <- readLines('../data/precip.txt')\ncon <- textConnection(dat[1], \"r\")\nread.fwf(con, c(3,8,4,2,4,2))\n\n   V1      V2   V3 V4   V5 V6\n1 DLY 1000807 PRCP HI 2010  2\n\n\nWe can create connections for writing output too. Just make sure to open the connection first."
  },
  {
    "objectID": "units/unit2-dataTech.html#file-paths",
    "href": "units/unit2-dataTech.html#file-paths",
    "title": "Data technologies, formats, and structures",
    "section": "File paths",
    "text": "File paths\nA few notes on file paths, related to ideas of reproducibility.\n\nIn general, you don’t want to hard-code absolute paths into your code files because those absolute paths won’t be available on the machines of anyone you share the code with. Instead, use paths relative to the directory the code file is in, or relative to a baseline directory for the project, e.g.:\n\n\ndat <- read.csv('../data/cpds.csv')\n\nBe careful with the directory separator in Windows files: you can either do C:\\\\mydir\\\\file.txt or C:/mydir/file.txt, but not C:\\mydir\\file.txt, and note the next comment about avoiding use of ‘\\’ for portability.\nUsing UNIX style directory separators will work in Windows, Mac or Linux, but using Windows style separators is not portable across operating systems.\n\n\n## good: will work on Windows\ndat <- read.csv('../data/cpds.csv')\n## bad: won't work on Mac or Linux\ndat <- read.csv('..\\\\data\\\\cpds.csv')  \n\nEven better, use file.path() so that paths are constructed specifically for the operating system the user is using:\n\n\n## good: operating-system independent\ndat <- read.csv(file.path('..', 'data', 'cpds.csv'))"
  },
  {
    "objectID": "units/unit2-dataTech.html#the-readr-package",
    "href": "units/unit2-dataTech.html#the-readr-package",
    "title": "Data technologies, formats, and structures",
    "section": "The readr package",
    "text": "The readr package\nreadr is intended to deal with some of the shortcomings of the base R functions, such as leaving column names unmodified, and recognizing dates/times. It reads data in much more quickly than the base R equivalents. See this blog post. Some of the readr functions that are analogs to the comparably-named base R functions are read_csv(), read_fwf(), read_lines(), and read_table().\nLet’s try out read_csv() on the airline dataset used in the R bootcamp.\n\nlibrary(readr)\n\n\nAttaching package: 'readr'\n\n\nThe following object is masked from 'package:curl':\n\n    parse_date\n\n## I'm violating the rule about absolute paths here!!\n## (airline.csv is big enough that I don't want to put it in the\n##    course repository)\ndir <- \"../data\"\nsystem.time(dat <- read.csv(file.path(dir, 'airline.csv'), stringsAsFactors = FALSE)) \n\n   user  system elapsed \n  3.246   0.107   3.356 \n\nsystem.time(dat2 <- read_csv(file.path(dir, 'airline.csv')))\n\nRows: 539895 Columns: 29\n\n\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (5): UniqueCarrier, TailNum, Origin, Dest, CancellationCode\ndbl (24): Year, Month, DayOfMonth, DayOfWeek, DepTime, CRSDepTime, ArrTime, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n   user  system elapsed \n  3.895   0.124   1.252"
  },
  {
    "objectID": "units/unit2-dataTech.html#reading-data-quickly",
    "href": "units/unit2-dataTech.html#reading-data-quickly",
    "title": "Data technologies, formats, and structures",
    "section": "Reading data quickly",
    "text": "Reading data quickly\nIn addition to the tips above, there are a number of packages that allow one to read large data files quickly, in particular data.table, arrow, and fst. In general, these provide the ability to load datasets into R without having them in memory, but rather stored in clever ways on disk that allow for fast access. Metadata is stored in R. More on this in the unit on big data and in the tutorial on large datasets mentioned in the reference list above."
  },
  {
    "objectID": "units/unit2-dataTech.html#writing-output-to-files",
    "href": "units/unit2-dataTech.html#writing-output-to-files",
    "title": "Data technologies, formats, and structures",
    "section": "Writing output to files",
    "text": "Writing output to files\nFunctions for text output are generally analogous to those for input. write.table(), write.csv(), and writeLines() are analogs of read.table(), read.csv(), and readLines(). write_csv() is the readr version of write.csv. write() can be used to write a matrix to a file, specifying the number of columns desired. cat() can be used when you want fine control of the format of what is written out and allows for outputting to a connection (e.g., a file).\ntoJSON() in the jsonlite package will output R objects as JSON. One use of JSON as output from R would be to serialize the information in an R object such that it could be read into another program.\nAnd of course you can always save to an R data file (a binary file format) using save.image() (to save all the objects in the workspace or save() to save only some objects. Happily this is platform-independent so can be used to transfer R objects between different OS."
  },
  {
    "objectID": "units/unit2-dataTech.html#formatting-output",
    "href": "units/unit2-dataTech.html#formatting-output",
    "title": "Data technologies, formats, and structures",
    "section": "Formatting output",
    "text": "Formatting output\ncat() is a good choice for printing a message to the screen, often better than print(), which is an object-oriented method. You generally won’t have control over how the output of a print() statement is actually printed.\n\nval <- 1.5\ncat('My value is ', val, '.\\n', sep = '')\n\nMy value is 1.5.\n\nprint(paste('My value is ', val, '.', sep = ''))\n\n[1] \"My value is 1.5.\"\n\n\nWe can do more to control formatting with cat():\n\n## input\nx <- 7\nn <- 5\n## display powers\ncat(\"Powers of\", x, \"\\n\")\n\nPowers of 7 \n\ncat(\"exponent   result\\n\\n\")\n\nexponent   result\n\nresult <- 1\nfor (i in 1:n) {\n    result <- result * x\n    cat(format(i, width = 8), format(result, width = 10),\n            \"\\n\", sep = \"\")\n}\n\n       1         7\n       2        49\n       3       343\n       4      2401\n       5     16807\n\n\nOne thing to be aware of when writing out numerical data is how many digits are included. For example, the default with write and cat is the number of digits that R displays to the screen, controlled by options()$digits. But note that options()$digits seems to have some variability in behavior across operating systems. If you want finer control, use sprintf. For example, here we print out temperatures as reals (“f”=floating point) with four decimal places and nine total character positions, followed by a C for Celsius:\n\ntemps <- c(12.5, 37.234324, 1342434324.79997234, 2.3456e-6, 1e10)\nsprintf(\"%9.4f C\", temps)\n\n[1] \"  12.5000 C\"        \"  37.2343 C\"        \"1342434324.8000 C\" \n[4] \"   0.0000 C\"        \"10000000000.0000 C\"\n\ncity <- \"Boston\"\nsprintf(\"The temperature in %s was %.4f C.\", city, temps[1])\n\n[1] \"The temperature in Boston was 12.5000 C.\"\n\nsprintf(\"The temperature in %s was %9.4f C.\", city, temps[1])\n\n[1] \"The temperature in Boston was   12.5000 C.\"\n\n\nTo change the number of digits printed to the screen, do options(digits = 5) or specify as an argument to print or use sprintf."
  },
  {
    "objectID": "units/unit2-dataTech.html#reading-html",
    "href": "units/unit2-dataTech.html#reading-html",
    "title": "Data technologies, formats, and structures",
    "section": "Reading HTML",
    "text": "Reading HTML\nHTML (Hypertext Markup Language) is the standard markup language used for displaying content in a web browser. In simple webpages (ignoring the more complicated pages that involve Javascript), what you see in your browser is simply a rendering (by the browser) of a text file containing HTML.\nHowever, instead of rendering the HTML in a browser, we might want to use code to extract information from the HTML.\nLet’s see a brief example of reading in HTML tables.\nNote that before doing any coding, it can be helpful to look at the raw HTML source code for a given page. We can explore the underlying HTML source in advance of writing our code by looking at the page source directly in the browser (e.g., in Firefox under the 3-lines “open menu” symbol, see Web Developer (or More Tools) -> Page Source and in Chrome View -> Developer -> View Source), or by downloading the webpage and looking at it in an editor, although in some cases (such as the nytimes.com case), what we might see is a lot of JavaScript.\nOne lesson here is not to write a lot of your own code to do something that someone else has probably already written a package for. We’ll use the rvest package.\n\nlibrary(rvest, quietly = TRUE)  # uses xml2\n\n\nAttaching package: 'rvest'\n\n\nThe following object is masked from 'package:readr':\n\n    guess_encoding\n\nURL <- \"https://en.wikipedia.org/wiki/List_of_countries_and_dependencies_by_population\"\nhtml <- read_html(URL)\ntbls <- html_table(html_elements(html, \"table\"))\nsapply(tbls, nrow)\n\n[1] 242  12\n\npop <- tbls[[1]]\nhead(pop)\n\n# A tibble: 6 × 9\n  Rank  `Country / Dependency` Conti…¹ Popul…² Perce…³ Date  Sourc…⁴ Notes ``   \n  <chr> <chr>                  <chr>   <chr>   <chr>   <chr> <chr>   <chr> <lgl>\n1 –     World                  All     7,976,… 100%    6 Se… UN pro… \"\"    NA   \n2 1     China                  Asia    1,412,… 17.7%   31 D… Offici… \"The… NA   \n3 2     India                  Asia    1,375,… 17.2%   1 Ma… Offici… \"The… NA   \n4 3     United States          North … 333,06… 4.18%   6 Se… Nation… \"The… NA   \n5 4     Indonesia              Asia[b] 275,77… 3.46%   1 Ju… Offici… \"\"    NA   \n6 5     Pakistan               Asia    235,82… 2.96%   1 Ju… UN pro… \"The… NA   \n# … with abbreviated variable names ¹​Continent, ²​Population,\n#   ³​`Percentage of the world`, ⁴​`Source (official or from the United Nations)`\n\n\n(Caution here – notice what format the columns are stored in…)\nread_html() works by reading in the HTML as text and then parsing it to build up a tree containing the HTML elements. Then html_elements() finds the HTML tables and html_table() converts them to data frames. rvest is part of the tidyverse, so it’s often used with piping, e.g.,\n\n## Turns out that html_table can take the entire html doc as input\ntbls <- URL |> read_html() |> html_table()\n\nIt’s often useful to be able to extract the hyperlinks in an HTML document. We’ll find the link using CSS selectors, which allow you to search for elements within HTML:\n\nURL <- \"http://www1.ncdc.noaa.gov/pub/data/ghcn/daily/by_year\"\n## approach 1: search for elements with 'href' attribute\nlinks <- read_html(URL) %>% html_elements(\"[href]\") %>% html_attr('href')\n## approach 2: search for HTML 'a' tags\nlinks <- read_html(URL) %>% html_elements(\"a\") %>% html_attr('href')\nhead(links, n = 10)\n\n [1] \"?C=N;O=D\"              \"?C=M;O=A\"              \"?C=S;O=A\"             \n [4] \"?C=D;O=A\"              \"/pub/data/ghcn/daily/\" \"1750.csv.gz\"          \n [7] \"1763.csv.gz\"           \"1764.csv.gz\"           \"1765.csv.gz\"          \n[10] \"1766.csv.gz\"          \n\n\nMore generally, we may want to read an HTML document, parse it into its components (i.e., the HTML elements), and navigate through the tree structure of the HTML. Here we use the XPath language to specify elements rather than CSS selectors. XPath can also be used for navigating through XML documents.\n\n## find all 'a' elements that have attribute 'href'; then\n## extract the 'href' attribute\nlinks <- read_html(URL) %>% html_elements(xpath = \"//a[@href]\") %>%\n    html_attr('href')\nhead(links)\n\n[1] \"?C=N;O=D\"              \"?C=M;O=A\"              \"?C=S;O=A\"             \n[4] \"?C=D;O=A\"              \"/pub/data/ghcn/daily/\" \"1750.csv.gz\"          \n\n## we can extract various information\nlistOfANodes <- read_html(URL) %>% html_elements(xpath = \"//a[@href]\")\nlistOfANodes %>% html_attr('href') %>% head(n = 10)\n\n [1] \"?C=N;O=D\"              \"?C=M;O=A\"              \"?C=S;O=A\"             \n [4] \"?C=D;O=A\"              \"/pub/data/ghcn/daily/\" \"1750.csv.gz\"          \n [7] \"1763.csv.gz\"           \"1764.csv.gz\"           \"1765.csv.gz\"          \n[10] \"1766.csv.gz\"          \n\nlistOfANodes %>% html_name() %>% head(n = 10)\n\n [1] \"a\" \"a\" \"a\" \"a\" \"a\" \"a\" \"a\" \"a\" \"a\" \"a\"\n\nlistOfANodes %>% html_text()  %>% head(n = 10)\n\n [1] \"Name\"             \"Last modified\"    \"Size\"             \"Description\"     \n [5] \"Parent Directory\" \"1750.csv.gz\"      \"1763.csv.gz\"      \"1764.csv.gz\"     \n [9] \"1765.csv.gz\"      \"1766.csv.gz\"     \n\n\nHere’s another example of extracting specific components of information from a webpage (results not shown, since headlines will vary from day to day).\n\nURL <- \"https://www.nytimes.com\"\nheadlines2 <- read_html(URL) %>% html_elements(\"h2\") %>% html_text()\nhead(headlines2)\nheadlines3 <- read_html(URL) %>% html_elements(\"h3\") %>% html_text()\nhead(headlines3)"
  },
  {
    "objectID": "units/unit2-dataTech.html#xml",
    "href": "units/unit2-dataTech.html#xml",
    "title": "Data technologies, formats, and structures",
    "section": "XML",
    "text": "XML\nXML is a markup language used to store data in self-describing (no metadata needed) format, often with a hierarchical structure. It consists of sets of elements (also known as nodes because they generally occur in a hierarchical structure and therefore have parents, children, etc.) with tags that identify/name the elements, with some similarity to HTML. Some examples of the use of XML include serving as the underlying format for Microsoft Office and Google Docs documents and for the KML language used for spatial information in Google Earth.\nHere’s a brief example. The book with id attribute bk101 is an element; the author of the book is also an element that is a child element of the book. The id attribute allows us to uniquely identify the element.\n    <?xml version=\"1.0\"?>\n    <catalog>\n       <book id=\"bk101\">\n          <author>Gambardella, Matthew</author>\n          <title>XML Developer's Guide</title>\n          <genre>Computer</genre>\n          <price>44.95</price>\n          <publish_date>2000-10-01</publish_date>\n          <description>An in-depth look at creating applications with XML.</description>\n       </book>\n       <book id=\"bk102\">\n          <author>Ralls, Kim</author>\n          <title>Midnight Rain</title>\n          <genre>Fantasy</genre>\n          <price>5.95</price>\n          <publish_date>2000-12-16</publish_date>\n         <description>A former architect battles corporate zombies, an evil sorceress, and her own childhood to become queen of the world.</description>\n       </book>\n    </catalog>\nWe can read XML documents into R using xml2::read_xml() and then manipulate it using other functions from the xml2 package. Here’s an example of working with lending data from the Kiva lending non-profit. You can see the XML format in a browser at\nhttp://api.kivaws.org/v1/loans/newest.xml.\nXML documents have a tree structure with information at nodes. As above with HTML, one can use the XPath language for navigating the tree and finding and extracting information from the node(s) of interest.\nHere is some example code for extracting loan info from the Kiva data. We’ll first show the ‘brute force’ approach of working with the data as a list and then the better approach of using XPath.\n\nlibrary(xml2)\ndoc <- read_xml(\"https://api.kivaws.org/v1/loans/newest.xml\")\ndata <- as_list(doc)\nnames(data)\n\n[1] \"response\"\n\nnames(data$response)\n\n[1] \"paging\" \"loans\" \n\nlength(data$response$loans)\n\n[1] 20\n\ndata$response$loans[[2]][c('name', 'activity',\n                           'sector', 'location', 'loan_amount')]\n\n$name\n$name[[1]]\n[1] \"Piseth\"\n\n\n$activity\n$activity[[1]]\n[1] \"Home Energy\"\n\n\n$sector\n$sector[[1]]\n[1] \"Personal Use\"\n\n\n$location\n$location$country_code\n$location$country_code[[1]]\n[1] \"KH\"\n\n\n$location$country\n$location$country[[1]]\n[1] \"Cambodia\"\n\n\n$location$town\n$location$town[[1]]\n[1] \"Siem Reap\"\n\n\n$location$geo\n$location$geo$level\n$location$geo$level[[1]]\n[1] \"town\"\n\n\n$location$geo$pairs\n$location$geo$pairs[[1]]\n[1] \"13.366667 103.85\"\n\n\n$location$geo$type\n$location$geo$type[[1]]\n[1] \"point\"\n\n\n\n\n$loan_amount\n$loan_amount[[1]]\n[1] \"800\"\n\n## alternatively, extract only the 'loans' info (and use pipes)\nloansNode <- doc %>% html_elements('loans')\nloanInfo <- loansNode %>% xml_children() %>% as_list()\nlength(loanInfo)\n\n[1] 20\n\nnames(loanInfo[[1]])\n\n [1] \"id\"                       \"name\"                    \n [3] \"description\"              \"status\"                  \n [5] \"funded_amount\"            \"basket_amount\"           \n [7] \"image\"                    \"activity\"                \n [9] \"sector\"                   \"themes\"                  \n[11] \"use\"                      \"location\"                \n[13] \"partner_id\"               \"posted_date\"             \n[15] \"planned_expiration_date\"  \"loan_amount\"             \n[17] \"borrower_count\"           \"lender_count\"            \n[19] \"bonus_credit_eligibility\" \"tags\"                    \n\nnames(loanInfo[[1]]$location)\n\n[1] \"country_code\" \"country\"      \"town\"         \"geo\"         \n\n## suppose we only want the country locations of the loans (using XPath)\nxml_find_all(loansNode, '//location//country')\n\n{xml_nodeset (20)}\n [1] <country>Cambodia</country>\n [2] <country>Cambodia</country>\n [3] <country>Cambodia</country>\n [4] <country>Cambodia</country>\n [5] <country>Ecuador</country>\n [6] <country>Cambodia</country>\n [7] <country>Vietnam</country>\n [8] <country>Vietnam</country>\n [9] <country>Guatemala</country>\n[10] <country>Timor-Leste</country>\n[11] <country>El Salvador</country>\n[12] <country>Colombia</country>\n[13] <country>Colombia</country>\n[14] <country>Kenya</country>\n[15] <country>Kenya</country>\n[16] <country>Kenya</country>\n[17] <country>Kenya</country>\n[18] <country>Colombia</country>\n[19] <country>Colombia</country>\n[20] <country>Honduras</country>\n\nxml_find_all(loansNode, '//location//country') %>% xml_text()\n\n [1] \"Cambodia\"    \"Cambodia\"    \"Cambodia\"    \"Cambodia\"    \"Ecuador\"    \n [6] \"Cambodia\"    \"Vietnam\"     \"Vietnam\"     \"Guatemala\"   \"Timor-Leste\"\n[11] \"El Salvador\" \"Colombia\"    \"Colombia\"    \"Kenya\"       \"Kenya\"      \n[16] \"Kenya\"       \"Kenya\"       \"Colombia\"    \"Colombia\"    \"Honduras\"   \n\n## or extract the geographic coordinates\nxml_find_all(loansNode, '//location//geo/pairs')\n\n{xml_nodeset (20)}\n [1] <pairs>13.09573 103.202206</pairs>\n [2] <pairs>13.366667 103.85</pairs>\n [3] <pairs>13.366667 103.85</pairs>\n [4] <pairs>13.366667 103.85</pairs>\n [5] <pairs>0.339176 -78.122234</pairs>\n [6] <pairs>12.5 104</pairs>\n [7] <pairs>21.40639 103.032155</pairs>\n [8] <pairs>21.40639 103.032155</pairs>\n [9] <pairs>14.421598 -91.404825</pairs>\n[10] <pairs>-8.930576 125.396797</pairs>\n[11] <pairs>13.833333 -88.916667</pairs>\n[12] <pairs>4 -72</pairs>\n[13] <pairs>5.472709 -74.667984</pairs>\n[14] <pairs>0.516667 35.283333</pairs>\n[15] <pairs>-0.785561 35.33914</pairs>\n[16] <pairs>0.035164 36.364292</pairs>\n[17] <pairs>-0.777114 34.945839</pairs>\n[18] <pairs>11.004107 -74.806981</pairs>\n[19] <pairs>10.391049 -75.479426</pairs>\n[20] <pairs>15.047154 -85.125776</pairs>"
  },
  {
    "objectID": "units/unit2-dataTech.html#json",
    "href": "units/unit2-dataTech.html#json",
    "title": "Data technologies, formats, and structures",
    "section": "JSON",
    "text": "JSON\nJSON files are structured as “attribute-value” pairs (aka “key-value” pairs), often with a hierarchical structure. Here’s a brief example:\n    {\n      \"firstName\": \"John\",\n      \"lastName\": \"Smith\",\n      \"isAlive\": true,\n      \"age\": 25,\n      \"address\": {\n        \"streetAddress\": \"21 2nd Street\",\n        \"city\": \"New York\",\n        \"state\": \"NY\",\n        \"postalCode\": \"10021-3100\"\n      },\n      \"phoneNumbers\": [\n        {\n          \"type\": \"home\",\n          \"number\": \"212 555-1234\"\n        },\n        {\n          \"type\": \"office\",\n          \"number\": \"646 555-4567\"\n        }\n      ],\n      \"children\": [],\n      \"spouse\": null\n    }\nA set of key-value pairs is a named array and is placed inside braces (squiggly brackets). Note the nestedness of arrays within arrays (e.g., address within the overarching person array and the use of square brackets for unnamed arrays (i.e., vectors of information), as well as the use of different types: character strings, numbers, null, and (not shown) boolean/logical values. JSON and XML can be used in similar ways, but JSON is less verbose than XML.\nWe can read JSON into R using fromJSON() in the jsonlite package. Let’s play again with the Kiva data. The same data that we had worked with in XML format is also available in JSON format: http://api.kivaws.org/v1/loans/newest.json.\n\nlibrary(jsonlite)\ndata <- fromJSON(\"http://api.kivaws.org/v1/loans/newest.json\")\nclass(data)\n\n[1] \"list\"\n\nnames(data)\n\n[1] \"paging\" \"loans\" \n\nclass(data$loans) # nice!\n\n[1] \"data.frame\"\n\nhead(data$loans)\n\n       id           name languages      status funded_amount basket_amount\n1 2426966            Hon        en fundraising            25            25\n2 2426991         Piseth        en fundraising             0             0\n3 2444685         Sareth        en fundraising             0             0\n4 2444689           Kheu        en fundraising             0             0\n5 2425164 Maria Consuelo    es, en fundraising             0             0\n6 2444680         Sophat        en fundraising             0             0\n  image.id image.template_id    activity       sector\n1  4946099                 1 Home Energy Personal Use\n2  4934240                 1 Home Energy Personal Use\n3  4947188                 1      Crafts         Arts\n4  4947194                 1      Crafts         Arts\n5  4914714                 1  Restaurant         Food\n6  4947060                 1     Farming  Agriculture\n                                  themes\n1 Social Enterprise, Clean Energy, Green\n2 Social Enterprise, Clean Energy, Green\n3                      Underfunded Areas\n4                      Underfunded Areas\n5                                   NULL\n6                                   NULL\n                                                                                            use\n1          to buy a biodigester that reduces kitchen smoke with biogas replacing the wood fuel.\n2          to buy a biodigester that reduces kitchen smoke with biogas replacing the wood fuel.\n3    to purchase more rattan and tropical fruit trees for making baskets to sell in the market.\n4    to purchase more rattan and tropical fruit trees for making baskets to sell in the market.\n5 to acquire cooking utensils and an industrial stove, as well as make renovations to her shop.\n6                             to buy rice seeds and fertilizer to support her farming business.\n  location.country_code location.country location.town location.geo.level\n1                    KH         Cambodia    Battambang               town\n2                    KH         Cambodia     Siem Reap               town\n3                    KH         Cambodia     Siem Reap               town\n4                    KH         Cambodia     Siem Reap               town\n5                    EC          Ecuador        Ibarra               town\n6                    KH         Cambodia        Pursat               town\n   location.geo.pairs location.geo.type partner_id          posted_date\n1 13.09573 103.202206             point        636 2022-09-07T15:40:51Z\n2    13.366667 103.85             point        636 2022-09-07T15:40:51Z\n3    13.366667 103.85             point        499 2022-09-07T15:40:51Z\n4    13.366667 103.85             point        499 2022-09-07T15:40:51Z\n5 0.339176 -78.122234             point        457 2022-09-07T15:40:50Z\n6            12.5 104             point        499 2022-09-07T15:40:50Z\n  planned_expiration_date loan_amount borrower_count lender_count\n1    2022-10-12T15:40:51Z         800              1            1\n2    2022-10-12T15:40:51Z         800              1            0\n3    2022-10-12T15:40:51Z         800              1            0\n4    2022-10-12T15:40:51Z         525              1            0\n5    2022-10-12T15:40:50Z        1600              1            0\n6    2022-10-12T15:40:50Z         500              1            0\n  bonus_credit_eligibility\n1                    FALSE\n2                    FALSE\n3                     TRUE\n4                     TRUE\n5                    FALSE\n6                     TRUE\n                                                                         tags\n1                                                            #Eco-friendly, 9\n2                                                                        NULL\n3                            #Woman-Owned Business, #Supporting Family, 6, 31\n4                            #Woman-Owned Business, #Supporting Family, 6, 31\n5 #Woman-Owned Business, #Biz Durable Asset, #Repair Renew Replace, 6, 35, 39\n6                          #Woman-Owned Business, #Vegan, #Elderly, 6, 10, 13\n\ndata$loans[1, 'location.geo.pairs'] # hmmm...\n\nNULL\n\ndata$loans[1, 'location']\n\n  country_code  country       town geo.level           geo.pairs geo.type\n1           KH Cambodia Battambang      town 13.09573 103.202206    point\n\n\nOne disadvantage of JSON is that it is not set up to deal with missing values, infinity, etc."
  },
  {
    "objectID": "units/unit2-dataTech.html#webscraping-and-web-apis",
    "href": "units/unit2-dataTech.html#webscraping-and-web-apis",
    "title": "Data technologies, formats, and structures",
    "section": "Webscraping and web APIs",
    "text": "Webscraping and web APIs\nHere we’ll see some examples of making requests over the Web to get data. We’ll use APIs to systematically query a website for information. Ideally, but not always, the API will be documented. In many cases that simply amounts to making an HTTP GET request, which is done by constructing a URL.\nThe packages RCurl and httr are useful for a wide variety of such functionality. Note that much of the functionality I describe below is also possible within the shell using either wget or curl.\n\nWebscraping ethics and best practices\nWebscraping is the process of extracting data from the web, either directly from a website or using a web API (application programming interface).\n\nShould you webscrape? In general, if we can avoid webscraping (particularly if there is not an API) and instead directly download a data file from a website, that is greatly preferred.\nMay you webscrape? Before you set up any automated downloading of materials/data from the web you should make sure that what you are about to do is consistent with the rules provided by the website.\n\nSome places to look for information on what the website allows are:\n\nlegal pages such as Terms of Service or Terms and Conditions on the website.\ncheck the robots.txt file (e.g., https://scholar.google.com/robots.txt) to see what a web crawler is allowed to do, and whether the site requires a particular delay between requests to the sites\npotentially contact the site owner if you plan to scrape a large amount of data\n\nHere are some links with useful information:\n\nA blog post overview on webscraping and robots.txt\nBlog post on webscraping ethics\nSome information on how to understand a robots.txt file\n\nTips for when you make automated requests:\n\nWhen debugging code that processes the result of such a request, just run the request once, save (i.e., cache) the result, and then work on the processing code applied to the result. Don’t make the same request over and over again.\nIn many cases you will want to include a time delay between your automated requests to a site, including if you are not actually crawling a site but just want to automate a small number of queries.\n\n\n\nWhat is HTTP?\nHTTP (hypertext transfer protocol) is a system for communicating information from a server (i.e., the website of interest) to a client (e.g., your laptop). The client sends a request and the server sends a response.\nWhen you go to a website in a browser, your browser makes an HTTP GET request to the website. Similarly, when we did some downloading of html from webpages above, we used an HTTP GET request.\nAnytime the URL you enter includes parameter information after a question mark (www.somewebsite.com?param1=arg1&param2=arg2), you are using an API.\nThe response to an HTTP request will include a status code, which can be interpreted based on this information.\nThe response will generally contain content in the form of text (e.g., HTML, XML, JSON) or raw bytes.\n\n\nAPIs: REST- and SOAP-based web services\nIdeally a web service documents their API (Applications Programming Interface) that serves data or allows other interactions. REST and SOAP are popular API standards/styles. Both REST and SOAP use HTTP requests; we’ll focus on REST as it is more common and simpler. When using REST, we access resources, which might be a Facebook account or a database of stock quotes. The API will (hopefully) document what information it expects from the user and will return the result in a standard format (often a particular file format rather than producing a webpage).\nOften the format of the request is a URL (aka an endpoint) plus a query string, passed as a GET request. Let’s search for plumbers near Berkeley, and we’ll see the GET request, in the form:\nhttps://www.yelp.com/search?find_desc=plumbers&find_loc=Berkeley+CA&ns=1\n\nthe query string begins with ?\nthere are one or more Parameter=Argument pairs\npairs are separated by &\n+ is used in place of each space\n\nLet’s see an example of accessing economic data from the World Bank, using the documentation for their API. Following the API call structure, we can download (for example), data on various countries. The documentation indicates that our REST-based query can use either a URL structure or an argument-based structure.\n\n## Queries based on the documentation\napi_url <- \"http://api.worldbank.org/V2/incomeLevel/LIC/country\"\napi_args <- \"http://api.worldbank.org/V2/country?incomeLevel=LIC\"\n\n## Generalizing a bit\nreq <- \"http://api.worldbank.org/V2/country?incomeLevel=MIC&format=json\"\ndata <- fromJSON(req)\n## Be careful of data truncation/pagination\nreq <- \"http://api.worldbank.org/V2/country?incomeLevel=MIC&format=json&per_page=1000\"\ndata <- fromJSON(req)\n\n## Programmatic control\nbaseURL <- \"http://api.worldbank.org/V2/country\"\ngroup <- 'MIC'\nformat <- 'json'\nargs <- c(incomeLevel = group, format = format, per_page = 1000)\nurl <- paste0(baseURL, \"?\",\n    paste(  paste(names(args), args, sep = \"=\"),  collapse = \"&\"))\n   \ndata <- fromJSON(url)\nclass(data)\n\n[1] \"list\"\n\nlength(data)\n\n[1] 2\n\nnames(data)\n\nNULL\n\nhead(data[[2]])\n\n   id iso2Code           name region.id region.iso2code\n1 AGO       AO         Angola       SSF              ZG\n2 ALB       AL        Albania       ECS              Z7\n3 ARG       AR      Argentina       LCN              ZJ\n4 ARM       AM        Armenia       ECS              Z7\n5 ASM       AS American Samoa       EAS              Z4\n6 AZE       AZ     Azerbaijan       ECS              Z7\n                region.value adminregion.id adminregion.iso2code\n1        Sub-Saharan Africa             SSA                   ZF\n2      Europe & Central Asia            ECA                   7E\n3 Latin America & Caribbean             LAC                   XJ\n4      Europe & Central Asia            ECA                   7E\n5        East Asia & Pacific            EAP                   4E\n6      Europe & Central Asia            ECA                   7E\n                                  adminregion.value incomeLevel.id\n1        Sub-Saharan Africa (excluding high income)            LMC\n2     Europe & Central Asia (excluding high income)            UMC\n3 Latin America & Caribbean (excluding high income)            UMC\n4     Europe & Central Asia (excluding high income)            UMC\n5       East Asia & Pacific (excluding high income)            UMC\n6     Europe & Central Asia (excluding high income)            UMC\n  incomeLevel.iso2code   incomeLevel.value lendingType.id lendingType.iso2code\n1                   XN Lower middle income            IBD                   XF\n2                   XT Upper middle income            IBD                   XF\n3                   XT Upper middle income            IBD                   XF\n4                   XT Upper middle income            IBD                   XF\n5                   XT Upper middle income            LNX                   XX\n6                   XT Upper middle income            IBD                   XF\n  lendingType.value  capitalCity longitude latitude\n1              IBRD       Luanda    13.242 -8.81155\n2              IBRD       Tirane   19.8172  41.3317\n3              IBRD Buenos Aires  -58.4173 -34.6118\n4              IBRD      Yerevan    44.509  40.1596\n5    Not classified    Pago Pago  -170.691 -14.2846\n6              IBRD         Baku   49.8932  40.3834\n\n\nAPIs can change and disappear. Last year the example above involved the World Bank’s Climate Data API, which I can no longer find!\nAs another example, here we can see the US Treasury Department API, which allows us to construct queries for federal financial data.\nThe Nolan and Temple Lang book provides a number of examples of different ways of authenticating with web services that control access to the service.\nFinally, some web services allow us to pass information to the service in addition to just getting data or information. E.g., you can programmatically interact with your Facebook, Dropbox, and Google Drive accounts using REST based on HTTP POST, PUT, and DELETE requests. Authentication is of course important in these contexts and some times you would first authenticate with your login and password and receive a “token”. This token would then be used in subsequent interactions in the same session.\nI created your github.berkeley.edu accounts from Python by interacting with the GitHub API using the requests package.\n\n\nHTTP requests by deconstructing an (undocumented) API\nIn some cases an API may not be documented or we might be lazy and not use the documentation. Instead we might deconstruct the queries a browser makes and then mimic that behavior, in some cases having to parse HTML output to get at data. Note that if the webpage changes even a little bit, our carefully constructed query syntax may fail.\nLet’s look at some UN data (agricultural crop data). By going to\nhttp://data.un.org/Explorer.aspx?d=FAO, and clicking on “Crops”, we’ll see a bunch of agricultural products with “View data” links. Click on “apricots” as an example and you’ll see a “Download” button that allows you to download a CSV of the data. Let’s select a range of years and then try to download “by hand”. Sometimes we can right-click on the link that will download the data and directly see the URL that is being accessed and then one can deconstruct it so that you can create URLs programmatically to download the data you want.\nIn this case, we can’t see the full URL that is being used because there’s some Javascript involved. Therefore, rather than looking at the URL associated with a link we need to view the actual HTTP request sent by our browser to the server. We can do this using features of the browser (e.g., in Firefox see Web Developer -> Network and in Chrome View -> Developer -> Developer tools and choose the Network tab) (or right-click on the webpage and select Inspect and then Network). Based on this we can see that an HTTP GET request is being used with a URL such as:\nhttp://data.un.org/Handlers/DownloadHandler.ashx?DataFilter=itemCode:526;year:2012,2013,2014,2015,2016,2017&DataMartId=FAO&Format=csv&c=2,4,5,6,7&s=countryName:asc,elementCode:asc,year:desc.\nWe’e now able to easily download the data using that URL, which we can fairly easily construct using string processing in bash, R, or Python, such as this (here I just paste it together directly, but using more structured syntax such as I used for the World Bank example would be better):\n\n## example URL:\n## http://data.un.org/Handlers/DownloadHandler.ashx?DataFilter=itemCode:526;\n##year:2012,2013,2014,2015,2016,2017&DataMartId=FAO&Format=csv&c=2,4,5,6,7&\n##s=countryName:asc,elementCode:asc,year:desc\nitemCode <- 526\nbaseURL <- \"http://data.un.org/Handlers/DownloadHandler.ashx\"\nyrs <- paste(as.character(2012:2017), collapse = \",\")\nfilter <- paste0(\"?DataFilter=itemCode:\", itemCode, \";year:\", yrs)\nargs1 <- \"&DataMartId=FAO&Format=csv&c=2,3,4,5,6,7&\"\nargs2 <- \"s=countryName:asc,elementCode:asc,year:desc\"\nurl <- paste0(baseURL, filter, args1, args2)\n## if the website provided a CSV we could just do this:\n## apricots <- read.csv(url)\n## but it zips the file\ntemp <- tempfile()  ## give name for a temporary file\ndownload.file(url, temp)\ndat <- read.csv(unzip(temp))  ## using a connection (see Section 2)\n\nhead(dat)\n\n  Country.or.Area Element.Code                                         Element\n1     Afghanistan          432 Gross Production Index Number (2014-2016 = 100)\n2     Afghanistan          432 Gross Production Index Number (2014-2016 = 100)\n3     Afghanistan          432 Gross Production Index Number (2014-2016 = 100)\n4     Afghanistan          432 Gross Production Index Number (2014-2016 = 100)\n5     Afghanistan          432 Gross Production Index Number (2014-2016 = 100)\n6     Afghanistan          432 Gross Production Index Number (2014-2016 = 100)\n  Year  Unit  Value Value.Footnotes\n1 2017 index 202.19              Fc\n2 2016 index  27.45              Fc\n3 2015 index 134.50              Fc\n4 2014 index 138.05              Fc\n5 2013 index 138.05              Fc\n6 2012 index 128.08              Fc\n\n\nSo, what have we achieved?\n\nWe have a reproducible workflow we can share with others (perhaps ourself in the future).\nWe can automate the process of downloading many such files.\n\n\n\nMore details on HTTP requests\nA more sophisticated way to do the download is to pass the request in a structured way with named input parameters. This request is easier to construct programmatically. Here what is returned is a zip file, which is represented in R as a sequence of “raw” bytes. We can use httr’s GET(), followed by writing to disk and reading back in, as follows (for some reason knitr won’t print the output…):\n\nlibrary(httr)\n\n\nAttaching package: 'httr'\n\n\nThe following object is masked from 'package:curl':\n\n    handle_reset\n\noutput2 <- GET(baseURL, query = list(\n               DataFilter = paste0(\"itemCode:\", itemCode, \";year:\", yrs),\n               DataMartID = \"FAO\", Format = \"csv\", c = \"2,3,4,5,6,7\",\n               s = \"countryName:asc,elementCode:asc,year:desc\"))\ntemp <- tempfile()  ## give name for a temporary file\nwriteBin(content(output2, 'raw'), temp)  ## write out as zip file\ndat <- read.csv(unzip(temp))\nhead(dat)\n\n  Country.or.Area Element.Code                                         Element\n1     Afghanistan          432 Gross Production Index Number (2014-2016 = 100)\n2     Afghanistan          432 Gross Production Index Number (2014-2016 = 100)\n3     Afghanistan          432 Gross Production Index Number (2014-2016 = 100)\n4     Afghanistan          432 Gross Production Index Number (2014-2016 = 100)\n5     Afghanistan          432 Gross Production Index Number (2014-2016 = 100)\n6     Afghanistan          432 Gross Production Index Number (2014-2016 = 100)\n  Year  Unit  Value Value.Footnotes\n1 2017 index 202.19              Fc\n2 2016 index  27.45              Fc\n3 2015 index 134.50              Fc\n4 2014 index 138.05              Fc\n5 2013 index 138.05              Fc\n6 2012 index 128.08              Fc\n\n\nIn some cases we may need to send a lot of information as part of the URL in a GET request. If it gets to be too long (e.g,, more than 2048 characters) many web servers will reject the request. Instead we may need to use an HTTP POST request (POST requests are often used for submitting web forms). A typical request would have syntax like this search (using RCurl):\n\nif(url.exists('http://www.wormbase.org/db/searches/advanced/dumper')) {\n      x = postForm('http://www.wormbase.org/db/searches/advanced/dumper',\n              species=\"briggsae\",\n              list=\"\",\n              flank3=\"0\",\n              flank5=\"0\",\n              feature=\"Gene Models\",\n              dump = \"Plain TEXT\",\n              orientation = \"Relative to feature\",\n              relative = \"Chromsome\",\n              DNA =\"flanking sequences only\",\n              .cgifields = paste(c(\"feature\", \"orientation\", \"DNA\",\n                                   \"dump\",\"relative\"), collapse=\", \"))\n}\n\nUnfortunately that specific search doesn’t work because the server URL and/or API seem to have changed. But it gives you an idea of what the format would look like.\nhttr and RCurl can handle other kinds of HTTP requests such as PUT and DELETE. Finally, some websites use cookies to keep track of users and you may need to download a cookie in the first interaction with the HTTP server and then send that cookie with later interactions. More details are available in the Nolan and Temple Lang book.\n\n\nPackaged access to an API\nFor popular websites/data sources, a developer may have packaged up the API calls in a user-friendly fashion for use from R, Python or other software. For example there are Python (twitter) and R (twitteR) packages for interfacing with Twitter via its API.\nHere’s some example code for Python (the Python package seems to be more fully-featured than the R package). This looks up the US senators’ Twitter names and then downloads a portion of each of their timelines, i.e., the time series of their tweets. Note that Twitter has limits on how much one can download at once.\n\nimport json\nimport twitter\n\n# You will need to set the following variables with your\n# personal information.  To do this you will need to create\n# a personal account on Twitter (if you don't already have\n# one).  Once you've created an account, create a new\n# application here:\n#    https://dev.twitter.com/apps\n#\n# You can manage your applications here:\n#    https://apps.twitter.com/\n#\n# Select your application and then under the section labeled\n# \"Key and Access Tokens\", you will find the information needed\n# below.  Keep this information private.\nCONSUMER_KEY       = \"\"\nCONSUMER_SECRET    = \"\"\nOAUTH_TOKEN        = \"\"\nOAUTH_TOKEN_SECRET = \"\"\n\nauth = twitter.oauth.OAuth(OAUTH_TOKEN, OAUTH_TOKEN_SECRET,\n                           CONSUMER_KEY, CONSUMER_SECRET)\napi = twitter.Twitter(auth=auth)\n\n# get the list of senators\nsenators = api.lists.members(owner_screen_name=\"gov\", slug=\"us-senate\", count=100)\n\n# get all the senators' timelines\nnames = [d[\"screen_name\"] for d in senators[\"users\"]]\ntimelines = [api.statuses.user_timeline(screen_name=name, count = 500) \n             for name in names]\n\n# save information out to JSON\nwith open(\"senators-list.json\", \"w\") as f:\n    json.dump(senators, f, indent=4, sort_keys=True)\nwith open(\"timelines.json\", \"w\") as f:\n    json.dump(timelines, f, indent=4, sort_keys=True)\n\n\n\nAccessing dynamic pages\nSome websites dynamically change in reaction to the user behavior. In these cases you need a tool that can mimic the behavior of a human interacting with a site. Some options are:\n\nselenium (and the RSelenium wrapper for R) is a popular tool for doing this.\nsplash (and the splashr wrapper for R) is another approach.\nhtmlunit is another tool for this."
  },
  {
    "objectID": "units/unit2-dataTech.html#standard-data-structures-in-r-and-python",
    "href": "units/unit2-dataTech.html#standard-data-structures-in-r-and-python",
    "title": "Data technologies, formats, and structures",
    "section": "Standard data structures in R and Python",
    "text": "Standard data structures in R and Python\n\nIn R and Python, one often ends up working with dataframes, lists, and vectors/matrices/arrays/tensors.\nIn R, if we are not working with rectangular datasets or standard numerical objects, we often end up using lists or enhanced versions of lists, sometimes with deeply nested structures.\nIn Python we commonly work with data structures that are part of additional packages, in particular numpy arrays and pandas dataframes.\nDictionaries in Python allow for easy use of key-value pairs where one can access values based on their key/label. In R one can do something similar with named vectors or named lists or (more efficiently) by using environments.\n\nIn Unit 7, we’ll talk about distributed data structures that allow one to easily work with data distributed across multiple computers."
  },
  {
    "objectID": "units/unit2-dataTech.html#other-kinds-of-data-structures",
    "href": "units/unit2-dataTech.html#other-kinds-of-data-structures",
    "title": "Data technologies, formats, and structures",
    "section": "Other kinds of data structures",
    "text": "Other kinds of data structures\nYou may have heard of various other kinds of data structures, such as linked lists, trees, graphs, queues, and stacks. One of the key aspects that differentiate such data structures is how one navigates through the elements.\nSets are collections of elements that don’t have any duplicates (like a mathematical set).\nWith a linked list, with each element (or node) has a value and a pointer (reference) to the location of the next element. (With a doubly-linked list, there is also a pointer back to the previous element.) One big advantage of this is that one can insert an element by simply modifying the pointers involved at the site of the insertion, without copying any of the other elements in the list. A big disadvantage is that to get to an element you have to navigate through the list.\n\n\n\nLinked list (courtesy of computersciencewiki.org)\n\n\nBoth trees and graphs are collections of nodes (vertices) and links (edges). A tree involves a set of nodes and links to child nodes (also possibly containing information linking the child nodes to their parent nodes). With a graph, the links might not be directional, and there can be cycles.\n\n\n\nTree (courtesy of computersciencewiki.org)\n\n\n\n\n\nGraph (courtesy of computersciencewiki.org)\n\n\nA stack is a collection of elements that behave like a stack of lunch trays. You can only access the top element directly(“last in, first out”), so the operations are that you can push a new element onto the stack or pop the top element off the stack. In fact, nested function calls behave as stacks, and the memory used in the process of evaluating the function calls is called the ‘stack’.\nA queue is like the line at a grocery store, behaving as “first in, first out”.\nOne can use such data structures either directly or via add-on packages in R and Python, though I don’t think they’re all that commonly used in R. This is probably because statistical/data science/machine learning workflows often involve either ‘rectangular’ data (i.e., dataframe-style data) and/or mathematical computations with arrays. That said, trees and graphs are widely used.\nSome related concepts that we’ll discuss further in Unit 5 include:\n\ntypes: this refers to how a given piece of information is stored and what operations can be done with the information.\n\n‘primitive’ types are the most basic types that often relate directly to how data are stored in memory or on disk (e.g., booleans, integers, numeric (real-valued), character, pointer (address, reference).\n\npointers: references to other locations (addresses) in memory. One often uses pointers to avoid unnecessary copying of data.\nhashes: hashing involves fast lookup of the value associated with a key (a label), using a hash function, which allows one to convert the key to an address. This avoids having to find the value associated with a specific key by looking through all the keys until the key of interest is found (an O(n) operation)."
  },
  {
    "objectID": "units/unit3-bash.html",
    "href": "units/unit3-bash.html",
    "title": "The bash shell and UNIX utilities",
    "section": "",
    "text": "PDF\nReference:\nNote that it can be difficult to distinguish what is shell-specific and what is just part of the operating system (i.e., a UNIX-style operating system). Some of the material here is not bash-specific but general to UNIX. I’ll use ‘UNIX’ to refer to the family of operating systems that descend from the path-breaking UNIX operating system developed at AT&T’s Bell Labs in the 1970s. These include MacOS and various flavors of Linux (e.g., Ubuntu, Debian, CentOS, Fedora).\nFor your work on this unit, either bash on a Linux machine, the older version of bash on MacOS, or zsh on MacOS (or Linux) are fine. I’ll probably demo everything using bash on a Linux machine, and there are some annoying differences from the older bash on MacOS that may be occasionally confusing (in particular the options to various commands can differ on MacOS)."
  },
  {
    "objectID": "units/unit3-bash.html#first-challenge",
    "href": "units/unit3-bash.html#first-challenge",
    "title": "The bash shell and UNIX utilities",
    "section": "4.1 First challenge",
    "text": "4.1 First challenge\nConsider the file cpds.csv. How would you write a shell command that returns “There are 8 occurrences of the word ‘Belgium’ in this file.”, where ‘8’ is actually the correct number of times the word occurs.\nExtra: make your code into a function that can operate on any file indicated by the user and any word of interest."
  },
  {
    "objectID": "units/unit3-bash.html#second-challenge",
    "href": "units/unit3-bash.html#second-challenge",
    "title": "The bash shell and UNIX utilities",
    "section": "4.2 Second challenge",
    "text": "4.2 Second challenge\nConsider the data in the RTADataSub.csv file. This is a subset of data giving freeway travel times for segments of a freeway in an Australian city. The data are from a kaggle.com competition. We want to try to understand the kinds of data in each field of the file. The following would be particularly useful if the data were in many files or the data were many gigabytes in size.\n\nFirst, take the fourth column. Figure out the unique values in that column.\nNext, automate the process of determining if any of the values are non-numeric so that you don’t have to scan through all of the unique values looking for non-numbers. You’ll need to look for the following regular expression pattern [^0-9], which is interpreted as NOT any of the numbers 0 through 9.\nNow, do it for all the fields, except the first one. Have your code print out the result in a human-readable way understandable by someone who didn’t write the code."
  },
  {
    "objectID": "units/unit3-bash.html#third-challenge",
    "href": "units/unit3-bash.html#third-challenge",
    "title": "The bash shell and UNIX utilities",
    "section": "4.3 Third challenge",
    "text": "4.3 Third challenge\n\nFor Belgium, determine the minimum unemployment value (field #6) in cpds.csv in a programmatic way.\nHave what is printed out to the screen look like “Belgium 6.2”.\nNow store the unique values of the countries in a variable, first stripping out the quotation marks.\nFigure out how to automate step 1 to do the calculation for all the countries and print to the screen.\nHow would you instead store the results in a new file?"
  },
  {
    "objectID": "units/unit3-bash.html#fourth-challenge",
    "href": "units/unit3-bash.html#fourth-challenge",
    "title": "The bash shell and UNIX utilities",
    "section": "4.4 Fourth challenge",
    "text": "4.4 Fourth challenge\nLet’s return to the RTADataSub.csv file and the issue of missing values.\n\nCreate a new file without any rows that have an ‘x’ (which indicate a missing value).\nTurn the code into a function that also prints out the number of rows that are being removed and that sends its output to stdout so that it can be used with piping.\nNow modify your function so that the user could provide the missing value string, the input filename and the output filename as arguments."
  },
  {
    "objectID": "units/unit3-bash.html#fifth-challenge",
    "href": "units/unit3-bash.html#fifth-challenge",
    "title": "The bash shell and UNIX utilities",
    "section": "4.5 Fifth challenge",
    "text": "4.5 Fifth challenge\nHere’s an advanced one - you’ll probably need to use sed, but the brief examples of text substitution in the using bash tutorial should be sufficient to solve the problem.\nConsider a CSV file that has rows that look like this:\n1,\"America, United States of\",45,96.1,\"continental, coastal\" \n2,\"France\",33,807.1,\"continental, coastal\"\nWhile R would be able to handle this using read.table(), using cut in UNIX won’t work because of the commas embedded within the fields. The challenge is to convert this file to one that we can use cut on, as follows.\nFigure out a way to make this into a new delimited file in which the delimiter is not a comma. At least one solution that will work for this particular two-line dataset does not require you to use regular expressions, just simple replacement of fixed patterns."
  },
  {
    "objectID": "units/unit5-programming.html",
    "href": "units/unit5-programming.html",
    "title": "Programming concepts",
    "section": "",
    "text": "PDF\nReferences:\n(Optional) Videos\nThere are various videos from 2020 in the bCourses Media Gallery that you can use for reference if you want to. Note that I’ve reorganized the material in this Unit relative to 2020, so the section numbers and ordering in the videos may differ from that in the current Unit, but you should be able to match things up fairly easily."
  },
  {
    "objectID": "units/unit5-programming.html#string-processing-and-regular-expressions-in-r",
    "href": "units/unit5-programming.html#string-processing-and-regular-expressions-in-r",
    "title": "Programming concepts",
    "section": "String processing and regular expressions in R",
    "text": "String processing and regular expressions in R\nFor details of string processing in R, including use of regular expressions, see the string processing tutorial. (You can ignore the sections on Python if you wish.) That tutorial then refers to the bash shell tutorial for details on regular expressions.\nIn class we’ll work through some problems in the string processing tutorial, focusing in particular on the use of regular expressions with the stringr package. This will augment our consideration of regular expressions in the shell, in particular by seeing how we can replace patterns in addition to finding them."
  },
  {
    "objectID": "units/unit5-programming.html#regexstring-processing-challenges",
    "href": "units/unit5-programming.html#regexstring-processing-challenges",
    "title": "Programming concepts",
    "section": "Regex/string processing challenges",
    "text": "Regex/string processing challenges\nWe’ll work on these challenges (and perhaps one or two others) in class in the process of working through the string processing tutorial.\n\nWhat regex would I use to find any number with or without a decimal place.\nSuppose a text string has dates in the form “Aug-3”, “May-9”, etc. and I want them in the form “3 Aug”, “9 May”, etc. How would I do this search and replace operation? (Alternatively, how could I do this without using regular expressions at all?)"
  },
  {
    "objectID": "units/unit5-programming.html#side-notes-on-special-characters-in-r",
    "href": "units/unit5-programming.html#side-notes-on-special-characters-in-r",
    "title": "Programming concepts",
    "section": "Side notes on special characters in R",
    "text": "Side notes on special characters in R\nRecall that when characters are used for special purposes, we need to ‘escape’ them if we want them interpreted as the actual character. In what follows, I show this in R, but similar manipulations are sometimes needed in the shell and in Python.\nThis can get particularly confusing in R as the backslash is also used to input special characters such as newline (\\n) or tab (\\t).\nHere are some examples of using special characters.\n\nNote It is hard to compile the Rmd file correctly for these R chunks, so I am just pasting in the output from running in R ‘manually’ in some cases.)\n\n\ntmp <- \"Harry said, \\\"Hi\\\"\"\n## cat(tmp)   # prints out without a newline -- this is hard to show in the pdf\ntmp <- \"Harry said, \\\"Hi\\\".\\n\"\ncat(tmp)      # prints out with the newline\n\nHarry said, \"Hi\".\n\ntmp <- c(\"azar\", \"foo\", \"hello\\tthere\\n\")\ncat(tmp)\n\nazar foo hello  there\n\nprint(tmp)\n\n[1] \"azar\"           \"foo\"            \"hello\\tthere\\n\"\n\ngrep(\"[\\tz]\", tmp)   ## search for a tab or a 'z'\n\n[1] 1 3\n\n\nAs a result, in R we often need two backslashes when working with regular expressions. In the next examples, the first backslash says to interpret the next backslash literally, with the second backslash being used to indicate that the caret (^) should be interpreted literally and not as a special character in the regular expression syntax.\n\n## Search for characters that are not 'z'\n## (using ^ as regular expression syntax)\ngrep(\"[^z]\", c(\"a^2\", \"93\", \"zzz\", \"zit\", \"azar\"))\n\n[1] 1 2 4 5\n\n## Search for either a '^' (as a regular charcter) or a 'z':\ngrep(\"[\\\\^z]\", c(\"a^2\", \"93\", \"zzz\", \"zit\", \"azar\"))\n\n[1] 1 3 4 5\n\n## This fails (and the Rmd won't compile) because\n## '\\^' is not an escape sequence (i.e., a special character):\n## grep(\"[\\^z]\", c(\"a^2\", \"93\", \"zit\", \"azar\", \"zzz\"))\n## Error: '\\^' is an unrecognized escape in character string starting \"\"[\\^\"\n\n## Search for exactly three characters\n## (using . as regular expression syntax)\ngrep(\"^.{3}$\", c(\"abc\", \"1234\", \"def\"))\n\n[1] 1 3\n\n## Search for a period (as a regular character)\ngrep(\"\\\\.\", c(\"3.9\", \"27\", \"4.2\"))\n\n[1] 1 3\n\n## This fails (and the Rmd won't compile) because \n## '\\.' is not an escape sequence (i.e., a special character):\n## grep(\"\\.\", c(\"3.9\", \"27\")))\n## Error: '\\.' is an unrecognized escape in character string starting \"\"\\.\"\n\n\nChallenge Explain why we use a single backslash to get a newline and double backslash to write out a Windows path in the examples here:\n\n\n## Suppose we want to use a \\ in our string:\ncat(\"hello\\nagain\")\n\nhello\nagain\n\ncat(\"hello\\\\nagain\")\n\nhello\\nagain\n\ncat(\"My Windows path is: C:\\\\Users\\\\My Documents.\")\n\nMy Windows path is: C:\\Users\\My Documents.\n\n\nFor more information, see ?Quotes in R and the subsections of the string processing tutorial that discuss backslashes and escaping.\nAdvanced note: Searching for an actual backslash gets even more complicated, because we need to pass two backslashes as the regular expression, so that a literal backslash is searched for. However, to pass two backslashes, we need to escape each of them with a backslash so R doesn’t treat each backslash as part of a special character. So that’s four backslashes to search for a single backslash! Yikes. One rule of thumb is just to keep entering backslashes until things work!\n\n## Search for an actual backslash\ntmp <- \"something \\\\ other\\n\"\ncat(tmp)\n\nsomething \\ other\n\ngrep(\"\\\\\\\\\", tmp)\n\n[1] 1\n\ntry(grep(\"\\\\\", tmp))\n\nWarning in grep(\"\\\\\", tmp): TRE pattern compilation error 'Trailing backslash'\n\n\nError in grep(\"\\\\\", tmp) : \n  invalid regular expression '\\', reason 'Trailing backslash'\n\n\n\nWarning Be careful when cutting and pasting from documents that are not text files as you may paste in something that looks like a single or double quote, but which R cannot interpret as a quote because it’s some other ASCII quote character. If you paste in a ” from PDF, it will not be interpreted as a standard R double quote mark.\n\nSimilar things come up in the shell and in Python, but in the shell you often don’t need two backslashes. E.g. you could do this to look for a literal ^ character.\n\ngrep '\\^' file.txt"
  },
  {
    "objectID": "units/unit5-programming.html#interacting-with-the-operating-system",
    "href": "units/unit5-programming.html#interacting-with-the-operating-system",
    "title": "Programming concepts",
    "section": "Interacting with the operating system",
    "text": "Interacting with the operating system\nScripting languages allow one to interact with the operating system in various ways. Most allow you to call out to the shell to run arbitrary shell code and save results within your session.\nI’ll assume everyone knows about the following functions/functionality for interacting with the filesystem and file in R: getwd, setwd, source, pdf, save, save.image, load.\n\nTo run UNIX commands from within R, use system(), as follows, noting that we can save the result of a system call to an R object:\n\nsystem(\"ls -al\")   ## results apparently not shown when compiled...\nfiles <- system(\"ls\", intern = TRUE)\nfiles[1:5]\n\n[1] \"0-bash-shell.sh\" \"badCode.R\"       \"cache\"           \"calc_mean.py\"   \n[5] \"convert.sh\"     \n\n\nThere are also a bunch of functions that will do specific queries of the filesystem, including\n\nfile.exists(\"unit2-dataTech.Rmd\")\n\n[1] TRUE\n\nlist.files(\"../data\")\n\n[1] \"airline.csv\"           \"coop.txt.gz\"           \"cpds.csv\"             \n[4] \"hivSequ.csv\"           \"IPs.RData\"             \"precip.txt\"           \n[7] \"precipData.txt\"        \"RTADataSub.csv\"        \"stackoverflow-2016.db\"\n\n\nThere are some tools for dealing with differences between operating systems. file.path is a nice example:\n\nlist.files(file.path(\"..\", \"data\"))\n\n[1] \"airline.csv\"           \"coop.txt.gz\"           \"cpds.csv\"             \n[4] \"hivSequ.csv\"           \"IPs.RData\"             \"precip.txt\"           \n[7] \"precipData.txt\"        \"RTADataSub.csv\"        \"stackoverflow-2016.db\"\n\n\nIt’s best if you can to write your code in a way that is agnostic to the underlying operating system.\nTo get some info on the system you’re running on:\n\nSys.info()\n\n                                       sysname \n                                       \"Linux\" \n                                       release \n                           \"5.4.0-120-generic\" \n                                       version \n\"#136-Ubuntu SMP Fri Jun 10 13:40:48 UTC 2022\" \n                                      nodename \n                                     \"smeagol\" \n                                       machine \n                                      \"x86_64\" \n                                         login \n                                    \"paciorek\" \n                                          user \n                                    \"paciorek\" \n                                effective_user \n                                    \"paciorek\""
  },
  {
    "objectID": "units/unit5-programming.html#controlling-the-behavior-of-r",
    "href": "units/unit5-programming.html#controlling-the-behavior-of-r",
    "title": "Programming concepts",
    "section": "Controlling the behavior of R",
    "text": "Controlling the behavior of R\nScripting languages generally allow you to control/customize their behavior in various ways by setting options.\n\nTo see some of the options that control how R behaves, try the options function. The width option changes the number of characters of width printed to the screen, while max.print revents too much of a large object from being printed to the screen.\n\n## options()  # this would print out a long list of options\noptions()[1:4]\n\n$add.smooth\n[1] TRUE\n\n$bitmapType\n[1] \"cairo\"\n\n$browser\n[1] \"xdg-open\"\n\n$browserNLdisabled\n[1] FALSE\n\noptions()[c('width', 'digits')]\n\n$width\n[1] 80\n\n$digits\n[1] 7\n\n## Often it's nice to have more characters in each line on the screen,\n## but that would cause overly lines in the compiled file.\n## options(width = 120)\n\noptions(max.print = 5000)\n\nThe digits option changes the number of digits of numbers printed to the screen (but be careful as this can be deceptive if you then try to compare two numbers based on what you see on the screen).\n\noptions(digits = 3)\na <- 0.123456; b <- 0.1234561\na; b; a == b\n\n[1] 0.123\n\n\n[1] 0.123\n\n\n[1] FALSE\n\n\nMore on how to (and how not to) compare real-valued numbers on a computer in Unit 8.\nUse Ctrl-C to interrupt execution. This will generally back out gracefully, returning you to a state as if the command had not been started. Note that if R is exceeding the amount of memory available, there can be a long delay. This can be frustrating, particularly since a primary reason you would want to interrupt is when R runs out of memory.\nsessionInfo gives information on the current R session and can be very helpful for recording the state of your session (including package versions) to allow for reproducibility.\n\nsessionInfo()\n\nR version 4.2.0 (2022-04-22)\nPlatform: x86_64-pc-linux-gnu (64-bit)\nRunning under: Ubuntu 20.04.3 LTS\n\nMatrix products: default\nBLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3\nLAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/liblapack.so.3\n\nlocale:\n [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C              \n [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8    \n [5] LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8   \n [7] LC_PAPER=en_US.UTF-8       LC_NAME=C                 \n [9] LC_ADDRESS=C               LC_TELEPHONE=C            \n[11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C       \n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] SCF_4.1.0\n\nloaded via a namespace (and not attached):\n [1] digest_0.6.29     jsonlite_1.8.0    magrittr_2.0.3    evaluate_0.15    \n [5] rlang_1.0.5       stringi_1.7.8     cli_3.3.0         rmarkdown_2.14   \n [9] tools_4.2.0       stringr_1.4.0     htmlwidgets_1.5.4 xfun_0.31        \n[13] yaml_2.3.5        fastmap_1.1.0     compiler_4.2.0    htmltools_0.5.3  \n[17] knitr_1.39       \n\n\nAny code that you wanted executed automatically when starting R can be placed in ~/.Rprofile (or in individual, project-specific .Rprofile files in specific directories). This could include loading packages (see below), sourcing files that contain user-defined functions that you commonly use (you can also put the function code itself in .Rprofile), assigning variables, and specifying options via options().\nYou can have an R script act as a shell script (like running a bash shell script) as follows. This will probably on work on Linux and Mac.\n\nWrite your R code in a text file, say exampleRscript.R.\nAs the first line of the file, include #!/usr/bin/Rscript (like #!/bin/bash in a bash shell file, as seen in Unit 2) or for more portability across machines, include #!/usr/bin/env Rscript.\nMake the R code file executable with chmod: chmod ugo+x exampleRscript.R.\nRun the script from the command line: ./exampleRscript.R\n\nIf you want to pass arguments into your script, you can do so as long as you set up the R code to interpret the incoming arguments:\n\nargs <- commandArgs(TRUE)\n\n## Now args is a character vector containing the arguments.\n## Suppose the first argument should be interpreted as a number \n## and the second as a character string and the third as a boolean:\n\nnumericArg <- as.numeric(args[1])\ncharArg <- args[2]\nlogicalArg <- as.logical(args[3])\n\ncat(\"First arg is: \", numericArg, \"; second is: \", charArg, \n    \"; third is: \", logicalArg, \".\\n\")\n\nNow we can run it as follows in the shell:\n\n./exampleRscript.R 53 blah T\n./exampleRscript.R blah 22.5 t\n\nFirst arg is:  53 ; second is:  blah ; third is:  TRUE .\nWarning message:\nNAs introduced by coercion \nFirst arg is:  NA ; second is:  22.5 ; third is:  NA ."
  },
  {
    "objectID": "units/unit5-programming.html#interacting-with-external-code",
    "href": "units/unit5-programming.html#interacting-with-external-code",
    "title": "Programming concepts",
    "section": "Interacting with external code",
    "text": "Interacting with external code\nScripting languages such as R, Python, and Julia allow you to call out to “external code”, which often means C or C++ (but also Fortran, Java and other languages).\nIn fact, the predecessor language to R, which was called ‘S’ was developed specifically (at AT&T’s Bell Labs in the 1970s and 1980s) as an interactive wrapper around Fortran, the numerical programming language most commonly used at the time (and still widely relied on today in various legacy codes).\nCalling out to external code is particularly important in languages like R and Python that are often much slower than compiled code and less important in a fast language like Julia (which uses Just-In-Time compilation – more on that later).\nIn R, one can call directly out to C or C++ code using .Call or one can use the Rcpp package. Rcpp is specifically designed to be able to write C++ code that feels somewhat like writing R code and where it is very easy to pass data between R and C++.\nIn Python, one can directly call out to C or C++ code or one can use Cython to interact with C. With Cython, one can: - Have Cython automatically translate Python code to C, if you provide type definitions for your variables. - Define C functions that can be called from your Python code."
  },
  {
    "objectID": "units/unit5-programming.html#loading-packages",
    "href": "units/unit5-programming.html#loading-packages",
    "title": "Programming concepts",
    "section": "Loading packages",
    "text": "Loading packages\nYou can use library to either (1) make a package available (loading it), (2) get an overview of the package, or (3) (if called without arguments) to see all the installed packages.\n\nlibrary(dplyr)              # load the package\nlibrary(help = dplyr)       # get some help info about the package\n\nPackages in R (and in Python, Julia, etc.) may be installed in various places on the filesystem, and it sometimes it is helpful (e.g., if you end up with multiple versions of a package installed on your system) to be able to figure out where on the filesystem the package is being loaded from. If you run library(), you’ll notice that some of the packages are in a system directory and some are in your home directory.\n.libPaths() shows where R looks for packages on your system and searchpaths() shows where individual packages currently loaded in your session have been loaded from. The help information for .libPaths gives some information about how R decides what locations to look in for packages (and how you can modify that).\n\n.libPaths()\n\n[1] \"/accounts/vis/paciorek/R/x86_64-pc-linux-gnu-library/4.2\"\n[2] \"/system/linux/lib/R-20.04/4.2.0/x86_64/site-library\"     \n[3] \"/usr/lib/R/site-library\"                                 \n[4] \"/usr/lib/R/library\"                                      \n\nsearchpaths()\n\n [1] \".GlobalEnv\"                                             \n [2] \"tools:quarto\"                                           \n [3] \"/usr/lib/R/library/stats\"                               \n [4] \"/usr/lib/R/library/graphics\"                            \n [5] \"/usr/lib/R/library/grDevices\"                           \n [6] \"/usr/lib/R/library/utils\"                               \n [7] \"/usr/lib/R/library/datasets\"                            \n [8] \"/system/linux/lib/R-20.04/4.2.0/x86_64/site-library/SCF\"\n [9] \"/usr/lib/R/library/methods\"                             \n[10] \"Autoloads\"                                              \n[11] \"/usr/lib/R/library/base\""
  },
  {
    "objectID": "units/unit5-programming.html#installing-packages",
    "href": "units/unit5-programming.html#installing-packages",
    "title": "Programming concepts",
    "section": "Installing packages",
    "text": "Installing packages\nIf a package is on CRAN but not on your system, you can install it easily (usually). You don’t need root permission on a machine to install a package (though sometimes you run into hassles if you are installing it just as a user, so if you have administrative privileges it may help to use them). Of course in RStudio, you can install via the GUI.\nPackages often depend on other packages. In general, if one package depends on another, R will install the dependency automatically, but sometimes you’ll need to install a dependency yourself. In general, package dependencies are handled very cleanly in R without you having having to worry much about it; this is less the case in Python.\nNote that R will generally install the package in a reasonable place by default but you can control where it is installed using the lib argument.\n\ninstall.packages('dplyr', lib = '~/Rlibs') # ~/Rlibs needs to exist!\n\nYou can also download the zipped source file from CRAN and install from the file; see the help page for install.packages. This is called “installing from source”. On Windows and Mac, you’ll need to do something like this:\n\ninstall.packages('dplyr_VERSION.tar.gz', repos = NULL, type = 'source')\n\nThis can be handy if you need to install an older version of a package for reproducibility or because of some dependency incompatibility.\nIf you’ve downloaded the binary package (files ending in .tgz for Mac and .zip for Windows) and want to install the package directly from the file, use the syntax above but omit the type= 'source' argument.\n\nSource vs. binary packages\nThe difference between a source package and a binary package is that the source package has the raw R (and C and Fortran, in some cases) code as text files while the binary package has all the code in a binary/non-text format, including that any C and Fortran code will have already been compiled. To install a source package with C or Fortran code in it, you’ll need to have developer/command-line tools (e.g., XCode on Mac or Rtools.exe on Windows) installed on your system so that you have a compiler."
  },
  {
    "objectID": "units/unit5-programming.html#managing-packages-using-package-managers",
    "href": "units/unit5-programming.html#managing-packages-using-package-managers",
    "title": "Programming concepts",
    "section": "Managing packages using package managers",
    "text": "Managing packages using package managers\nFor reproducibility, it’s important to know the versions of the packages you use (and the version of R). Package managers make it easy to do this. Some useful packages that do package management in R are checkpoint, renv, and packrat. The basic commonality is that they try to make it easy to ‘freeze’ the versions of hte packages you are using, record that information, and restore the versions (potentially on some other machine and by some user other than yourself). The package manager may tell you where the packages are installed, but you can always verify things with .libPaths().\nIn Python, you can set up and manage isolated environments in which you can control the package versions using virtualenvs or Conda environments."
  },
  {
    "objectID": "units/unit5-programming.html#package-namespaces",
    "href": "units/unit5-programming.html#package-namespaces",
    "title": "Programming concepts",
    "section": "Package namespaces",
    "text": "Package namespaces\nThe objects in a package (primarily functions, but also data) are in their own workspaces, and are accessible after you load the package using library(), but are not directly visible when you use ls(). In other words, each package has its own namespace. Namespaces help achieve modularity and avoid having zillions of objects all reside in your workspace. If we want to see the objects in a package’s namespace, we can do the following:\n\nsearch()\n\n [1] \".GlobalEnv\"        \"tools:quarto\"      \"package:stats\"    \n [4] \"package:graphics\"  \"package:grDevices\" \"package:utils\"    \n [7] \"package:datasets\"  \"package:SCF\"       \"package:methods\"  \n[10] \"Autoloads\"         \"package:base\"     \n\n## ls(pos = 4) # for the stats package\nls(pos = 4)[1:5] # just show the first few\n\n[1] \"abline\"    \"arrows\"    \"assocplot\" \"axis\"      \"Axis\"     \n\nls(\"package:stats\")[1:5] # equivalent\n\n[1] \"acf\"        \"acf2AR\"     \"add.scope\"  \"add1\"       \"addmargins\"\n\nls(\"package:stats\", pattern = \"^lm\")\n\n[1] \"lm\"           \"lm.fit\"       \"lm.influence\" \"lm.wfit\"     \n\n\n\nWhy have namespaces?\nWe’ll talk more about namespaces when we talk about variable scope and environments. But as some motivation for why this is useful, consider the following.\nThe lm function calls the lm.fit function to calculate the least squares solution in regression.\nSuppose we write our own lm.fit function that does something else:\n\nlm.fit <- function(x)\n    print('hi')\n\nx <- 7\nlm.fit(x)\n\n[1] \"hi\"\n\n\nOne might expect that if one now uses lm() to fit a regression, that it wouldn’t work correctly because we have an lm.fit function in our workspace that doesn’t calculate the least squares solution. But it works just fine (see below), because lm and lm.fit are in the stats package namespace (see above) and R’s scoping rules (more later) ensure that the lm.fit that is found when I run lm is the lm.fit needed to run the regression and not my silly lm.fit function in current workspace.\n\nn <- 10\nx <- runif(n)\ny <- runif(n)\nmod <- lm(y ~ x)\nmod\n\n\nCall:\nlm(formula = y ~ x)\n\nCoefficients:\n(Intercept)            x  \n      0.555       -0.208  \n\n\n\n\nNamespace resolution\nStandard practice in R has generally been to load a package and then use any of the items in the package namespace directly, e.g.,\n\nlibrary(stringr)\nstr_detect(\"hello there\", \"hello\")\n\n[1] TRUE\n\n\nHowever, particularly if you’re using the package in only a limited way, it can be a nice idea to not load the entire package and instead use the namespace resolution operator in a style that might remind you of Python and some other languages:\n\nstringr::str_detect(\"hello there\", \"hello\")\n\n[1] TRUE\n\n\n\nimport numpy as np\nx = np.ndarray([0,3,5])\n\nOf course in Python you could also load the entire package (i.e., import the entire namespace), though it’s not standard practice:\n\nfrom numpy import *\n## OR: from numpy import ndarray\nx = ndarray([0,3,5])\n\nLoading entire packages often causes ‘name collisions’ where there are multiple functions (or variables, more genreally) that have the same name. This can be confusing. We’ll see how R determines what function to use later in the Unit."
  },
  {
    "objectID": "units/unit5-programming.html#data-structures",
    "href": "units/unit5-programming.html#data-structures",
    "title": "Programming concepts",
    "section": "Data structures",
    "text": "Data structures\nPlease see the data structures section of Unit 2 for some general discussion of data structures.\nWe’ll also see more complicated data structures when we consider objects in the next section on object-oriented programming."
  },
  {
    "objectID": "units/unit5-programming.html#types-and-classes",
    "href": "units/unit5-programming.html#types-and-classes",
    "title": "Programming concepts",
    "section": "Types and classes",
    "text": "Types and classes\n\nOverview and static vs. dynamic typing\nThe term ‘type’ refers to how a given piece of information is stored and what operations can be done with the information. ‘Primitive’ types are the most basic types that often relate directly to how data are stored in memory or on disk (e.g., boolean, integer, numeric (real-valued, aka double or floating point), character, pointer (aka address, reference).\nIn compiled languages like C and C++, one has to define the type of each variable. Such languages are statically typed. Interpreted (or scripting) languages such as Python and R have dynamic types. One can associate different types of information with a given variable name at different times and without declaring the type of the variable:\n\nx <- 'hello'\nprint(x)\n\n[1] \"hello\"\n\nx <- 7\nx*3\n\n[1] 21\n\n\nIn contrast in a language like C, one has to declare a variable based on its type before using it:\ndouble y;\ndouble x = 3.1;\ny = x * 7.1;\nDynamic typing can be quite helpful from the perspective of quick implementation and avoiding tedious type definitions and problems from minor inconsistencies between types (e.g., multiplying an integer by a real-valued number). But static typing has some critical advantages from the perspective of software development, including:\n\nprotecting against errors from mismatched values and unexpected user inputs, and\ngenerally much faster execution because the type of a variable does not need to be checked when the code is run.\n\nMore complex types in R (and in Python) often use references (pointers, aka addresses) to the actual locations of the data. We’ll see this in detail later in the Unit.\n\n\nTypes and classes in R\nYou should be familiar with vectors as the basic data structure in R, with character, integer, numeric, etc. classes. Vectors are either atomic vectors or lists. Atomic vectors generally contain one of the four following types: logical, integer, double (i.e., numeric), and character.\nEverything in R is an object and all objects have a class. For simple objects class and type are often closely related, but this is not the case for more complicated objects. As we’ll see later in the Unit, the class describes what the object contains and standard functions associated with it. In general, you mainly need to know what class an object is rather than its type.\n\nNote You can look at Table 7.1 in the Adler book to see some other types.\n\nLet’s look at the type and class of various data structures in R. We’ll first see that real-valued are stored as double-precision (8 byte) floating point numbers internally in R (as ‘doubles’ in C, as the R interpreter is a program written in C).\n\ndevs <- rnorm(5)\nclass(devs)\n\n[1] \"numeric\"\n\ntypeof(devs)   \n\n[1] \"double\"\n\na <- data.frame(x = 1:2)\nclass(a)\n\n[1] \"data.frame\"\n\ntypeof(a)\n\n[1] \"list\"\n\nis.data.frame(a)\n\n[1] TRUE\n\nis.matrix(a)\n\n[1] FALSE\n\nis(a, \"matrix\")\n\n[1] FALSE\n\nm <- matrix(1:4, nrow = 2) \nclass(m) \n\n[1] \"matrix\" \"array\" \n\ntypeof(m)\n\n[1] \"integer\"\n\n\nIn most cases integer-valued numbers are stored as numeric values in R, but there are exceptions such as the result of using the sequence operater, :, above. We can force R to store values as integers:\n\nvals <- c(1, 2, 3)\nclass(vals)\n\n[1] \"numeric\"\n\nvals <- 1:3\nclass(vals)\n\n[1] \"integer\"\n\nvals <- c(1L, 2L, 3L)\nvals\n\n[1] 1 2 3\n\nclass(vals)\n\n[1] \"integer\"\n\n\n\n\nAttributes\nWe saw the notion of attributes when looking at HTML and XML, where the information was stored as key-value pairs that in many cases had additional information in the form of attributes.\nIn R, attributes are information about an object attached to an object as something that looks like a named list. Attributes are often copied when operating on an object. This can lead to some weird-looking formatting when in subsequent operations the names attribute is carried along:\n\nx <- rnorm(10 * 365)\nattributes(x)\n\nNULL\n\nqs <- quantile(x, c(.025, .975))\nattributes(qs)\n\n$names\n[1] \"2.5%\"  \"97.5%\"\n\nqs\n\n 2.5% 97.5% \n-2.00  1.94 \n\nqs[1] + 3\n\n 2.5% \n0.997 \n\nobject.size(qs)\n\n352 bytes\n\n\nWe can get rid of the attribute:\n\nnames(qs) <- NULL\nqs\n\n[1] -2.00  1.94\n\nobject.size(qs)\n\n64 bytes\n\n\nA common use of attributes is that rows and columns may be named in matrices and data frames, and elements in vectors:\n\ndf <- data.frame(x = 1:2, y = 3:4)\nattributes(df)\n\n$names\n[1] \"x\" \"y\"\n\n$class\n[1] \"data.frame\"\n\n$row.names\n[1] 1 2\n\nrow.names(df) <- c(\"first\", \"second\")\ndf\n\n       x y\nfirst  1 3\nsecond 2 4\n\nattributes(df)\n\n$names\n[1] \"x\" \"y\"\n\n$class\n[1] \"data.frame\"\n\n$row.names\n[1] \"first\"  \"second\"\n\nvec <- c(first = 7, second = 1, third = 5)\nvec['first']\n\nfirst \n    7 \n\nattributes(vec)\n\n$names\n[1] \"first\"  \"second\" \"third\" \n\n\n\n\nConverting between types\nThis also goes by the term coercion and casting. Casting often needs to be done explicitly in compiled languages and somewhat less so in interpreted languages like R.\nWe convert between classes using variants on as: e.g.,\n\nas.character(c(1,2,3))\n\n[1] \"1\" \"2\" \"3\"\n\nas.numeric(c(\"1\", \"2.73\"))\n\n[1] 1.00 2.73\n\nas.factor(c(\"a\", \"b\", \"c\"))\n\n[1] a b c\nLevels: a b c\n\n\nSome common conversions are converting numbers that are being interpreted as characters into actual numbers, converting between factors and characters, and converting between logical TRUE/FALSE vectors and numeric 1/0 vectors.\nIn some cases R will automatically do conversions behind the scenes in a smart way (or occasionally not so smart way). Consider these examples of implicit coercion:\n\nx <- rnorm(5)\nx[3] <- 'hat' # What do you think is going to happen?\nindices <- c(1, 2.73)\nmyVec <- 1:10\nmyVec[indices]\n\n[1] 1 2\n\n\nHere’s an example we can work through that will help illustrate how type conversions occur behind the scenes in R.\n\nn <- 5\ndf <- data.frame(label = rep('a', n), val1 = rnorm(n), val2 = rnorm(n))\ndf\n\n  label   val1   val2\n1     a -0.754 -0.411\n2     a  1.368 -0.343\n3     a -0.434  0.343\n4     a -0.741 -1.561\n5     a -0.180 -0.498\n\n## Why does the following not work?\ntry( apply(df, 1, function(x) x[2] + x[3]) )\n\nError in x[2] + x[3] : non-numeric argument to binary operator\n\n## Instead, this will work. Why?\napply(df[ , 2:3], 1, function(x) x[1] + x[2])\n\n[1] -1.1656  1.0253 -0.0909 -2.3021 -0.6787\n\n\nBe careful of using factors as indices:\n\nstudents <- factor(c(\"basic\", \"proficient\", \"advanced\",\n                     \"basic\", \"advanced\", \"minimal\"))\nscore <- c(minimal = 65, basic = 75, proficient = 85, advanced = 95)\nscore[\"advanced\"]\n\nadvanced \n      95 \n\nstudents[3]\n\n[1] advanced\nLevels: advanced basic minimal proficient\n\nscore[students[3]]\n\nminimal \n     65 \n\nscore[as.character(students[3])]\n\nadvanced \n      95 \n\n\nWhat has gone wrong and how does it relate to type coercion?"
  },
  {
    "objectID": "units/unit5-programming.html#data-frames-and-related-concepts",
    "href": "units/unit5-programming.html#data-frames-and-related-concepts",
    "title": "Programming concepts",
    "section": "Data frames and related concepts",
    "text": "Data frames and related concepts\n\nSome notes on data frames and operations on data frames\nBase R provides a variety of functions for manipulating data frames, but now many researchers use add-on packages (many written by Hadley Wickham as part of a group of packages called the tidyverse) to do these manipulations in a more elegant way. Module 6 of the R bootcamp describes some of these new tools in more details, but I’ll touch on some aspects of this here, without showing much of the tidyverse syntax.\n\n\nsplit-apply-combine\nOften analyses are done in a stratified fashion - the same operation or analysis is done on subsets of the data set. The subsets might be different time points, different locations, different hospitals, different people, etc.\nThe split-apply-combine framework is intended to operate in this kind of context: first one splits the dataset by one or more variables, then one does something to each subset, and then one combines the results. The dplyr package implements this framework (as does the pandas package for Python). One can also do similar operations using various flavors of the lapply family of functions such as by, tapply, and aggregate, but the dplyr-based tools are often nicer to use.\nsplit-apply-combine is also closely related to the famous Map-Reduce framework underlying big data tools such as Hadoop and Spark.\nIt’s also very similar to standard SQL queries involving filtering, grouping, and aggregation.\n\n\nLong and wide formats\nFinally, we may want to convert between so-called ‘long’ and ‘wide’ formats, which we can motivate in the context of longitudinal data (multiple observations per subject) and panel data (temporal data for each of multiple units such as in econometrics). The wide format has repeated measurements for a subject in separate columns, while the long format has repeated measurements in separate rows, with a column for differentiating the repeated measurements.\n\nlong <- data.frame(id = c(1, 1, 1, 2, 2, 2),\n                   time = c(1980, 1990, 2000, 1980, 1990, 2000),\n                   value = c(5, 8, 9, 7, 4, 7))\nwide <- data.frame(id = c(1, 2),\n                   value_1980 = c(5, 7), value_1990 = c(8, 4), value_2000 = c(9, 7))\nlong\n\n  id time value\n1  1 1980     5\n2  1 1990     8\n3  1 2000     9\n4  2 1980     7\n5  2 1990     4\n6  2 2000     7\n\nwide\n\n  id value_1980 value_1990 value_2000\n1  1          5          8          9\n2  2          7          4          7\n\n\nThe wide format can be useful in some situations for treating each row as a (multivariate observation), but the long formatwhile the long format is often what is needed for analyses such as mixed models. ANOVA, or for plotting, such as with ggplot2.\nThere are a variety of functions for converting between wide and long formats. I recommend pivot_longer and pivot_wider in the tidyr package. There are also older tidyr functions called gather and spread. There are also the melt and cast in the reshape2 package. These are easier to use than the functions in base R such as reshape or stack and unstack functions.\n\n\nPiping\nPiping was introduced into R in conjuction with dplyr and the tidyverse.\nThe tidyverse pipe is %>% while the new base R pipe is |>. These are based on the UNIX pipe, which we saw in Unit 3, though they behave somewhat differently in that the output of the previous function is passed in as the first argument of the next function. In the shell, the pipe connects stdout from the previous command to stdin for the next command.\n\n\nNon-standard evaluation and the tidyverse\nMany tidyverse packages use non-standard evaluation to make it easier to code. For example in the following dplyr example, you can refer directly to country and unemp, which are variables in the data frame, without using data$country or data$unemp and without using quotes around the variable names, as in \"country\" or \"unemp\". Referring directly to the variables in the data frame is not standard R usage, hence the term “non-standard evaluation”. One reason it is not standard is that country and unemp are not themselves independent R variables so R can’t find them in the usual way using scoping (discussed later in the Unit).\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\ncpds <- read.csv(file.path('..', 'data', 'cpds.csv'))\n\ncpds2 <- cpds %>% group_by(country) %>%\n                  mutate(mean_unemp = mean(unemp))\n\nhead(cpds2)\n\n# A tibble: 6 × 7\n# Groups:   country [1]\n   year country   vturn outlays realgdpgr unemp mean_unemp\n  <int> <chr>     <dbl>   <dbl>     <dbl> <dbl>      <dbl>\n1  1960 Australia  95.5    NA       NA     1.42       5.52\n2  1961 Australia  95.3    NA       -0.07  2.79       5.52\n3  1962 Australia  95.3    23.2      5.71  2.63       5.52\n4  1963 Australia  95.7    23.0      6.1   2.12       5.52\n5  1964 Australia  95.7    22.9      6.28  1.15       5.52\n6  1965 Australia  95.7    24.9      4.97  1.15       5.52\n\n\nThis ‘magic’ is done by capturing the code expression you write and evaluating it in a special way in the context of the data frame. I believe this uses R’s environment class (discussed later in the Unit), but haven’t looked more deeply.\nWhile this has benefits, this so-called non-standard evaluation makes it harder to program functions in the usual way, as illustrated in the following code chunk, where neither attempt to use the function works.\n\nadd_mean <- function(data, group_var, summarize_var) {\n    data %>% group_by(group_var) %>%\n             mutate(mean_of_var = mean(summarize_var))\n}\n\ntry(cpds2 <- add_mean(cpds, country, unemp))\n\nError in group_by(., group_var) : \n  Must group by variables found in `.data`.\n✖ Column `group_var` is not found.\n\ntry(cpds2 <- add_mean(cpds, 'country', 'unemp'))\n\nError in group_by(., group_var) : \n  Must group by variables found in `.data`.\n✖ Column `group_var` is not found.\n\n\nFor more details on how to avoid this problem when writing functions that involve tidyverse manipulations, see this tidyverse programming guide.\nNote that the tidyverse is not the only place where non-standard evaluation is used. Consider this lm call:\n\nlm(y ~ x, weights = w, data = mydf)\n\n\nChallenge Where is the non-standard evaluation there?"
  },
  {
    "objectID": "units/unit5-programming.html#principles",
    "href": "units/unit5-programming.html#principles",
    "title": "Programming concepts",
    "section": "Principles",
    "text": "Principles\nSome of the standard concepts in object-oriented programming include encapsulation, inheritance, polymorphism, and abstraction.\nEncapsulation involves preventing direct access to internal data in an object from outside the object. Instead the class is designed so that access (reading or writing) happens through the interface set up by the programmer (e.g., ‘getter’ and ‘setter’ methods). We’ll see this in our R6 class example below.\nInheritance allows one class to be based on another class, adding more specialized features. An example in R’s S3 system is that the glm class inherits from the lm class.\nPolymorphism allows for different behavior of an object or function depending on the context. A polymorphic function behaves differently depending on the input types. A polymorphic object is one that can belong to different classes (e.g., based on inheritance), and a given method name can be used with any of the classes. An example would be having a base or super class called ‘algorithm’ and various specific machine learning algorithms inheriting from that class. All of the classes might have a ‘predict’ method.\nAbstraction involves hiding the details of how something is done (e.g., via the method of a class), giving the user an interface to provide inputs and get outputs. By making the actual computation a black box, the programmer can modify the internals without changing how a user uses the system.\nClasses generally have constructors that initialize objects of the class and destructors that remove objects."
  },
  {
    "objectID": "units/unit5-programming.html#generic-function-oop",
    "href": "units/unit5-programming.html#generic-function-oop",
    "title": "Programming concepts",
    "section": "Generic function OOP",
    "text": "Generic function OOP\nMuch of the object-oriented programming in R uses generic function OOP, also known as functional OOP. In this style, classes don’t have methods. Instead there are generic functions (also known as generic methods) that change their behavior based on the type of the input(s). Another way to put it is that the nouns and the verbs are separate, unliked in standard OOP.\nThe use of generic functions is similar in spirit to function or method overloading in C++ and Java.\nGeneric function OOP is how the (very) old S3 system in R works. It’s also a key part of the (fairly) new Julia language.\n\nS3 classes in R\nS3 classes are widely-used, in particular for statistical models in the stats package. S3 classes are very informal in that there’s not a formal definition for an S3 class. Instead, an S3 object is just a primitive R object such as a list or vector with additional attributes including a class name.\n\nCreating our own class\nWe can create an object with a new class as follows:\n\nyog <- list(firstname = 'Yogi', surname = 'the Bear', age = 20)\nclass(yog) <- 'bear' \n\nActually, if we want to create a new class that we’ll use again, we want to create a constructor function that initializes new bears:\n\nbear <- function(firstname = NA, surname = NA, age = NA){\n    # constructor for 'indiv' class\n    obj <- list(firstname = firstname, surname = surname,\n                    age = age)\n    class(obj) <- 'bear' \n    return(obj)\n}\nsmoke <- bear('Smokey','Bear')\n\nFor those of you used to more formal OOP, the following is probably disconcerting:\n\nclass(smoke) <- \"celebrity\"\n\nGenerally S3 classes inherit from lists (i.e., are special cases of lists), so you can obtain components of the object using the $ operator.\n\n\nGeneric methods\nThe real power of the S3 system comes from defining class-specific methods. For example,\n\nx <- rnorm(10)\nsummary(x)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n -1.394  -1.052   0.103  -0.090   0.673   1.052 \n\ny <- rnorm(10)\nmod <- lm(y ~ x)\nsummary(mod)\n\n\nCall:\nlm(formula = y ~ x)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-1.727 -0.520 -0.127  0.274  2.397 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept)   -0.152      0.370   -0.41     0.69\nx             -0.384      0.399   -0.96     0.36\n\nResidual standard error: 1.16 on 8 degrees of freedom\nMultiple R-squared:  0.104, Adjusted R-squared:  -0.00851 \nF-statistic: 0.924 on 1 and 8 DF,  p-value: 0.365\n\n\nHere summary is a generic function (or generic method) that, based on the type of object given to it (the first argument), dispatches a class-specific function (method) that operates on the object.\nThe above is equivalent to directly calling the class-specific methods:\n\nidentical(summary(x), summary.default(x))\n\n[1] TRUE\n\nidentical(summary(mod), summary.lm(mod))\n\n[1] TRUE\n\n\nThis use of generic functions is convenient in that it allows us to work with a variety of kinds of objects using familiar functions. Consider the generic methods plot, print, summary, [, and others. We can look at a function and easily see that it is a generic method.\n\nsummary\n\nfunction (object, ...) \nUseMethod(\"summary\")\n<bytecode: 0x563f9dfd7810>\n<environment: namespace:base>\n\n\nThe UseMethod syntax is what causes the dispatching of the class-specific method associated with object and calls that method. In many cases there will be a default method (here, summary.default), so if no method is defined for the class, R uses the default. Sidenote: arguments to a generic method are passed along to the selected method by passing along the calling environment.\nWe can also see what classes have methods for a given generic function.\n\nmethods(summary)\n\n [1] summary,ANY-method                   summary,DBIObject-method            \n [3] summary,diagonalMatrix-method        summary,sparseMatrix-method         \n [5] summary.aov                          summary.aovlist*                    \n [7] summary.aspell*                      summary.check_packages_in_dir*      \n [9] summary.connection                   summary.data.frame                  \n[11] summary.Date                         summary.default                     \n[13] summary.ecdf*                        summary.factor                      \n[15] summary.glm                          summary.infl*                       \n[17] summary.lm                           summary.loess*                      \n[19] summary.manova                       summary.matrix                      \n[21] summary.mlm*                         summary.nls*                        \n[23] summary.packageStatus*               summary.pandas.core.frame.DataFrame*\n[25] summary.pandas.core.series.Series*   summary.POSIXct                     \n[27] summary.POSIXlt                      summary.ppr*                        \n[29] summary.prcomp*                      summary.princomp*                   \n[31] summary.proc_time                    summary.python.builtin.object*      \n[33] summary.rlang_error*                 summary.rlang_message*              \n[35] summary.rlang_trace*                 summary.rlang_warning*              \n[37] summary.rlang:::list_of_conditions*  summary.shingle*                    \n[39] summary.srcfile                      summary.srcref                      \n[41] summary.stepfun                      summary.stl*                        \n[43] summary.table                        summary.trellis*                    \n[45] summary.tukeysmooth*                 summary.vctrs_sclr*                 \n[47] summary.vctrs_vctr*                  summary.warnings                    \nsee '?methods' for accessing help and source code\n\n\nOr from a different angle we can see what specific methods are available for a given class.\n\nmethods(class = 'lm')\n\n [1] add1           alias          anova          case.names     coerce        \n [6] confint        cooks.distance deviance       dfbeta         dfbetas       \n[11] drop1          dummy.coef     effects        extractAIC     family        \n[16] formula        hatvalues      influence      initialize     kappa         \n[21] labels         logLik         model.frame    model.matrix   nobs          \n[26] plot           predict        print          proj           qr            \n[31] residuals      rstandard      rstudent       show           simulate      \n[36] slotsFromS3    summary        variable.names vcov          \nsee '?methods' for accessing help and source code\n\n\nLet’s try this functionality out on our bear class.\n\nsummary.bear <- function(object) \n    with(object, cat(\"Bear of age \", age, \n    \" whose name is \", firstname, \" \", surname, \".\\n\",\n        sep = \"\"))\n    invisible(NULL)\n\nsummary(yog)\n\nBear of age 20 whose name is Yogi the Bear.\n\n\nWe can also define a new generic function.\nLet’s do this for the bear class as an illustration, though this won’t provide any functionality beyond what we did with summary\n\nsummarize <- function(object, ...) \n    UseMethod(\"summarize\") \n\n\nsummarize.bear <- function(object) \n    with(object, cat(\"Bear of age \", age, \n    \" whose name is \", firstname, \" \", surname, \".\\n\",\n        sep = \"\"))\n    invisible(NULL)\n\nsummarize(yog)\n\nBear of age 20 whose name is Yogi the Bear.\n\n\n\n\nWhy use generic functions?\nWe could have written summary as a regular function with a bunch of if statements or if-else clauses (or switch) so that it can handle different kinds of input objects.\nThis has two disadvantages:\n\nWe need to write the code that does the checking (and all the code for the different cases all lives inside one potentially very long function, unless we create class-specific helper functions).\nMuch more importantly, summary will only work for existing classes. And users can’t easily extend it for new classes that they create because they don’t control the summary function. So a user could not add the additional conditions/classes in a big if-else statement. The generic function approach makes the system extensible – we can build our own new functionality on what is already in R. For example, we could have written summary.bear.\n\n\n\nThe print method\nLike summary, print is a generic method, with various class-specific methods, such as print.lm. We could write our own print.bear specific method.\nNote that the print function is what is called when you simply type the name of the object, so we can have object information printed out in a structured way. Thus, the output when we type the name of an lm object is NOT simply a regurgitation of the elements of the list - rather print.lm is called.\n\nmod\n\n\nCall:\nlm(formula = y ~ x)\n\nCoefficients:\n(Intercept)            x  \n     -0.152       -0.384  \n\nprint(mod)\n\n\nCall:\nlm(formula = y ~ x)\n\nCoefficients:\n(Intercept)            x  \n     -0.152       -0.384  \n\nstats:::print.lm(mod)  ## print.lm is private to the stats namespace\n\n\nCall:\nlm(formula = y ~ x)\n\nCoefficients:\n(Intercept)            x  \n     -0.152       -0.384  \n\n# print.default(mod)   ## lots of output, so don't print in document...\n\n\nstats:::print.lm\n\nfunction (x, digits = max(3L, getOption(\"digits\") - 3L), ...) \n{\n    cat(\"\\nCall:\\n\", paste(deparse(x$call), sep = \"\\n\", collapse = \"\\n\"), \n        \"\\n\\n\", sep = \"\")\n    if (length(coef(x))) {\n        cat(\"Coefficients:\\n\")\n        print.default(format(coef(x), digits = digits), print.gap = 2L, \n            quote = FALSE)\n    }\n    else cat(\"No coefficients\\n\")\n    cat(\"\\n\")\n    invisible(x)\n}\n<bytecode: 0x563f99ee4aa0>\n<environment: namespace:stats>\n\n\nSurprisingly, the summary method generally doesn’t actually print out information; rather it computes things not stored in the original object and returns it as a new class (e.g., class summary.lm), which is then automatically printed, per my comment above (e.g., using print.summary.lm), unless one assigns it to a new object. Note that print.summary.lm is hidden from user view (it’s a private object in the stats namespace).\n\nout <- summary(mod)\nclass(out)\n\n[1] \"summary.lm\"\n\nout\n\n\nCall:\nlm(formula = y ~ x)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-1.727 -0.520 -0.127  0.274  2.397 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept)   -0.152      0.370   -0.41     0.69\nx             -0.384      0.399   -0.96     0.36\n\nResidual standard error: 1.16 on 8 degrees of freedom\nMultiple R-squared:  0.104, Adjusted R-squared:  -0.00851 \nF-statistic: 0.924 on 1 and 8 DF,  p-value: 0.365\n\nprint(out)\n\n\nCall:\nlm(formula = y ~ x)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-1.727 -0.520 -0.127  0.274  2.397 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept)   -0.152      0.370   -0.41     0.69\nx             -0.384      0.399   -0.96     0.36\n\nResidual standard error: 1.16 on 8 degrees of freedom\nMultiple R-squared:  0.104, Adjusted R-squared:  -0.00851 \nF-statistic: 0.924 on 1 and 8 DF,  p-value: 0.365\n\n## One can look at the code for the method (not shown):\n## getS3method(f = \"print\", class = \"summary.lm\")\n\n\n\nInheritance\nLet’s look at the lm class, which builds on lists, and glm class, which builds on the lm class. Here mod is an object (an instance) of class lm.\n\nlibrary(methods)\nybin <- sample(c(0, 1), 10, replace = TRUE)\nycont <- rnorm(10)\nx <- rnorm(10)\nmod1 <- lm(ycont ~ x)\nmod2 <- glm(ybin ~ x, family = binomial)\nclass(mod1)\n\n[1] \"lm\"\n\nclass(mod2)\n\n[1] \"glm\" \"lm\" \n\nis.list(mod1)\n\n[1] TRUE\n\nnames(mod1)\n\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"xlevels\"       \"call\"          \"terms\"         \"model\"        \n\nis(mod2, \"lm\")\n\n[1] TRUE\n\n\nHere’s an example of why this is useful. We don’t have to define methods for the glm class if the given method for the lm class would work fine:\n\nmodel.matrix(mod1)\n\n   (Intercept)      x\n1            1 -1.534\n2            1  1.413\n3            1  0.197\n4            1  0.411\n5            1 -0.557\n6            1 -0.267\n7            1 -0.302\n8            1  1.942\n9            1  0.627\n10           1  0.232\nattr(,\"assign\")\n[1] 0 1\n\nmodel.matrix(mod2)\n\n   (Intercept)      x\n1            1 -1.534\n2            1  1.413\n3            1  0.197\n4            1  0.411\n5            1 -0.557\n6            1 -0.267\n7            1 -0.302\n8            1  1.942\n9            1  0.627\n10           1  0.232\nattr(,\"assign\")\n[1] 0 1\n\nmethods(model.matrix)\n\n[1] model.matrix.default model.matrix.lm     \nsee '?methods' for accessing help and source code\n\n\nAs noted with lm and glm objects, we can assign more than one class to an object. Here summarize still works, even though the primary class is grizzly_bear.\n\nclass(yog) <- c('grizzly_bear', 'bear')\nsummarize(yog) \n\nBear of age 20 whose name is Yogi the Bear.\n\n\nThe classes should nest within one another with the more specific classes to the left, e.g., here a grizzly_bear would have some additional fields on top of those of a bear, perhaps number_of_people_killed (since grizzly bears are much more dangerous than some other kinds of bears), and perhaps additional or modified methods. grizzly_bear inherits from bear, and R uses methods for the first class before methods for the next class(es).\nThe above is an example of polymorphism. yog is a polymorphic object and the various methods are polymorphic in that print can be used with the bear class, the grizzly_bear class, and other classes beyond that.\n\nChallenge How would you get R to quit immediately, without asking for any more information, when you simply type k (no parentheses!) instead of quit()? (Hint: you can do this by understanding what happens when you type k and how to exploit the S3 system.)\n\n\n\n\nMultiple dispatch OOP\nS3 method dispatch involves only the first argument to the function. In contrast, Julia emphasizes the importance of multiple dispatch as particularly important for mathematical computation. With multiple dispatch, the specific method can be chosen based on more than one argument.\nThe old (but still used in some contexts) S4 system in R and the (very) new R7 system both provide for multiple dispatch.\nAs a very simple example unrelated to any specific language, multiple dispatch would allow one to do the following with the addition operator:\n3 + 7    # 10\n3 + 'a'  # '3a'\n'hi' +  ' there'  # 'hi there'\nThe idea of having the behavior of an operator or function adapt to the type of the input(s) is one aspect of polymorphism.\nBoth S4 and R7 are designed to be more formal than the S3 system (recall how we could just ‘create’ an S3 class by giving a class name to an existing list). With S4 and R7, you need to define your classes."
  },
  {
    "objectID": "units/unit5-programming.html#standard-oop",
    "href": "units/unit5-programming.html#standard-oop",
    "title": "Programming concepts",
    "section": "‘Standard’ OOP",
    "text": "‘Standard’ OOP\nWhat I’m calling ‘standard’ object-oriented programming is the style of OOP used in languages such as Python, C++, and Java. In R, one can use this style via the R6 system (or the older referenceClass system).\nIn this style, objects belong to classes. A class is made up of fields (the data objects) that store information and methods that operate on the fields. Thus, unlike generic function OOP, the verbs are part of the nouns.\nWe’ll illustrate this style of OOP using an example with an R6 class.\n\nR6 classes\nR6 classes are a somewhat new construct in R, with a class-based approach fairly similar to Python and C++. Importantly, they behave like pointers. We’ll discuss pointers in detail later. Let’s work through an example where we set up the fields of the class and class methods, including a constructor.\n\nExample\nOur example is to create a class for working with random time series. Each object of the class has specific parameter values that control the stochastic behavior of the time series. With a given object we can simulate one or more time series (realizations).\nHere’s the initial definition of the class, with both public (user-facing) and private (internal use only) methods and fields.\n\nlibrary(R6)\n\ntsSimClass <- R6Class(\"tsSimClass\",\n    ## class for holding time series simulators\n    public = list(\n        initialize = function(times, mean = 0, corParam = 1) {\n            library(fields)\n            stopifnot(is.numeric(corParam), length(corParam) == 1)\n            stopifnot(is.numeric(times))\n            private$times <- times\n            private$n <- length(times)\n            private$mean <- mean\n            private$corParam <- corParam\n            private$currentU <- FALSE\n            private$calcMats()\n        },\n        \n        setTimes = function(newTimes) {\n            private$times <- newTimes\n            private$calcMats()\n        },\n        \n        getTimes = function() {\n            return(private$times)\n        },\n\n        print = function() { # 'print' method\n            cat(\"R6 Object of class 'tsSimClass' with \",\n                private$n, \" time points.\\n\", sep = '')\n            invisible(self)\n        },\n\n        simulate = function() {\n            if(!private$currentU)     \n                private$calcMats()\n            ## analogous to mu+sigma*z for generating N(mu, sigma^2)\n            return(private$mean + crossprod(private$U, rnorm(private$n)))\n        }\n    ),\n\n    ## private methods and functions not accessible externally\n    private = list(\n        calcMats = function() {\n            ## calculates correlation matrix and Cholesky factor\n            lagMat <- fields::rdist(private$times) # local variable\n            corMat <- exp(-lagMat^2 / private$corParam^2)\n            private$U <- chol(corMat) # square root matrix\n            cat(\"Done updating correlation matrix and Cholesky factor.\\n\")\n            private$currentU <- TRUE\n            invisible(self)\n        },\n        n = NULL, \n        times = NULL,\n        mean = NULL,\n        corParam = NULL,\n        U = NULL,\n        currentU = FALSE\n    )\n)   \n\nNow let’s see how we would use the class.\n\nmyts <- tsSimClass$new(1:100, 2, 1)\n\nDone updating correlation matrix and Cholesky factor.\n\nmyts\n\nR6 Object of class 'tsSimClass' with 100 time points.\n\nset.seed(1)\n## here's a simulated time series\ny1 <- myts$simulate()\nplot(myts$getTimes(), y1, type = 'l', xlab = 'time',\n     ylab = 'process values')\n## simulate a second series\ny2 <- myts$simulate()\nlines(myts$getTimes(), y2, lty = 2)\n\n\n\n\nWe could set up a different object that has different parameter values. That new simulated time series is less wiggly because the corParam value is larger than before.\n\nmyts2 <- tsSimClass$new(1:100, 2, 4)\n\nDone updating correlation matrix and Cholesky factor.\n\nset.seed(1)\n## here's a simulated time series with a different value of\n## the correlation parameter (corParam)\ny3 <- myts2$simulate()\n\nplot(myts$getTimes(), y1, type = 'l', xlab = 'time',\n     ylab = 'process values')\nlines(myts$getTimes(), y2, lty = 2)\nlines(myts2$getTimes(), y3, col = 'red')\n\n\n\n\n\n\nCopies and references\nNext let’s think about when copies are made. In the next example mytsRef is a copy of myts in the sense that both names point to the same underlying object. But no data were copied when the assignment to mytsRef was done.\n\nmytsRef <- myts\n## 'mytsRef' and 'myts' are names for the same underlying object\nmytsFullCopy <- myts$clone()  \n\n## Now let's change the values of a field\nmyts$setTimes(seq(0,1000, length = 100))\n\nDone updating correlation matrix and Cholesky factor.\n\nmyts$getTimes()[1:5] \n\n[1]  0.0 10.1 20.2 30.3 40.4\n\nmytsRef$getTimes()[1:5] # the same as `myts`\n\n[1]  0.0 10.1 20.2 30.3 40.4\n\nmytsFullCopy$getTimes()[1:5] # different from `myts`\n\n[1] 1 2 3 4 5\n\n\nIn contrast mytsFullCopy is a reference to a different object, and all the data from myts had to be copied over to mytsFullCopy. This takes additional memory (and time), but is also safer, as it avoids the possibility that the user might modify myts and not realize that they were also affecting mytsRef.\n\n\nEncapsulation\nWhy have private fields (i.e., encapsulation)? The use of private fields shields them from modification by users. In this case, that prevent users from modifying the times field. Why is this important? In this example, the correlation matrix and the Cholesky factor U are both functions of the vector of times. So we don’t want to allow a user to directly modify times. If they did, it would leave the fields of the object in inconsistent states. Instead we force them to use setTimes, which correctly keeps all the fields in the object internally consistent (by calling calcMats). It also allows us to improve efficiency by controlling when computationally expensive operations are carried out.\n\ntry(myts$times <- 1:10)\n\nError in myts$times <- 1:10 : cannot add bindings to a locked environment\n\n\n\n\nFinal comments\n\nAs we saw above, a copy of an object is just a pointer to the original object, unless we explicitly invoke the clone method.\nClasses can inherit from other classes. E.g., if we had a simClass and we wanted the tsSimClass to inherit from it:\nR6Class(tsSimClass, inherit = simClass, ...) \nIf you need to refer to methods and fields you refer to the entire object as either self or private.\n\nMore details on R6 classes can be found in the Advanced R book."
  },
  {
    "objectID": "units/unit5-programming.html#overview-of-functional-programming",
    "href": "units/unit5-programming.html#overview-of-functional-programming",
    "title": "Programming concepts",
    "section": "Overview of functional programming",
    "text": "Overview of functional programming\nFunctional programming is an approach to programming that emphasizes the use of modular, self-contained functions. Such functions should operate only on arguments provided to them (avoiding global variables), and produce no side effects, although in some cases there are good reasons for making an exception. Another aspect of functional programming is that functions are considered ‘first-class’ citizens in that they can be passed as arguments to another function, returned as the result of a function, and assigned to variables. In other words, a function can be treated as any other variable.\nIn many cases (including R and Python), anonymous functions (also called ‘lambda functions’) can be created on-the-fly for use in various circumstances."
  },
  {
    "objectID": "units/unit5-programming.html#functional-programming-in-r",
    "href": "units/unit5-programming.html#functional-programming-in-r",
    "title": "Programming concepts",
    "section": "Functional programming in R",
    "text": "Functional programming in R\nR is a language that has strong functional programming aspects to it, including:\n\nAll operations are carried out by functions.\nFunctions are first class citizens.\nFunctions (generally) do not have side effects.\nMap operations (e.g., lapply) are central to programming in R.\n\nFunctions that are not implemented internally in R are also referred to officially as closures (this is their type) - this terminology sometimes comes up in error messages.\n\ntypeof(mean)\n\n[1] \"closure\"\n\ntypeof(lm)\n\n[1] \"closure\"\n\ntypeof(length)\n\n[1] \"builtin\"\n\n\n\nNo side effects\nMost functions available in R (and ideally functions that you write as well) operate by taking in arguments and producing output that is then (presumably) used subsequently. The functions generally don’t have any effect on the state of your R environment/session other than the output they produce.\nAn important reason for this (plus for not using global variables) is that it means that it is easy for people using the language to understand what code does. Every function can be treated a black box – you don’t need to understand what happens in the function or worry that the function might do something unexpected (such as changing the value of one of your variables). The result of running code is simply the result of a composition of functions, as in mathematical function composition.\nOne aspect of this is that R uses a pass-by-value approach to function arguments (as opposed to a pass-by-reference approach). We’ll talk about function arguments and when copies are made in much more detail later, but briefly, when you pass an object in as an argument and then modify it in the function, you are modifying a local copy of the variable that exists in the context (the frame) of the function and is deleted when the function call finishes:\n\nx <- 1:3\nmyfun <- function(x) {\n      x[2] <- 7\n      print(x)\n      return(x)\n}\n\nnew_x <- myfun(x)\n\n[1] 1 7 3\n\nx   # unmodified\n\n[1] 1 2 3\n\n\nIn contrast, let’s see what happens in Python\n\nx = [1,2,3]\ndef myfun(x):\n  x[1] = 7\n  print(x)\n  return(x)\n\nnew_x = myfun(x)\n\n[1, 7, 3]\n\nx   # modified!\n\n[1, 7, 3]\n\n\nThere are some (necessary) exceptions to the idea of no side effects in R. An important exception is par(). If you change graphics parameters by calling par() in a user-defined function, they are changed permanently outside of the function. One trick is as follows:\n\nf <- function(){\n  oldpar <- par()\n  par(cex = 2)\n          \n  # body of code\n\n  par() <- oldpar\n}\n\nNote that changing graphics parameters within a specific plotting function - e.g., plot(x, y, pch = '+'), doesn’t change things except for that particular plot.\n\nChallenge What are some other functions that are called for the purpose of the side effects they produce? (For example, which functions change the state of your R session in some way?\n\n\n\nFunctions are first-class objects\nEverything in R is an object, including functions. We can assign functions to variables in the same way we assign numeric and other values.\n\nx <- 3\nclass(x); typeof(x)\n\n[1] \"numeric\"\n\n\n[1] \"double\"\n\ntry(x(2))    # x is not a function (yet)\n\nError in x(2) : could not find function \"x\"\n\nx <- function(z) z^2  # now it is a function\nx(2)\n\n[1] 4\n\nclass(x); typeof(x)\n\n[1] \"function\"\n\n\n[1] \"closure\"\n\n\nWe can call a function based on the text name of the function.\n\nmyFun <- 'mean'; x <- rnorm(10)\neval(as.name(myFun))(x)\n\n[1] 0.347\n\n\nWe can also pass a function into another function as the actual function object. This is an important aspect of R being a functional programming language.\n\nx <- rnorm(10)\nsapply(x, abs)\n\n [1] 0.636 0.462 1.432 0.651 0.207 0.393 0.320 0.279 0.494 0.177\n\nf <- function(fxn, x) {\n    fxn(x)\n}\nf(mean, x)\n\n[1] -0.12\n\n\nWe can also pass in a function based on a a character vector of length one with the name of the function. Here match.fun() is a handy function that extracts a function when the function is passed in as an argument of a function. It looks in the calling environment for the function and can handle when the function is passed in as a function object or as a character vector of length 1 giving the function name.\n\nf <- function(fxn, x){\n  match.fun(fxn)(x) \n}\nf(\"mean\", x)\n\n[1] -0.12\n\nf(mean, x)\n\n[1] -0.12\n\n\nFunction objects contain three components: an argument list, a body (a parsed R statement), and an environment.\n\nf1 <- function(x) y <- x^2\nf2 <- function(x) {\n    y <- x^2\n    z <- x^3\n    return(list(y, z))\n}\nclass(f1)\n\n[1] \"function\"\n\nbody(f2)\n\n{\n    y <- x^2\n    z <- x^3\n    return(list(y, z))\n}\n\ntypeof(body(f1)); class(body(f1))\n\n[1] \"language\"\n\n\n[1] \"<-\"\n\ntypeof(body(f2)); class(body(f2))\n\n[1] \"language\"\n\n\n[1] \"{\"\n\n\nWe’ll see more about objects relating to the R language and parsed code in the final section of this Unit. For now, just realize that the parsed code itself is treated as an object(s) with certain types and certain classes.\nThe do.call function is another example of a function that takes a function as an argument. It will apply a function to the elements of a list. For example, we can rbind() together (if compatible) the elements of a list of vectors instead of having to loop over the elements or manually type them in:\n\nmyList <- list(a = 1:3, b = 11:13, c = 21:23)\nargs(rbind)\n\nfunction (..., deparse.level = 1) \nNULL\n\nrbind(myList$a, myList$b, myList$c)\n\n     [,1] [,2] [,3]\n[1,]    1    2    3\n[2,]   11   12   13\n[3,]   21   22   23\n\nrbind(myList)\n\n       a         b         c        \nmyList integer,3 integer,3 integer,3\n\ndo.call(rbind, myList)\n\n  [,1] [,2] [,3]\na    1    2    3\nb   11   12   13\nc   21   22   23\n\n\nWhy couldn’t we just use rbind directly? Basically we’re using do.call() to use functions that take ... as input (i.e., functions accepting an arbitrary number of arguments) and to use the list as the input instead (i.e., to use the list elements).\nMore generally do.call is a way to pass arguments to a function when the arguments you want to pass are part of a list.\n\ndo.call(mean, list(1:10, na.rm = TRUE))\n\n[1] 5.5\n\n\n\n\nAll operations are functions\nAll operations in R are actually function calls, even things that don’t look like function calls, including various operators (such as addition, subtraction, etc.), printing to the screen, etc.\n\nOperators\nOperators, such as + and [ are just functions, but their arguments can occur both before and after the function call:\n\na <- 7; b <- 3\n# let's think about the following as a mathematical function\n#  -- what's the function call?\na + b \n\n[1] 10\n\n`+`(a, b)\n\n[1] 10\n\n\nIn general, you can use back-ticks to refer to the operators as operators instead of characters. In some cases single or double quotes also work. We can look at the code of an operator as follows using back-ticks to escape out of the standard R parsing, e.g.,\n\n`%*%`\n\nfunction (x, y)  .Primitive(\"%*%\")\n\n\nFinally, since an operator is just a function, you can use it as an argument in various places:\n\nx <- 1:3; y <- c(100,200,300)\nouter(x, y, `+`)\n\n     [,1] [,2] [,3]\n[1,]  101  201  301\n[2,]  102  202  302\n[3,]  103  203  303\n\nmyList <- list(list(state = 'new york', value = 1:5),\n               list(state = 'california', value = 6:10),\n               list(state = 'delaware', value = 11:15))\n\n## note that the index \"2\" is the additional argument to the [[ function\nresult <- lapply(myList, `[[`, 2)\nresult\n\n[[1]]\n[1] 1 2 3 4 5\n\n[[2]]\n[1]  6  7  8  9 10\n\n[[3]]\n[1] 11 12 13 14 15\n\nmyMat <- sapply(myList, `[[`, 2)\nmyMat\n\n     [,1] [,2] [,3]\n[1,]    1    6   11\n[2,]    2    7   12\n[3,]    3    8   13\n[4,]    4    9   14\n[5,]    5   10   15\n\ncbind(myList[[1]][[2]], myList[[2]][[2]])  ## equivalent but doesn't scale\n\n     [,1] [,2]\n[1,]    1    6\n[2,]    2    7\n[3,]    3    8\n[4,]    4    9\n[5,]    5   10\n\n\nYou can define your own binary operator (an operator taking two arguments) using a string inside % symbols. Here’s how we could do Python-style string addition:\n\n`%+%` <- function(a, b) paste0(a, b, collapse = '')\n\"Hi \" %+% \"there\"\n\n[1] \"Hi there\"\n\n\nSince operators are just functions, there are cases in which there are optional arguments that we might not expect. Here’s how to pass a sometimes useful argument to the bracket operator (in this case avoiding conversion from a matrix to a vector, which can mess up subsequent code).\n\nmat <- matrix(1:4, 2, 2)\nmat[ , 1] \n\n[1] 1 2\n\nmat[ , 1, drop = FALSE] # what's the difference?\n\n     [,1]\n[1,]    1\n[2,]    2\n\n\nWe can also use operators with our S3 classes. Picking up our example from our discussion of S3 OOP, the following example will be a bit silly (it would make more sense with a class that is a mathematical object) but indicates the power of having methods.\n\nyog <- list(firstname = 'Yogi', surname = 'the Bear', age = 20)\nclass(yog) <- 'bear' \n\nmethods(`+`)\n\n [1] +,dgTMatrix,dgTMatrix-method +,Matrix,missing-method     \n [3] +,matrix,spam-method         +,spam,matrix-method        \n [5] +,spam,missing-method        +,spam,spam-method          \n [7] +.Date                       +.gg*                       \n [9] +.glue*                      +.POSIXt                    \n[11] +.vctrs_vctr*               \nsee '?methods' for accessing help and source code\n\n`+.bear` <- function(object, incr) {\n    object$age <- object$age + incr\n    return(object)\n}\nolder_yog <- yog + 15\n\nolder_yog\n\n$firstname\n[1] \"Yogi\"\n\n$surname\n[1] \"the Bear\"\n\n$age\n[1] 35\n\nattr(,\"class\")\n[1] \"bear\"\n\n\n\n\nOther operations that are functions\nEven beyond operators, all code in R can be viewed as a function call, including if statements and for and while loops.\nWhat do you think is the functional version of the following code? What are the arguments?\n\nif(x > 27){\n    print(x)    \n} else{\n    print(\"too small\") \n}\n\n\n\nReplacement functions\nAssignments that involve functions or operators on the left-hand side (LHS) are called replacement expressions or replacement functions. These can be quite handy. Here are a few examples:\n\ndiag(mat) <- c(3, 2)\nis.na(vec) <- 3\nnames(df) <- c('var1', 'var2')\n\nReplacement expressions are actually function calls. The R interpreter calls the replacement function (which often creates a new object that includes the replacement) and then assigns the result to the name of the original object.\n\nmat <- matrix(rnorm(4), 2, 2)\ndiag(mat) <- c(3, 2)\nmat\n\n     [,1]   [,2]\n[1,] 3.00 -0.215\n[2,] 1.34  2.000\n\nmat <- `diag<-`(mat, c(10, 21))\nmat\n\n      [,1]   [,2]\n[1,] 10.00 -0.215\n[2,]  1.34 21.000\n\nbase::`diag<-`\n\nfunction (x, value) \n{\n    dx <- dim(x)\n    if (length(dx) != 2L) \n        stop(\"only matrix diagonals can be replaced\")\n    len.i <- min(dx)\n    len.v <- length(value)\n    if (len.v != 1L && len.v != len.i) \n        stop(\"replacement diagonal has wrong length\")\n    if (len.i) {\n        i <- seq_len(len.i)\n        x[cbind(i, i)] <- value\n    }\n    x\n}\n<bytecode: 0x563f9e90bac8>\n<environment: namespace:base>\n\n\nThe old version of mat still exists until R’s memory management cleans it up, but it’s no longer referred to by the symbol mat. This can cause memory use to increase temporarily (but generally very briefly). So it’s something to keep in mind if you’re doing replacements on large objects.\nYou can define your own replacement functions like this, with the requirements that the last argument be named value and that the function return the entire object:\n\nyog <- list(firstName = 'Yogi', lastName = 'Bear')\n\n`firstName<-` <- function(obj, value){\n  obj$firstName <- value\n  return(obj)\n}\n\nfirstName(yog) <- 'Yogisandra'\n\nWe can use replacement functions with functional OOP. We need to define the generic replacement function and then the class-specific one.\n\n`age<-` <- function(x, ...) UseMethod(\"age<-\")\n\n`age<-.bear` <- function(object, value){ \n    object$age <- value\n    return(object)\n}\nage(older_yog) <- 60\n\nolder_yog\n\n$firstname\n[1] \"Yogi\"\n\n$surname\n[1] \"the Bear\"\n\n$age\n[1] 60\n\nattr(,\"class\")\n[1] \"bear\"\n\n\n\n\n\nMap operations\nA map operation takes a function and runs the function on each element of some collection of items, analogous to a mathematical map. This kind of operation is very commonly used in programming, particularly functional programming, and often makes for clean, concise, and readable code.\nBase R provides a variety of map-type functions: lapply and sapply and their variants, as well as apply. In addition, the purrr package for functional programming provides purrr::map. In R, often the map-type function is run on the elements of a list, but they can also generally be run on elements of a vector and in other ways. In other languages, map-type functions are run on a variety of data structures. These are examples of higher-order functions – functions that take a function as an argument.\nLet’s compare using lapply to using a for loop to run a stratified analysis for a generic example (this code won’t run because the variables don’t exist):\n\n# stratification \nsubsets <- split(df, grouping_variable)\n\n# lapply: one line, easy to understand\nresults <- lapply(subsets, analysis_function)\n\n# for loop: needs storage set up and multiple lines\nresults <- list()\nlength(results) <- length(subsets)\nfor(i in seq_along(subsets)) \n  results[[i]] <- analysis_function(subsets[[i]])\n\nMap operations are also at the heart of the famous map-reduce paradigm, used in Hadoop and Spark for big data processing."
  },
  {
    "objectID": "units/unit5-programming.html#function-evaluation-frames-and-the-call-stack",
    "href": "units/unit5-programming.html#function-evaluation-frames-and-the-call-stack",
    "title": "Programming concepts",
    "section": "Function evaluation, frames, and the call stack",
    "text": "Function evaluation, frames, and the call stack\n\nOverview\nWhen we run code, we end up calling functions inside of other function calls. This leads to a nested series of function calls. The series of calls is the call stack. The stack operates like a stack of cafeteria trays - when a function is called, it is added to the stack (pushed) and when it finishes, it is removed (popped).\nUnderstanding the series of calls is important when reading error messages and debugging. In Python, when an error occurs, the call stack is shown, which has the advantage of giving the complete history of what led to the error and the disadvantage of producing often very verbose output that can be hard to understand. In R, only the function in which the error occurs is shown, but you can see the full call stack by invoking traceback() (see the debugging tutorial).\nWhat happens when an R function is evaluated?\n\nThe user-provided function arguments are evaluated in the calling environment and the results are matched to the argument names in the function definition.\nA new environment with its own frame is created, with the frame on the call stack. Assignment to the argument names is done in the environment, including any default arguments.\nThe body of the function is evaluated in the environment. Any look-up of variables not found in the environment is done using R’s lexical scoping rules to look in the series of enclosing environments.\nWhen the function finishes, the return value is passed back to the calling frame and the function frame is taken off the stack. The environment is removed, unless the environment serves as the enclosing environment of another environment.\n\nI’m not expecting you to fully understand that previous paragraph and all the terms in it yet. We’ll see all the details as we proceed through this Unit.\n\n\nFrames and the call stack\nR keeps track of the call stack. Each function call is associated with a frame that contains the local variables for that function call.\nThere are a bunch of functions that let us query what frames are on the stack and access objects in particular frames of interest. This gives us the ability to work with objects in the frame from which a function was called.\nSome terminology: for our purposes we’ll use the terms frame and environment somewhat interchangeably for the moment. A frame or environment is a collection of named objects. (Note that when we talk about variable scope later in this Unit, we’ll have to be more careful with our terminology.) So in the context of a function call, the frame is the set of local variables available in the function, including arguments passed to the function.\nR provides some functions that allow you to query the call stack and its frames. sys.nframe returns the number of the current frame/environment and sys.parent the number of the parent, while parent.frame gives the name of the frame/environment of the parent (i.e., the calling) frame. sys.frame gives the name of the frame/environment for a given frame number (for non-negative numbers). For negative numbers, it goes back that many frames in the call stack and returns the name of the frame/environment. I need to manually insert the output here because the R Markdown processing up the frame counting somehow.\n\nsys.nframe()\nf <- function() {\n    cat('in f: Frame number is ', sys.nframe(),\n            '; parent frame number is ', sys.parent(), '.\\n', sep = '')\n    cat('in f: Frame (i.e., environment) is: ')\n    print(sys.frame(sys.nframe()))\n    cat('in f: Parent is ')\n    print(parent.frame())\n    cat('in f: Two frames up is ')\n    print(sys.frame(-2))\n}\nf()\n\nin f: Frame number is 1; parent frame number is 0.\nin f: Frame (i.e., environment) is: <environment: 0x55a4d71beb88>\nin f: Parent is <environment: R_GlobalEnv>\nin f: Two frames up is Error in sys.frame(-2) : not that many frames on the stack\n\nff <- function() {\n    cat('in ff: Frame (i.e., environment) is: ')\n    print(sys.frame(sys.nframe()))\n    cat('in ff: Parent is ')\n    print(parent.frame())   \n    f()\n}\nff() \n\nin ff: Frame (i.e., environment) is: <environment: 0x55a4d7391700>\nin ff: Parent is <environment: R_GlobalEnv>\nin f: Frame number is 2; parent frame number is 1.\nin f: Frame (i.e., environment) is: <environment: 0x55a4d7393b38>\nin f: Parent is <environment: 0x55a4d7391700>\nin f: Two frames up is <environment: R_GlobalEnv>\nNext we’ll use a recursive function to illustrate what information we can gather about the call stack using sys.status. sys.status gives extensive information about the call stack and the frames involved (sys.status uses sys.calls, sys.parents and sys.frames).\n\ng <- function(y) {\n    if(y > 0) g(y-1) else gg()\n}\n\n## Ultimately, gg() is called, and it prints out info about the call stack\ngg <- function() {\n    ## this gives us the information from sys.calls(),\n    ##   sys.parents() and sys.frames() as one object\n    ## Rather than running print(sys.status()),\n    ## which would involve adding print() to the call stack,\n    ## we'll run sys.status and then print the result out.\n    tmp <- sys.status()\n    print(tmp)\n}\n\ng(3)\n\n$sys.calls\n$sys.calls[[1]]\ng(3)\n\n$sys.calls[[2]]\nif(y > 0) g(y-1) else gg()\n\n$sys.calls[[3]]\nif(y > 0) g(y-1) else gg()\n\n$sys.calls[[4]]\nif(y > 0) g(y-1) else gg()\n\n$sys.calls[[5]]\nif(y > 0) g(y-1) else gg()\n\n$sys.calls[[6]]\ntmp <- sys.status()\n\n\n$sys.parents\n[1] 0 1 2 3 4 5\n\n$sys.frames\n$sys.frames[[1]]\n<environment: 0x55a4d63479e8>\n\n$sys.frames[[2]]\n<environment: 0x55a4d6347ba8>\n\n$sys.frames[[3]]\n<environment: 0x55a4d5736638>\n\n$sys.frames[[4]]\n<environment: 0x55a4d5732028>\n\n$sys.frames[[5]]\n<environment: 0x55a4d5732098>\n\n$sys.frames[[6]]\n<environment: 0x55a4d5732488>\n\nChallenge Why did I not do print(sys.status()) directly?\n\nIf you’re interested in parsing a somewhat complicated example of frames in action, Adler provides a user-defined timing function that evaluates statements in the calling frame."
  },
  {
    "objectID": "units/unit5-programming.html#function-inputs-and-outputs",
    "href": "units/unit5-programming.html#function-inputs-and-outputs",
    "title": "Programming concepts",
    "section": "Function inputs and outputs",
    "text": "Function inputs and outputs\n\nArguments\nArguments can be specified by position (based on the order of the inputs) or by name, using name = value. R first tries to match arguments by name and then by position. In general the more important arguments are specified first. You can see the arguments and defaults for a function using args:\n\nargs(lm)\n\nfunction (formula, data, subset, weights, na.action, method = \"qr\", \n    model = TRUE, x = FALSE, y = FALSE, qr = TRUE, singular.ok = TRUE, \n    contrasts = NULL, offset, ...) \nNULL\n\n\nYou can’t generally tell directly which arguments are required; in general you’d need to look at the documentation. For example, lm() requires formula but not data, subset, etc., even though none of them have default arguments.\nR will error out if it is expecting an argument, rather than looking for that argument elsewhere.\n\nprint(sum)\n\nfunction (..., na.rm = FALSE)  .Primitive(\"sum\")\n\nsum()\n\n[1] 0\n\nprint(quantile)\n\nfunction (x, ...) \nUseMethod(\"quantile\")\n<bytecode: 0x563f984be1c8>\n<environment: namespace:stats>\n\ntry(quantile())\n\nError in is.factor(x) : argument \"x\" is missing, with no default\n\nx <- 1\ny <- 2\nmyfun <- function(x) {\n    z <- y+3\n    w <- x+3\n}\ntry(myfun())\n\nError in myfun() : argument \"x\" is missing, with no default\n\n\nYou can check if an argument is missing with missing(). Arguments can also have default values, which may be NULL. If you are writing a function and designate the default as argname = NULL, you can check whether the user provided anything using is.null(argname). The default values can also relate to other arguments. As an example, consider dgamma:\n\nargs(dgamma)\n\nfunction (x, shape, rate = 1, scale = 1/rate, log = FALSE) \nNULL\n\n\nFunctions may have unspecified arguments, which are designated using .... Unspecified arguments occurring at the beginning of the argument list are generally a collection of like objects that will be manipulated (consider paste, c, and rbind), while unspecified arguments occurring at the end are often optional arguments (consider plot). These optional arguments are sometimes passed along to a function within the function. For example, here’s my own wrapper for plotting, where any additional arguments specified by the user (such as xlab and ylab) will get passed along to plot:\n\npplot <- function(x, y, pch = 16, cex = 0.4, ...) {\n    plot(x, y, pch = pch, cex = cex, ...)\n}\npplot(rnorm(10), rnorm(10), xlab = 'x', ylab = 'y')\n\nIf you want to manipulate what the user passed in as the ... args, rather than just passing them along, you can extract them:\n\nmyFun <- function(...){\n  print(..2) \n  args <- list(...)\n  print(args[[2]])\n}\nmyFun(1,3,5,7)\n\n[1] 3\n[1] 3\n\n\nAs we’ve seen, functions can be passed in as arguments (e.g., see the variants of apply and lapply). Note that one does not need to pass in a named function - you can create the function on the spot - this is called an anonymous function (also called a lambda function in some languages such as Python):\n\nmylist <- list(rnorm(2), rnorm(3), rnorm(5))\nsapply(mylist, length)\n\n[1] 2 3 5\n\nlapply(mylist, function(x) x[x < 0])\n\n[[1]]\n[1] -0.1\n\n[[2]]\n[1] -0.0736 -0.0376 -0.6817\n\n[[3]]\n[1] -0.324 -0.589 -1.518\n\n\nWe can see the arguments using args() and extract the arguments using formals(). formals() can be helpful if you need to manipulate the arguments.\n\nf <- function(x, y = 2, z = 3 / y) { \n  x + y + z \n}\nargs(f)\n\nfunction (x, y = 2, z = 3/y) \nNULL\n\nformals(f)\n\n$x\n\n\n$y\n[1] 2\n\n$z\n3/y\n\n\nmatch.call() will show the user-suppled arguments explicitly matched to named arguments.\n\nmatch.call(definition = mean, \n  call = quote(mean(y, na.rm = TRUE))) \n\nmean(x = y, na.rm = TRUE)\n\n\n\nChallenge In the above code, what do you think quote()does? Why is it needed?\n\n\nWhere are arguments evaluated?\nUser-supplied arguments are evaluated in the calling frame (why?), while default arguments are evaluated in the frame of the function (why?):\n\nz <- 3\nx <- 100\nf <- function(x, y = x*3) {x+y}\nf(z*5)\n\n[1] 60\n\n\nHere, when f() is called and the code is evaluated, z is evaluated in the calling frame and z*5 is assigned to x in the frame of the function, while x*3 is evaluated in the frame of the function (using the local x that was just created) and assigned to y.\n\n\n\nFunction outputs\nreturn(x) will specify x as the output of the function. By default, if return() is not specified, the output is the result of the last evaluated statement. return() can occur anywhere in the function, and allows the function to exit as soon as it is done.\n\nf <- function(x) { \n    if(x < 0) {\n        return(-x^2)\n    } else res <- x^2\n}\nf(-3)\n\n[1] -9\n\nf(3)\na <- f(3)\na\n\n[1] 9\n\n\ninvisible(x) will return x and the result can be assigned in the calling environment but it will not be printed if not assigned:\n\nf <- function(x){ \n  invisible(x^2) \n}\nf(3)\na <- f(3)\na\n\n[1] 9\n\n\nA function can only return a single object (unlike Matlab, e.g.), but of course we can tack things together as a list and return that, as occurs with many functions, such as lm. (Of course lm() actually returns an object of the S3 lm class, which inherits from the list class.)\n\nmod <- lm(mpg ~ cyl, data = mtcars)\nclass(mod)\n\n[1] \"lm\"\n\nis.list(mod)\n\n[1] TRUE"
  },
  {
    "objectID": "units/unit5-programming.html#pass-by-value-vs.-pass-by-reference",
    "href": "units/unit5-programming.html#pass-by-value-vs.-pass-by-reference",
    "title": "Programming concepts",
    "section": "Pass by value vs. pass by reference",
    "text": "Pass by value vs. pass by reference\nWhen talking about programming languages, one often distinguishes pass-by-value and pass-by-reference.\nPass-by-value means that when a function is called with one or more arguments, a copy is made of each argument and the function operates on those copies.\nPass-by-reference means that the arguments are not copied, but rather that information is passed allowing the function to find and modify the original value of the objects passed into the function.\nIn pass-by-value, changes to an argument made within a function do not affect the value of the argument in the calling environment. In pass-by-reference changes inside a function do affect the object outside of the function. R is (roughly) pass-by-value. R’s designers chose not to allow pass-by-reference because they didn’t like the idea that a function could have the side effect of changing an object. However, passing by reference can sometimes be very helpful, and we’ll see ways of passing by reference later (and also note our discussion of R6 classes).\nPass-by-value is elegant and modular in that functions do not have side effects - the effect of the function occurs only through the return value of the function. However, it can be inefficient in terms of the amount of computation and of memory used. In contrast, pass-by-reference is more efficient, but also more dangerous and less modular. It’s more difficult to reason about code that uses pass-by-reference because effects of calling a function can be hidden inside the function. Thus pass-by-value is directly related to functional programming.\nArrays in Python are pass-by-reference (but note that tuples are immutable, so one could not modify a tuple that is passed as an argument).\n\ndef myfun(x):\n    x[1] = 99\n\ny = [0, 1, 2]\nz = myfun(y)\ny\n\n[0, 99, 2]\n\n\n\nPointers\nBy way of contrast to a pass-by-value system, I want to briefly discuss the idea of a pointer, common in compiled languages such as C.\nint x = 3;\nint* ptr;\nptr = &x;\n*ptr * 7; // returns 21\n\nThe int* declares ptr to be a pointer to (the address of) the integer x.\nThe &x gets the address where x is stored.\n*ptr dereferences ptr, returning the value in that address (which is 3 since ptr is the address of x.\n\nVectors in C are really pointers to a block of memory:\nint x[10];\nIn this case x will be the address of the first element of the vector. We can access the first element as x[0] or *x.\nWhy have we gone into this? In C, you can pass a pointer as an argument to a function. The result is that only the scalar address is copied and not the entire object, and inside the function, one can modify the original object, with the new value persisting on exit from the function. For example in the following example one passes in the address of an object and that object is then modified in place, affecting its value when the function call finishes.\nint myCal(int* ptr){\n    *ptr = *ptr + *ptr;\n}\n\nmyCal(&x)  # x itself will be modified\n\nNote When calling C or C++ from R, one (implicitly) passes pointers to the vectors into C.\n\n\n\nPointers in R?\nAre there pointers in R? From a user perspective, one might say ‘no’, because an R programmer can’t use pointers explicitly. But pointer-like behavior is occurring behind the scenes in lots of ways:\n\nLists in R are essentially vectors of pointers to the elements of the list.\nCharacter vectors in R are essentially pointers to the individual character strings.\nEnvironments behave like pointers and are passed by reference rather than by copy.\nR6 objects behave like pointers and are passed by reference, as seen earlier.\n\nWe’ll see more on these ideas later in the Unit.\n\n\nAlternatives to pass by value in R\nThere are occasions we do not want to pass by value. In addition to avoiding copies and the computation and memory use that that causes, another reason is when we want a function to modify a complicated object without having to return it and re-assign it in the parent environment. There are several work-arounds:\n\nWe can use R6 (or Reference Class) objects.\nWe can use a closure, as discussed later.\nWe can access the object in the enclosing environment as a ‘global variable’, as we’ll see when discussing scoping. More generally we can access the object using get(), specifying the environment from which we want to obtain the variable. To specify the location of an object when using get(), we can generally specify (1) a position in the search path, (2) an explicit environment, or (3) a location in the call stack by using sys.frame(). However we cannot change the value of the object in the parent environment without some additional tools:\n\nWe can use the <<- operator to assign into an object in the enclosing environment (provided an object of that name exists in the enclosing environment). We’ll discuss enclosing environments when we talk about scoping.\nWe can also use assign(), specifying the environment in which we want the assignment to occur. While these techniques are possible and ok for exploratory coding, they’re generally bad practice for more formal code development.\n\nWe can use replacement functions, which hide the reassignment in the parent environment from the user. Note that a second copy is generally created in this case, but the original copy is quickly removed.\n\n\n\nPromises and lazy evaluation\nIn actuality, R is not quite pass-by-value; rather it is call-by-value. Copying of arguments is delayed in two ways:\n\nThe first is the idea of promises, described next. Promises are an example of a general programming concept called lazy evaluation.\nThe second is the idea of copy-on-modify, described in more detail later. Basically, with copy-on-modify, copies of arguments are only made if the argument is changed within the function. Until then the object in the function just refers back to the original object.\n\nLet’s see what a promise object is. In function calls, when R matches user input arguments to formal argument names, it does not (usually) evaluate the arguments until they are needed, which is called lazy evaluation. Instead the formal arguments are of a special type called a promise. Let’s see lazy evaluation in action.\nWhat’s strange about this?\n\nf <- function(x) print(\"hi\")\nsystem.time(mean(rnorm(1000000)))\n\n   user  system elapsed \n  0.058   0.000   0.059 \n\nsystem.time(f(3))\n\n[1] \"hi\"\n\n\n   user  system elapsed \n  0.001   0.000   0.000 \n\nsystem.time(f(mean(rnorm(1000000)))) \n\n[1] \"hi\"\n\n\n   user  system elapsed \n  0.001   0.000   0.002 \n\n\nHere’s an even stranger situation. Do you think the following code will run?\n\nf <- function(a, b = d) {\n    d <- a*3; \n    return(a*b)\n}\n\nb <- 100\nf(5)\n\nLazy evaluation is not just an R thing. It also occurs in Tensorflow (particularly version 1), the Python Dask package, and in Spark. The basic idea is to delay executation until it’s really needed, with the goal that if one does so, the system may be able to better optimize a series of multiple steps as a joint operation relative to executing them one by one."
  },
  {
    "objectID": "units/unit5-programming.html#variable-scope-and-lookup",
    "href": "units/unit5-programming.html#variable-scope-and-lookup",
    "title": "Programming concepts",
    "section": "Variable scope and lookup",
    "text": "Variable scope and lookup\n\nLexical scoping\nIn this section, we seek to understand what happens in the following circumstance. Namely, where does R get the value for the object x?\n\nf <- function(y) {\n  return(x + y)\n}\nf(3)\n\n[1] 103\n\n\nTo consider variable scope, we need to define the terms environment and frame. Environments and frames are closely related.\n\nA frame is a collection of named objects.\nAn environment is a frame, with a pointer to the ‘enclosing environment’, i.e., the next environment to look for something in. (Be careful as this is different than the parent frame of a function, discussed when we were talking about the call stack.)\n\nVariables in the enclosing environment (also called the parent environment) are available within a function. This is the analog of global variables in other languages. The enclosing environment is the environment in which a function is defined, not the environment from which a function is called.\nThis approach is called lexical scoping. Python and many other languages also use lexical scoping.\nWhy is the enclosing environment defined in this way? Recall our example where I tried to break the usage of the lm function by redefining lm.fit.\n\nlm.fit <- function(x) print('hi')\ny <- rnorm(10)\nx <- rnorm(10)\nmod <- lm(y~x)  # this still works!\n\nWhen R looks for lm.fit when it is called within lm, it looks in the enclosing environment of lm. That is where lm is defined, which is the stats package namespace. It finds lm.fit there. All is well! In contrast, if the scoping rules looked for lm.fit where lm was called from, then the user-defined lm.fit would be found and lm() would not work until that lm.fit was removed. That would be a very fragile system!\nLet’s dig deeper to understand where R looks for non-local variables, illustrating lexical scoping:\n\nx <- 3\nf2 <- function() print(x)\nf <- function() {\n    x <- 7\n    f2()\n}\nf() # what will happen?\n\nx <- 3\nf2 <- function() print(x)\nf <- function() {\n    x <- 7\n    f2()\n}\nx <- 100\nf() # what will happen?\n\nx <- 3\nf <- function() {\n    f2 <- function() { print(x) }\n    x <- 7\n    f2()\n}\nf() # what will happen?\n\nx <- 3\nf <- function() { \n    f2 <- function() { print(x) }\n    f2()\n} \nf() # what will happen?\n\nHere’s a tricky example:\n\ny <- 100\nfun_constructor <- function(){\n    y <- 10\n    g <- function(x) {\n            return(x + y)\n        }\n    return(g)\n}\n## fun_constructor() creates functions\nmyfun <- fun_constructor()\nmyfun(3)\n\n[1] 13\n\n\nLet’s work through this:\n\nWhat is the enclosing environment of the function g()?\nWhat does g() use for y?\nWhen fun_constructor() finishes, does its environment disappear? What would happen if it did?\nWhat is the enclosing environment of myfun()?\n\nThe following code helps explain things, but it’s a bit confusing because environment() gives back different results depending on whether it is given a function as its argument. If given a function, it returns the enclosing environment for that function. If given no argument, it returns the current execution environment.\n\nenvironment(myfun)  # enclosing environment of h()\n\n<environment: 0x563fa34d5720>\n\nls(environment(myfun)) # objects in that environment\n\n[1] \"g\" \"y\"\n\nfun_constructor <- function(){\n    print(environment()) # execution environment of fun_constructor()\n    y <- 10\n    g <- function(x) x + y\n    return(g)\n}\nmyfun <- fun_constructor()\n\n<environment: 0x563fa25c1a40>\n\nenvironment(myfun)\n\n<environment: 0x563fa25c1a40>\n\nmyfun(3)\n\n[1] 13\n\nenvironment(myfun)$y\n\n[1] 10\n\n## advanced: explain this:\nenvironment(myfun)$g\n\nfunction(x) x + y\n<environment: 0x563fa25c1a40>\n\n\nBe careful when using variables from the enclosing environment as the value of that variable in the enclosing environment may well not be what you expect it to be. In general it’s bad practice to use variables that are taken from environments outside that of a function, but in some cases it can be useful. Here are some examples of using variables outside of the frame of a function.\n\nx <- 3\nf <- function() {x <- x^2; print(x)}\nf()\nx # what do you expect?\nf <- function() { assign('x', x^2, env = .GlobalEnv) } \n## careful: could be dangerous as a variable is changed as a side effect\nf()\nx\nf <- function(x) { x <<- x^2 }\n## careful: could be dangerous as a variable is changed as a side effect\nf(5)\nx\n\n\nComprehension problem\nHere’s a case where something I tried failed and I had to think more carefully about scoping to understand why.\n\nset.seed(1) \nrnorm(1) \n\n[1] -0.626\n\nsave(.Random.seed, file = 'tmp.Rda') \nrnorm(1)\n\n[1] 0.184\n\ntmp <- function() { \n  load('tmp.Rda') \n  print(rnorm(1)) \n}\ntmp()\n\n[1] -0.836\n\n\nQuestion: what was I hoping that code to do, and why didn’t it work?\n\n\nDetecting non-local variables\nWe can use codetools::findGlobals() to detect non-local variables when we are programming.\n\nf <- function() {\n    y <- 3\n    print(x + y)\n}\ncodetools::findGlobals(f)\n\n[1] \"{\"     \"+\"     \"<-\"    \"print\" \"x\"    \n\n\nIs that result what you would expect? What does it say about my statement that using non-local variables is a bad idea?\n\n\n\nClosures\nOne way to avoid passing data by value is to associate data with a function, using a closure. This is a functional programming way to achieve something like an OOP class. This Wikipedia entry nicely summarizes the idea, which is a general functional programming idea and not specific to R.\nUsing a closure involves creating one (or more functions) within a function call and returning the function(s) as the output. When one executes the original function, the new function(s) is created and returned and one can then call that new function(s). The new function then can access objects in the enclosing environment (the environment of the original function) and can use <<- to assign into the enclosing environment, to which the function (or the multiple functions) have access. The nice thing about this compared to using a global variable is that the data in the closure is bound up with the function(s) and is protected from being changed by the user of the closure. Chambers provides an example of this in Sec. 5.4.\n\nx <- rnorm(10)\nscaler_constructor <- function(input){\n    data <- input\n    g <- function(param) return(param * data) \n    return(g)\n}\nscaler <- scaler_constructor(x)\nrm(x) # to demonstrate we no longer need x\nscaler(3)\n\n [1]  4.786  0.989 -2.461  1.462  2.215  1.727 -0.916  4.535  1.170 -1.864\n\n\nSo calling scaler(3) multiplies 3 by the value of data stored in the closure (the enclosing environment) of the function scaler.\nIt turns out that it can be hard to see the memory used involved in the closure.\n\nx <- rnorm(1e7)\nscaler <- scaler_constructor(x)\nobject.size(scaler) # hmmm\n\n3800 bytes\n\nobject.size(environment(scaler)$data)\n\n80000048 bytes\n\nlibrary(pryr)\n\n\nAttaching package: 'pryr'\n\n\nThe following object is masked _by_ '.GlobalEnv':\n\n    f\n\nobject_size(scaler) # that's better!\n\n80,012,560 B\n\n\nHere’s a fun example. You might do this with an apply variant, in particular replicate, but this is slick:\n\nmake_container <- function(n) {\n    x <- numeric(n)\n    i <- 1\n    \n    function(value = NULL) {\n        if (is.null(value)) {\n            return(x)\n        } else {\n            x[i] <<- value\n            i <<- i + 1\n        }    \n    }\n}\nnboot <- 100\nbootmeans <- make_container(nboot)\ndata <- faithful[ , 1] # Old Faithful geyser eruption lengths\nfor (i in 1:nboot)\n    bootmeans(mean(sample(data, length(data),\n      replace=TRUE)))\nbootmeans()\n\n  [1] 3.59 3.41 3.47 3.46 3.43 3.48 3.51 3.48 3.50 3.46 3.41 3.62 3.46 3.46 3.49\n [16] 3.50 3.56 3.50 3.58 3.60 3.46 3.45 3.50 3.41 3.46 3.59 3.35 3.50 3.51 3.37\n [31] 3.46 3.38 3.58 3.52 3.45 3.58 3.50 3.47 3.54 3.57 3.53 3.58 3.40 3.50 3.50\n [46] 3.56 3.41 3.45 3.50 3.53 3.49 3.57 3.46 3.50 3.43 3.48 3.54 3.45 3.53 3.53\n [61] 3.46 3.36 3.41 3.58 3.58 3.47 3.51 3.50 3.56 3.48 3.39 3.48 3.62 3.54 3.51\n [76] 3.52 3.47 3.49 3.43 3.45 3.40 3.52 3.43 3.49 3.51 3.56 3.55 3.46 3.30 3.56\n [91] 3.47 3.49 3.41 3.40 3.46 3.43 3.43 3.44 3.45 3.42\n\n\nThe closure stores the bootstrapped values and\n\n\nEnvironments and the search path\nSo far we’ve seen lexical scoping in action primarily in terms of finding variables in a single enclosing environment. But what if the variable is not found in either the frame/environment of the function or the enclosing environment? When R goes looking for an object (in the form of a symbol), it starts in the current environment (e.g., the frame/environment of a function) and then runs up through the enclosing environments, until it reaches the global environment, which is where R starts when you open R.\nThen, if R can’t find the object when reaching the global environment, it runs through the search path, which you can see with search(). The search path is a set of additional environments, mainly the namespaces of packages loaded in the R session.\n\nsearch()\n\n [1] \".GlobalEnv\"          \"package:pryr\"        \"package:R6\"         \n [4] \"package:fields\"      \"package:viridis\"     \"package:viridisLite\"\n [7] \"package:spam\"        \"package:dplyr\"       \"package:stringr\"    \n[10] \"tools:quarto\"        \"package:stats\"       \"package:graphics\"   \n[13] \"package:grDevices\"   \"package:utils\"       \"package:datasets\"   \n[16] \"package:SCF\"         \"package:methods\"     \"Autoloads\"          \n[19] \"package:base\"       \n\n\nWe can see the full set of environments in which R looks using code such as the following. This illustrates that in looking for a local variable used in lm the search process would go through the stats namespace, the base R namespace, the global environment and then the various packages loaded in the current R session.\n\nx <- environment(lm)\nwhile (environmentName(x) != environmentName(emptyenv())) {\n    print(environmentName(x))\n    x <- parent.env(x) # enclosing env't, NOT parent frame!\n}\n\n[1] \"stats\"\n[1] \"imports:stats\"\n[1] \"base\"\n[1] \"R_GlobalEnv\"\n[1] \"package:pryr\"\n[1] \"package:R6\"\n[1] \"package:fields\"\n[1] \"package:viridis\"\n[1] \"package:viridisLite\"\n[1] \"package:spam\"\n[1] \"package:dplyr\"\n[1] \"package:stringr\"\n[1] \"tools:quarto\"\n[1] \"package:stats\"\n[1] \"package:graphics\"\n[1] \"package:grDevices\"\n[1] \"package:utils\"\n[1] \"package:datasets\"\n[1] \"package:SCF\"\n[1] \"package:methods\"\n[1] \"Autoloads\"\n[1] \"base\"\n\n\nThat code uses environmentName(), which prints out a nice-looking version of the environment name.\nHere’s an alternative way using pryr:\n\nlibrary(pryr)\nx <- environment(lm)\nparenvs(x, all = TRUE)  \n\n   label                              name                 \n1  <environment: namespace:stats>     \"\"                   \n2  <environment: 0x563f96c86f68>      \"imports:stats\"      \n3  <environment: namespace:base>      \"\"                   \n4  <environment: R_GlobalEnv>         \"\"                   \n5  <environment: package:pryr>        \"package:pryr\"       \n6  <environment: package:R6>          \"package:R6\"         \n7  <environment: package:fields>      \"package:fields\"     \n8  <environment: package:viridis>     \"package:viridis\"    \n9  <environment: package:viridisLite> \"package:viridisLite\"\n10 <environment: package:spam>        \"package:spam\"       \n11 <environment: package:dplyr>       \"package:dplyr\"      \n12 <environment: package:stringr>     \"package:stringr\"    \n13 <environment: 0x563f969c5580>      \"tools:quarto\"       \n14 <environment: package:stats>       \"package:stats\"      \n15 <environment: package:graphics>    \"package:graphics\"   \n16 <environment: package:grDevices>   \"package:grDevices\"  \n17 <environment: package:utils>       \"package:utils\"      \n18 <environment: package:datasets>    \"package:datasets\"   \n19 <environment: package:SCF>         \"package:SCF\"        \n20 <environment: package:methods>     \"package:methods\"    \n21 <environment: 0x563f969e9510>      \"Autoloads\"          \n22 <environment: base>                \"\"                   \n23 <environment: R_EmptyEnv>          \"\"                   \n\n\nNote that eventually the global environment and the environments of the packages are nested within the base environment (of the base package) and the empty environment."
  },
  {
    "objectID": "units/unit5-programming.html#overview-2",
    "href": "units/unit5-programming.html#overview-2",
    "title": "Programming concepts",
    "section": "Overview",
    "text": "Overview\nThe main things to remember when thinking about memory use are: (1) numeric vectors take 8 bytes per element and (2) we need to keep track of when large objects are created, including local variables in the frames of functions.\nIn some of our work here we’ll use functions from the pryr package, which provides functions to help understand what is going on under the hood in R.\nIn general, don’t try to run this code within RStudio, as some of how RStudio works affects when copies are made. In particular the environment pane causes there to be an additional reference to each object. Also, as noted in the document, some of the output in the PDF does not reflect what is happening when running code directly within R, because of effects from the process of rendering the document.\n\nAllocating and freeing memory\nUnlike compiled languages like C, in R we do not need to explicitly allocate storage for objects. (However, we will see that there are times that we do want to allocate storage in advance, rather than successively concatenating onto a larger object.)\nR automatically manages memory, releasing memory back to the operating system when it’s not needed via garbage collection. Very occasionally you may want to remove large objects as soon as they are not needed. rm() does not actually free up memory, it just disassociates the name from the memory used to store the object. In general R will quickly clean up such objects without a reference (i.e., a name), so there is generally no need to call gc() to force the garbage collection. In particular, calling gc() uses some computation so it’s generally not recommended.\nIn a language like C in which the user allocates and frees up memory, memory leaks are a major cause of bugs. Basically if you are looping and you allocate memory at each iteration and forget to free it, the memory use builds up inexorably and eventually the machine runs out of memory. In R, with automatic garbage collection, this is generally not an issue, but occasionally memory leaks do occur.\n\n\nThe heap and the stack\nThe heap is the memory that is available for dynamically creating new objects while a program is executing, e.g., if you create a new object in R or call new in C++. When more memory is needed the program can request more from the operating system. When objects are removed in R, R will handle the garbage collection of releasing that memory.\nThe stack is the memory used for local variables when a function is called.\nThere’s a nice discussion of this on this Stack Overflow thread."
  },
  {
    "objectID": "units/unit5-programming.html#monitoring-memory-use",
    "href": "units/unit5-programming.html#monitoring-memory-use",
    "title": "Programming concepts",
    "section": "Monitoring memory use",
    "text": "Monitoring memory use\n\nMonitoring overall memory use on a UNIX-style computer\nTo understand how much memory is available on your computer, one needs to have a clear understanding of disk caching. The operating system will generally cache files/data in memory when it reads from disk. Then if that information is still in memory the next time it is needed, it will be much faster to access it the second time around than if it had to read the information from disk. While the cached information is using memory, that same memory is immediately available to other processes, so the memory is available even though it is “in use”.\nWe can see this via free -h (the -h is for ‘human-readable’, i.e. show in GB (G)) on Linux machine.\n          total used free shared buff/cache available \n    Mem:   251G 998M 221G   2.6G        29G      247G \n    Swap:  7.6G 210M 7.4G\nYou’ll generally be interested in the Mem row. (See below for some comments on Swap.) The shared column is complicated and probably won’t be of use to you. The buff/cache column shows how much space is used for disk caching and related purposes but is actually available. Hence the available column is the sum of the free and buff/cache columns (more or less). In this case only about 1 GB is in use (indicated in the used column).\ntop (Linux or Mac) and vmstat (on Linux) both show overall memory use, but remember that the amount actually available to you is the amount free plus any buff/cache usage. Here is some example output from vmstat:\n\n    procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu----- \n    r b   swpd      free   buff    cache si so bi bo in cs us sy id wa st \n    1 0 215140 231655120 677944 30660296  0  0  1  2  0  0 18  0 82  0  0\nIt shows 232 GB free and 31 GB used for cache and therefore available, for a total of 263 GB available.\nHere are some example lines from top:\n    KiB Mem : 26413715+total, 23180236+free, 999704 used, 31335072 buff/cache \n    KiB Swap:  7999484 total,  7784336 free, 215148 used. 25953483+avail Mem\nWe see that this machine has 264 GB RAM (the total column in the Mem row), with 259.5 GB available (232 GB free plus 31 GB buff/cache as seen in the Mem row). (I realize the numbers don’t quite add up for reasons I don’t fully understand, but we probably don’t need to worry about that degree of exactness.) Only 1 GB is in use.\nSwap is essentially the reverse of disk caching. It is disk space that is used for memory when the machine runs out of physical memory. You never want your machine to be using swap for memory because your jobs will slow to a crawl. As seen above, the swap line in both free and top shows 8 GB swap space, with very little in use, as desired.\n\n\nMonitoring memory use in R\nThere are a number of ways to see how much memory is being used. When R is actively executing statements, you can use top from the UNIX shell. In R, you can use gc().\n\ngc()\n\n           used (Mb) gc trigger (Mb) max used (Mb)\nNcells  2347352  125    4157415  222  4157415  222\nVcells 14223044  109   23059500  176 14770369  113\n\n\ngc() reports memory use and free memory as Ncells and Vcells. Ncells concerns the overhead of running R and Vcells relates to objects created by the user, so you’ll want to focus on Vcells. You can see the number of Mb currently used (the “used” column of the output) and the maximum used in the session (the “max used” column)“.\nWe can see the size of an object with object.size() from base R or object_size from pryr:\n\nx <- rnorm(1e8) # should use about 800 Mb\nobject.size(x)\n\n800000048 bytes\n\npryr::object_size(x)\n\n800,000,048 B\n\n\nA newer alternative to gc() is to use functions in the pryr package such as mem_used() and mem_change().\n\nlibrary(pryr)\nmem_used() \n\n1.05 GB\n\ngc()\n\n           used (Mb) gc trigger (Mb) max used (Mb)\nNcells 2.35e+06  126   4.16e+06  222 4.16e+06  222\nVcells 1.14e+08  872   1.67e+08 1275 1.14e+08  872\n\nrm(x)\nmem_used()\n\n245 MB\n\ngc() # note the \"max used\" column is unchanged\n\n           used (Mb) gc trigger (Mb) max used (Mb)\nNcells  2348689  126   4.16e+06  222 4.16e+06  222\nVcells 14226529  109   1.07e+08  816 1.14e+08  872\n\nmem_change(x <- rnorm(1e8)) \n\n800 MB\n\nmem_change(x <- rnorm(1e7))\n\n-720 MB\n\n\nYou can reset the value given for max used, with gc(reset = TRUE).\nIn Windows only, memory.size() tells how much memory is being used.\nHere is a useful function, ls.sizes(), that wraps object.size() to report the largest \\(n\\) objects in a given environment:\n\nls.sizes <- function(howMany = 10, minSize = 1){\n    pf <- parent.frame()\n    obj <- ls(pf) # or ls(sys.frame(-1)) \n    objSizes <- sapply(obj, function(x) {\n                               pryr::object_size(get(x, pf))\n                           })\n    ## or sys.frame(-4) to get out of FUN, lapply(), sapply() and sizes()\n    objNames <- names(objSizes)\n    howmany <- min(howMany, length(objSizes))\n    ord <- order(objSizes, decreasing = TRUE)\n    objSizes <- objSizes[ord][1:howMany]\n    objSizes <- objSizes[objSizes > minSize]\n    objSizes <- matrix(objSizes, ncol = 1)\n    rownames(objSizes) <- objNames[ord][1:length(objSizes)]\n    colnames(objSizes) <- \"bytes\"\n    cat('object')\n    print(format(objSizes, justify = \"right\", width = 11),\n              quote = FALSE)\n}\n\nUnfortunately with R6 and ReferenceClasses, closures, environments, and other such “containers”, it can be hard to see how much memory the object is using, including all the components of the object. Here’s a trick where we serialize the object, as if to export it, and then see how long the binary representation is. In this case the object is a closure that contains a large vector.\n\nlibrary(pryr)\nx <- rnorm(1e7)\nf <- function(input){\n    data <- input\n    g <- function(param) return(param * data) \n    return(g)\n}\nmyFun <- f(x)\nrm(x)\nobject.size(myFun)\n\n1608 bytes\n\nobject_size(myFun)\n\n80,013,376 B\n\nlength(serialize(myFun, NULL))\n\n[1] 160007714\n\n\nNote that our discussion of copy-on-modify should help us understand why the serialized object is 160 MB, but only 80 MB is used to store the 10,000,000 numbers.\nHere we examine the size of an environment:\n\ne <- new.env()\ne$x <- rnorm(1e7)\nobject.size(e)\n\n56 bytes\n\nobject_size(e)\n\n80,000,496 B\n\nlength(serialize(e, NULL))\n\n[1] 80000192\n\n\nOne frustration with memory management is that if your code bumps up against the memory limits of the machine, it can be very slow to respond even when you’re trying to cancel the statement with Ctrl-C. You can impose memory limits in Linux by starting R (from the UNIX prompt) in a fashion such as this\n\nR --max-vsize=1000M\n\nThen if you try to create an object that will push you over that limit or execute code that involves going over the limit, it will simply fail with the message “Error: vector memory exhausted (limit reached?)”. So this approach may be a nice way to avoid paging/swapping by setting the maximum in relation to the physical memory of the machine. It might also help in debugging memory leaks because the program would fail at the point that memory use was increasing. I haven’t played around with this much, so I offer this with a note of caution.\nApparently there is a memory profiler in R, Rprofmem, but it needs to be enabled when R is compiled (i.e., installed on the machine), because it slows R down even when not used. So I’ve never gotten to the point of playing around with it."
  },
  {
    "objectID": "units/unit5-programming.html#how-memory-is-used-in-r",
    "href": "units/unit5-programming.html#how-memory-is-used-in-r",
    "title": "Programming concepts",
    "section": "How memory is used in R",
    "text": "How memory is used in R\n\nA secret weapon: inspect\nWe can use an internal function called inspect to see where in memory an object is stored. It’s particularly useful for understanding storage and memory use for complicated data structures. We’ll also see that this can be a handy tool for seeing where copies are made and where they are not.\n\nx <- rnorm(5)\n.Internal(inspect(x))\n\n@563fa7b9f2f8 14 REALSXP g0c4 [REF(2)] (len=5, tl=0) -0.742042,0.577906,1.85653,0.449136,0.746484\n\n\nThe first output is the address in memory (in hexadecimal) of the vector. The REALSXP indicates that the vector is stored as a real-valued “S” object under the hood in C. REF(2) indicates that only two variables are referring to this particular memory location (more on this in much detail in a bit).\n\n\nMemory use in specific circumstances\n\nHow lists are stored\nHere we can use inspect() to see how the overall list is stored as well as the elements of the list and the attributes of the list.\n\nnums <- rnorm(5)\nobj <- list(a = nums, b = nums, c = rnorm(5), d = list(some_string = \"adfs\"))\n.Internal(inspect(obj$a))\n\n@563fa89e0058 14 REALSXP g0c4 [REF(4)] (len=5, tl=0) -1.47998,1.38282,0.22264,0.0640071,0.565649\n\n.Internal(inspect(obj$b))\n\n@563fa89e0058 14 REALSXP g0c4 [REF(5)] (len=5, tl=0) -1.47998,1.38282,0.22264,0.0640071,0.565649\n\n.Internal(inspect(obj$c))\n\n@563fa89e06e8 14 REALSXP g0c4 [REF(1)] (len=5, tl=0) -1.07213,-0.451605,-0.0594438,0.153536,-0.939612\n\n.Internal(inspect(obj))\n\n@563fa89cad88 19 VECSXP g0c3 [REF(2),ATT] (len=4, tl=0)\n  @563fa89e0058 14 REALSXP g0c4 [REF(6)] (len=5, tl=0) -1.47998,1.38282,0.22264,0.0640071,0.565649\n  @563fa89e0058 14 REALSXP g0c4 [REF(6)] (len=5, tl=0) -1.47998,1.38282,0.22264,0.0640071,0.565649\n  @563fa89e06e8 14 REALSXP g0c4 [REF(2)] (len=5, tl=0) -1.07213,-0.451605,-0.0594438,0.153536,-0.939612\n  @563fa7693628 19 VECSXP g0c1 [REF(1),ATT] (len=1, tl=0)\n    @563fa78cee78 16 STRSXP g0c1 [REF(3)] (len=1, tl=0)\n      @563fa78ceeb0 09 CHARSXP g0c1 [REF(2),gp=0x60] [ASCII] [cached] \"adfs\"\n  ATTRIB:\n    @563fa2a9c230 02 LISTSXP g0c0 [REF(1)] \n      TAG: @563f96738800 01 SYMSXP g1c0 [MARK,REF(65535),LCK,gp=0x6000] \"names\" (has value)\n      @563fa7693660 16 STRSXP g0c1 [REF(1)] (len=1, tl=0)\n    @563fa59fa8f8 09 CHARSXP g0c2 [REF(4),gp=0x61] [ASCII] [cached] \"some_string\"\nATTRIB:\n  @563fa2a9c2a0 02 LISTSXP g0c0 [REF(1)] \n    TAG: @563f96738800 01 SYMSXP g1c0 [MARK,REF(65535),LCK,gp=0x6000] \"names\" (has value)\n    @563fa89cadd8 16 STRSXP g0c3 [REF(65535)] (len=4, tl=0)\n      @563f96a3a5d0 09 CHARSXP g1c1 [MARK,REF(807),gp=0x61] [ASCII] [cached] \"a\"\n      @563f96d59b80 09 CHARSXP g1c1 [MARK,REF(699),gp=0x61] [ASCII] [cached] \"b\"\n      @563f96739bc0 09 CHARSXP g1c1 [MARK,REF(1008),gp=0x61] [ASCII] [cached] \"c\"\n      @563f96bd86b0 09 CHARSXP g1c1 [MARK,REF(421),gp=0x61] [ASCII] [cached] \"d\"\n\n\nWhat do we notice?\n\nThe list itself is a vector of pointers to the component elements and a pointer to the attributes information.\nEach element has its own address.\nAttributes are themselves stored in particular locations.\nTwo elements of a list can use the same memory (see a and b here, both located at @55a3e79de068).\n\nThe pryr package provides address() or inspect() as an alternative to .Internal(inspect()) though even pryr::inspect() doesn’t give us the richness of information about complicated objects that .Internal(inspect()) does.\n\naddress(obj)\n\n[1] \"0x563fa89cad88\"\n\ninspect(obj)\n\n<VECSXP 0x563fa89cad88>\n  <REALSXP 0x563fa89e0058>\n  [REALSXP 0x563fa89e0058]\n  <REALSXP 0x563fa89e06e8>\n  <VECSXP 0x563fa7693628>\n    <STRSXP 0x563fa78cee78>\n      <CHARSXP 0x563fa78ceeb0>\n  attributes: \n    <LISTSXP 0x563fa2a9c230>\n    tag: \n      <SYMSXP 0x563f96738800>\n    car: \n      <STRSXP 0x563fa7693660>\n        <CHARSXP 0x563fa59fa8f8>\n    cdr: \n      NULL\nattributes: \n  <LISTSXP 0x563fa2a9c2a0>\n  tag: \n    [SYMSXP 0x563f96738800]\n  car: \n    <STRSXP 0x563fa89cadd8>\n      <CHARSXP 0x563f96a3a5d0>\n      <CHARSXP 0x563f96d59b80>\n      <CHARSXP 0x563f96739bc0>\n      <CHARSXP 0x563f96bd86b0>\n  cdr: \n    NULL\n\ntry(address(obj$a)) # doesn't work\n\nError : x must be the name of an object\n\n\n\n\nHow character strings are stored.\nSimilar tricks are used for storing character vectors. We’ll explore this in a problem on PS4 using inspect().\n\n\nReplacement functions\nReplacement functions can hide the use of additional memory. How much memory is used here? (Try running in R (not RStudio) on your own computer and note the max_used column in the gc() result should increase after we modify the dimensionality of x, indicating a copy was made.)\n\nrm(x)\ngc(reset = TRUE)\n\n           used (Mb) gc trigger (Mb) max used (Mb)\nNcells  2349335  126   4.16e+06  222  2349335  126\nVcells 34228661  261   1.07e+08  816 34228661  261\n\nx <- rnorm(1e7)\ngc()\n\n           used (Mb) gc trigger (Mb) max used (Mb)\nNcells  2349260  126   4.16e+06  222  2361600  126\nVcells 44228535  338   1.07e+08  816 44249652  338\n\ndim(x) <- c(1e4, 1e3)\ndiag(x) <- 1\ngc()\n\n           used (Mb) gc trigger (Mb) max used (Mb)\nNcells  2349287  126   4.16e+06  222  2367186  126\nVcells 44228575  338   1.07e+08  816 54261390  414\n\n\nHowever, not all replacement functions actually involve creating a new object and replacing the original object. Here [<- is a primitive function, so the modification of the vector can be done without a copy in the underlying execution in C.\n\nWarning: For some reason when I compile this document a copy is made. Try it in R (not RStudio) on your own computer, and you should see that the address of x is unchanged and no additional memory has been used.\n\n\nrm(x)\ngc(reset = TRUE)\n\n           used (Mb) gc trigger (Mb) max used (Mb)\nNcells  2349257  126   4.16e+06  222  2349257  126\nVcells 34228673  261   1.07e+08  816 34228673  261\n\nx <- rnorm(1e7)\naddress(x)\n\n[1] \"0x7f2a30ee4010\"\n\ngc()\n\n           used (Mb) gc trigger (Mb) max used (Mb)\nNcells  2349286  126   4.16e+06  222  2367727  126\nVcells 44228717  338   1.07e+08  816 44260257  338\n\nx[5] <- 7\n`[<-`\n\n.Primitive(\"[<-\")\n\n## When run plainly in R, should be the same address as before,\n## indicating no copy was made. Rendering the doc messes the\n## result up!\naddress(x)\n\n[1] \"0x7f2a2c298010\"\n\ngc()\n\n           used (Mb) gc trigger (Mb) max used (Mb)\nNcells  2349337  126   4.16e+06  222  2374346  127\nVcells 44228807  338   1.07e+08  816 54270479  414\n\n\nIt makes some sense that modifying elements of a vector doesn’t cause a copy – if it did, working with large vectors would be very difficult.\n\n\nFast representations of sequences\nAs of R 3.5.0, 1:n is not stored in memory as a vector of length n, but rather is represented by the first and last value in the sequence. However, some of the functions we use to determine object size don’t give us the right answer in this case.\n\nlibrary(microbenchmark)\n\nn <- 1e6\nmicrobenchmark(tmp <- 1:n)\n\nUnit: nanoseconds\n       expr min  lq mean median  uq  max neval\n tmp <- 1:n 170 179  259    183 254 3929   100\n\nobject.size(tmp)  # incorrect\n\n4000048 bytes\n\nobject_size(tmp)  # correct\n\n680 B\n\nmem_change(mySeq <- 1:n)  # not sure why the result is negative!\n\n-11.1 kB\n\nlength(serialize(mySeq, NULL)) \n\n[1] 133\n\n\nOne implication is that in older versions of R, indexing large subsets can involve a lot of memory use.\n\nx <- rnorm(1e7)\ny <- x[1:(length(x) - 1)]\n\nIn this case, in old versions of R, more memory was used than just for x and y, because the index sequence itself used a bunch of memory.\n\n\n\nCopy-on-modify\nNext we’ll see that something like lazy evaluation occurs as well with some functionality called delayed copying or copy-on-modify. When we discussed R as being call-by-value, copy-on-modify was one of the reasons that copies of arguments are not always made. (But we didn’t talk about it at that time.)\n\nCopy-on-modify in function calls\nLet’s see what goes on within a function in terms of memory use in different situations.\n\nrm(x)\ngc(reset = TRUE)\n\n           used (Mb) gc trigger (Mb) max used (Mb)\nNcells  2386381  128   4.16e+06  222  2386381  128\nVcells 34282400  262   1.07e+08  816 34282400  262\n\nf <- function(x){\n    print(gc())\n    print(x[1])\n    print(gc())\n    address(x)\n    return(x)\n}\n\ny <- rnorm(1e7)\ngc()\n\n           used (Mb) gc trigger (Mb) max used (Mb)\nNcells  2386361  128   4.16e+06  222  2404627  128\nVcells 44282397  338   1.07e+08  816 44313612  338\n\naddress(y)\n\n[1] \"0x7f2a30860010\"\n\nout <- f(y)\n\n           used (Mb) gc trigger (Mb) max used (Mb)\nNcells  2386391  128   4.16e+06  222  2404627  128\nVcells 44282437  338   1.07e+08  816 44313612  338\n[1] 1.38\n           used (Mb) gc trigger (Mb) max used (Mb)\nNcells  2386401  128   4.16e+06  222  2404627  128\nVcells 44282460  338   1.07e+08  816 44313612  338\n\naddress(y)\n\n[1] \"0x7f2a30860010\"\n\naddress(out)\n\n[1] \"0x7f2a30860010\"\n\n\nWe see that y, the local variable x in the function frame, and out all use the same memory, so no copies are made here. The gc() output confirms that no additional memory was used.\n\n\nCopy-on-modify in general\nIn fact, copy-on-modify occurs outside function calls as well. Copies of objects are not made until one of the objects is actually modified. Initially, the copy points to the same memory location as the original object.\n\nrm(y); rm(out)\ngc(reset = TRUE)\n\n           used (Mb) gc trigger (Mb) max used (Mb)\nNcells  2386428  128   4.16e+06  222  2386428  128\nVcells 34283542  262   1.07e+08  816 34283542  262\n\ny <- rnorm(1e7)\ngc()\n\n           used (Mb) gc trigger (Mb) max used (Mb)\nNcells  2386440  128   4.16e+06  222  2398693  128\nVcells 44283561  338   1.07e+08  816 44304533  338\n\naddress(y)\n\n[1] \"0x7f2a30860010\"\n\nx <- y\ngc()\n\n           used (Mb) gc trigger (Mb) max used (Mb)\nNcells  2386466  128   4.16e+06  222  2404694  128\nVcells 44283606  338   1.07e+08  816 44314795  338\n\nobject_size(x, y)  # from pryr\n\n80,000,048 B\n\naddress(x)\n\n[1] \"0x7f2a30860010\"\n\nx[1] <- 5\ngc()\n\n           used (Mb) gc trigger (Mb) max used (Mb)\nNcells  2386499  128   4.16e+06  222  2412508  129\nVcells 54283655  414   1.07e+08  816 54325827  414\n\naddress(x)\n\n[1] \"0x7f2a229ff010\"\n\nobject_size(x, y)\n\n160,000,096 B\n\nrm(x)\nx <- y\naddress(x)\n\n[1] \"0x7f2a30860010\"\n\naddress(y)\n\n[1] \"0x7f2a30860010\"\n\ny[1] <- 5\naddress(x)\n\n[1] \"0x7f2a30860010\"\n\naddress(y)\n\n[1] \"0x7f2a1ddb3010\"\n\n\nOr we can see this using mem_change().\n\nlibrary(pryr)\nrm(x)\nrm(y)\nmem_change(x <- rnorm(1e7))\n\n80 MB\n\naddress(x)\n\n[1] \"0x7f2a30860010\"\n\nmem_change(x[3] <- 8)\n\n320 B\n\naddress(x)\n\n[1] \"0x7f2a30860010\"\n\nmem_change(y <- x)\n\n376 B\n\naddress(y)\n\n[1] \"0x7f2a30860010\"\n\nmem_change(x[3] <- 8)\n\n80 MB\n\naddress(x)\n\n[1] \"0x7f2a229ff010\"\n\naddress(y)\n\n[1] \"0x7f2a30860010\"\n\n\nChallenge: explain the results of the example above.\n\n\nHow does copy-on-modify work?\nR keeps track of how many names refer to an object and only makes copies as needed when multiple names refer to an object. Note the value of REF and the address returned by .Internal(inspect()), or simply use refs() and address() from pryr.\nWe’ll see this live in class. Unfortunately both rendering and RStudio can give us confusing results for ref(), so I’m adding the clean results from just running in R here in comments.\n\na <- rnorm(5)\naddress(a)\n\n[1] \"0x563fa2730658\"\n\n## refs(a)  ## 1\nb <- a\naddress(b)\n\n[1] \"0x563fa2730658\"\n\n## refs(a)  ## 2\n## refs(b)  ## 2\nrm(b)\n## refs(a)  ## 1\n\nb <- a\na[2] <- 0\naddress(a)\n\n[1] \"0x563fa2c40218\"\n\naddress(b)\n\n[1] \"0x563fa2730658\"\n\n## refs(a)  ## 1\n## refs(b)  ## 1\n\nThis notion of reference counting occurs in other contexts, such as shared pointers in C++ and garbage collection (deletion of unused objects) in Python and R.\nIn older versions of R (before R 4.0) there were some shortcomings in how R managed this, and one could see different results than shown above."
  },
  {
    "objectID": "units/unit5-programming.html#strategies-for-saving-memory",
    "href": "units/unit5-programming.html#strategies-for-saving-memory",
    "title": "Programming concepts",
    "section": "Strategies for saving memory",
    "text": "Strategies for saving memory\nA couple basic strategies for saving memory include:\n\nAvoiding unnecessary copies.\nRemoving objects that are not being used, at which point the R garbage collector should free up the memory.\n\nIf you’re really trying to optimize memory use, you may also consider:\n\nUsing R6 classes and similar strategies to pass by reference.\nSubstituting integer and logical vectors for numeric vectors when possible."
  },
  {
    "objectID": "units/unit5-programming.html#example-1",
    "href": "units/unit5-programming.html#example-1",
    "title": "Programming concepts",
    "section": "Example",
    "text": "Example\nLet’s work through a real example where we keep a running tally of current memory in use and maximum memory used in a function call. We’ll want to consider hidden uses of memory, when copies are made, and lazy evaluation. This code is courtesy of Yuval Benjamini. For our purposes here, let’s assume that xvar and yvar are very long vectors using a lot of memory. The use of .C() calls out to some user-written C code. (In a real example we’d also want to think about when copies are made in calling compiled code, but we don’t do that here.)\n\nfastcount <- function(xvar, yvar) {\n    print(xvar[1])\n    print(yvar[1])\n    naline <- is.na(xvar)\n    naline[is.na(yvar)] = TRUE\n    xvar[naline] <- 0\n    yvar[naline] <- 0\n    useline <- !naline\n    ## We'll ignore the rest of the code.\n    ## Table must be initialized for -1's\n    tablex <- numeric(max(xvar)+1)\n    tabley <- numeric(max(yvar)+1)\n    stopifnot(length(xvar) == length(yvar))\n    res <- .C(\"fastcount\",PACKAGE=\"GCcorrect\",\n              tablex = as.integer(tablex), tabley = as.integer(tabley),\n              as.integer(xvar), as.integer(yvar), as.integer(useline),\n              as.integer(length(xvar)))\n    xuse <- which(res$tablex>0)\n    xnames <- xuse - 1\n    resb <- rbind(res$tablex[xuse], res$tabley[xuse]) \n    colnames(resb) <- xnames\n    return(resb)\n}"
  },
  {
    "objectID": "units/unit1-intro.html",
    "href": "units/unit1-intro.html",
    "title": "Introduction to UNIX, computers, and key tools",
    "section": "",
    "text": "PDF"
  },
  {
    "objectID": "units/unit1-intro.html#some-useful-editors",
    "href": "units/unit1-intro.html#some-useful-editors",
    "title": "Introduction to UNIX, computers, and key tools",
    "section": "Some useful editors",
    "text": "Some useful editors\n\nvarious editors available on all operating systems:\n\ntraditional editors born in UNIX: emacs, vim\nsome newer editors: Atom, Sublime Text (Sublime is proprietary/not free)\n\nWindows-specific: WinEdt\nMac-specific: Aquamacs Emacs, TextMate, TextEdit\nRStudio provides a built-in editor for R code and R Markdown files. (Note: RStudio as a whole is an IDE (integrated development environment. The editor is just the editing window where you edit code (and R Markdown) files.)\nVSCode has a powerful code editor that is customized to work with various languages.\n\nAs you get started it’s ok to use a very simple text editor such as Notepad in Windows, but you should take the time in the next few weeks to try out more powerful editors such as one of those listed above. It will be well worth your time over the course of your graduate work and then your career.\nBe careful in Windows - file suffixes are often hidden."
  },
  {
    "objectID": "units/unit1-intro.html#optional-basic-emacs",
    "href": "units/unit1-intro.html#optional-basic-emacs",
    "title": "Introduction to UNIX, computers, and key tools",
    "section": "(Optional) Basic emacs",
    "text": "(Optional) Basic emacs\nEmacs is one option as an editor. I use Emacs a fair amount, so I’m including some tips here, but other editors listed above are just as good.\n\nEmacs has special modes for different types of files: R code files, C code files, Latex files – it’s worth your time to figure out how to set this up on your machine for the kinds of files you often work on\n\nFor working with R, ESS (emacs speaks statistics) mode is helpful. This is built into Aquamacs Emacs.\n\nTo open emacs in the terminal window rather than as a new window, which is handy when it’s too slow (or impossible) to pass (i.e., tunnel) the graphical emacs window through ssh: emacs -nw file.txt"
  },
  {
    "objectID": "units/unit1-intro.html#optional-emacs-keystroke-sequence-shortcuts.",
    "href": "units/unit1-intro.html#optional-emacs-keystroke-sequence-shortcuts.",
    "title": "Introduction to UNIX, computers, and key tools",
    "section": "(Optional) Emacs keystroke sequence shortcuts.",
    "text": "(Optional) Emacs keystroke sequence shortcuts.\n\nNote Several of these (Ctrl-a, Ctrl-e, Ctrl-k, Ctrl-y) work in the command line and other places as well.\n\n\n\n\n\n\n\n\nSequence\nResult\n\n\n\n\nCtrl-x,Ctrl-c\nClose the file\n\n\nCtrl-x,Ctrl-s\nSave the file\n\n\nCtrl-x,Ctrl-w\nSave with a new name\n\n\nCtrl-s\nSearch\n\n\nESC\nGet out of command buffer at bottom of screen\n\n\nCtrl-a\nGo to beginning of line\n\n\nCtrl-e\nGo to end of line\n\n\nCtrl-k\nDelete the rest of the line from cursor forward\n\n\nCtrl-space, then move to end of block\nHighlight a block of text\n\n\nCtrl-w\nRemove the highlighted block, putting it in the kill buffer\n\n\nCtrl-y (after using Ctrl-k or Ctrl-w)\nPaste from kill buffer (‘y’ is for ‘yank’)"
  },
  {
    "objectID": "units/unit4-goodPractices.html",
    "href": "units/unit4-goodPractices.html",
    "title": "Good practices: coding practices, debugging, and reproducible research",
    "section": "",
    "text": "PDF\nSources:\nThis unit covers good coding/software development practices, debugging (and practices for avoiding bugs), and doing reproducible research. As in later units of the course, the material is generally not specific to R, but some details and the examples are in R."
  },
  {
    "objectID": "units/unit4-goodPractices.html#editors",
    "href": "units/unit4-goodPractices.html#editors",
    "title": "Good practices: coding practices, debugging, and reproducible research",
    "section": "Editors",
    "text": "Editors\nUse an editor that supports the language you are using (e.g., Atom, Emacs/Aquamacs, Sublime, vim, VSCode, TextMate, WinEdt, or the built-in editor in RStudio). Some advantages of this can include: (1) helpful color coding of different types of syntax and of strings, (2) automatic indentation and spacing, (3) code can often be run or compiled from within the editor, (4) parenthesis matching, (5) line numbering (good for finding bugs)."
  },
  {
    "objectID": "units/unit4-goodPractices.html#coding-syntax",
    "href": "units/unit4-goodPractices.html#coding-syntax",
    "title": "Good practices: coding practices, debugging, and reproducible research",
    "section": "Coding syntax",
    "text": "Coding syntax\nThe files goodCode.R and badCode.R in the units directory of the class repository provide examples of code written such that it does and does not conform to the suggestions listed in this section.\nHere are some R-related style guides:\n\nAdler has style tips.\nThe tidyverse style guide or its offshoot Google R style.\nA empirical style guide based on the code of R Core and key package developers.\nThis R journal article summarizes the state of naming styles on CRAN.\n\nAnd here’s a summary of my own thoughts:\n\nHeader information: put metainfo on the code into the first few lines of the file as comments. Include who, when, what, how the code fits within a larger program (if appropriate), possibly the versions of R and key packages that you used.\nIndentation: do this systematically (your editor can help here). This helps you and others to read and understand the code and can help in detecting errors in your code because it can expose lack of symmetry.\nWhitespace: use a lot of it. Some places where it is good to have it are\n\naround operators (assignment and arithmetic);\nbetween function arguments;\nbetween list elements; and\nbetween matrix/array indices, in particular for missing indices.\n\nUse blank lines to separate blocks of code and comments to say what the block does\nSplit long lines at meaningful places.\nUse parentheses for clarity even if not needed for order of operations. For example, a/y*x will work but is not easy to read and you can easily induce a bug if you forget the order of ops.\nDocumentation - add lots of comments (but don’t belabor the obvious). Remember that in a few months, you may not follow your own code any better than a stranger. Some key things to document: (1) summarizing a block of code, (2) explaining a very complicated piece of code - recall our complicated regular expressions, (3) explaining arbitrary constant values.\nFor software development, break code into separate files (2000-3000 lines per file) with meaningful file names and related functions grouped within a file.\nChoose a consistent naming style for objects and functions: e.g. nIts vs. n.its vs numberOfIts vs. n_its\n\nThis R journal article summarizes the state of naming styles on CRAN.\nIn object-oriented languages such as Python and Java, periods are used in the context of object-oriented programming, so I recommend not using periods in the names of your objects.\n\nTry to have the names be informative without being overly long.\nDon’t overwrite names of objects/functions that already exist in R. E.g., don’t use ‘lm’. That said, the namespace system helps with the unavoidable cases where there are name conflicts.\nUse active names for functions (e.g., calcLogLik, calc_logLik rather than logLik or logLikCalc). The idea is that a function in a programming language is like a verb in regular language (a function does something), so use a verb in naming it.\nLearn from others’ code\n\nThis semester, someone will be reading your code - the GSI and and me when we look at your assignments. So to help us in understanding your code and develop good habits, put these ideas into practice in your assignments."
  },
  {
    "objectID": "units/unit4-goodPractices.html#coding-style",
    "href": "units/unit4-goodPractices.html#coding-style",
    "title": "Good practices: coding practices, debugging, and reproducible research",
    "section": "Coding style",
    "text": "Coding style\nThis is particularly focused on software development, but some of the ideas are useful for data analysis as well.\n\nBreak down tasks into core units\nWrite reusable code for core functionality and keep a single copy of the code (w/ backups of course or ideally version control) so you only need to make changes to a piece of code in one place\nSmaller functions are easier to debug, easier to understand, and can be combined in a modular fashion (like the UNIX utilities)\nWrite functions that take data as an argument and not lines of code that operate on specific data objects. Why? Functions allow us to reuse blocks of code easily for later use and for recreating an analysis (reproducible research). It’s more transparent than sourcing a file of code because the inputs and outputs are specified formally, so you don’t have to read through the code to figure out what it does.\nFunctions should:\n\nbe modular (having a single task);\nhave meaningful name; and\nhave a comment describing their purpose, inputs and outputs (see the help file for an R function of your choice for how this is done in that context).\n\nWrite tests for each function (i.e., unit tests)\nDon’t hard code numbers - use variables (e.g., number of iterations, parameter values in simulations), even if you don’t expect to change the value, as this makes the code more readable. For example, the speed of light is a constant in a scientific sense, but best to make it a variable in code: speedOfLight <- 3e8\nUse R lists to keep disparate parts of related data together\nPractice defensive programming (see also the discussion below on assertions)\n\ncheck function inputs and warn users if the code will do something they might not expect or makes particular choices;\ncheck inputs to if and the ranges in for loops:\n\nuse seq_len() and seq_along() instead of 1:n in setting up for loops\nuse if(isTRUE(condition)) in if statements in case the condition is NA (which would otherwise cause an error)\n\nprovide reasonable default arguments;\ndocument the range of valid inputs;\ncheck that the output produced is valid; and\nstop execution based on checks and give an informative error message.\n\nTry to avoid system-dependent code that only runs on a specific version of an OS or specific OS\nLearn from others’ code\nConsider rewriting your code once you know all the settings and conditions; often analyses and projects meander as we do our work and the initial plan for the code no longer makes sense and the code is no longer designed specifically for the job being done."
  },
  {
    "objectID": "units/unit4-goodPractices.html#assertions-and-testing",
    "href": "units/unit4-goodPractices.html#assertions-and-testing",
    "title": "Good practices: coding practices, debugging, and reproducible research",
    "section": "Assertions and testing",
    "text": "Assertions and testing\nBoth tests and assertions are critically important for writing robust code that is less likely to contain bugs.\nAssertions are checks in your code that the state of the program is as you expect, including arguments provided by users. In addition to simple use of stopifnot() and using if() combined with stop() and warning(), the assertthat and assertr packages provide useful tools. The checkmate package specifically helps in checking arguments.\nTests evaluate whether your code operates correctly. This can include tests that your code provides correct and useful errors when something goes wrong (so that means that a test might be to see if problematic input correctly produces an error). Unit tests are intended to test the behavior of small pieces (units) of code, generally individual functions. Unit tests naturally work well with the ideas above of writing small, modular functions. testthat and other packages are designed to make it easier to write sets of good tests.\nIn Lab 2, we’ll go over assertions and testing in detail."
  },
  {
    "objectID": "units/unit4-goodPractices.html#version-control",
    "href": "units/unit4-goodPractices.html#version-control",
    "title": "Good practices: coding practices, debugging, and reproducible research",
    "section": "Version control",
    "text": "Version control\n\nUse it! Even for projects that only you are working on.\nUse an issues tracker (e.g., the GitHub issues tracker is quite nice), or at least a simple to-do file, noting changes you’d like to make in the future.\nIn addition to good commit messages, it’s a good idea to keep good running notes documenting your projects.\n\nWe’ve already seen Git some and will see it in a lot more detail later in the semester, so I don’t have more to say here."
  },
  {
    "objectID": "units/unit4-goodPractices.html#some-basic-strategies",
    "href": "units/unit4-goodPractices.html#some-basic-strategies",
    "title": "Good practices: coding practices, debugging, and reproducible research",
    "section": "Some basic strategies",
    "text": "Some basic strategies\n\nHave a directory for each project with subdirectories with meaningful and standardized names: e.g., code, data, paper. The Journal of the American Statistical Association (JASA) has a template GitHub repository with some suggestions.\nHave a file of code for pre-processing, one or more for analysis, and one for figure/table preparation.\n\nThe pre-processing may involve time-consuming steps. Save the output of the pre-processing as a file that can be read in to the analysis script.\nYou may want to name your files something like this, so there is an obvious ordering: “1-prep.R”, “2-analysis.R”, “3-figs.R”.\nHave the code file for the figures produce the exact manuscript/report figures, operating on a file (e.g., .Rda file) that contains all the objects necessary to run the figure-producing code; the code producing the .Rda file should be in your analysis code file (or somewhere else sensible).\nAlternatively, use knitr, R Markdown, or Jupyter notebooks for your document preparation.\n\nKeep a document describing your running analysis with dates in a text file (i.e., a lab book).\nNote where data were obtained (and when, which can be helpful when publishing) and pre-processing steps in the lab book. Have data version numbers with a file describing the changes and dates (or in lab book). If possible, have all changes to data represented as code that processes the data relative to a fixed baseline dataset.\nNote what code files do what in the lab book.\nKeep track of the details of the system and software you are running your code under, e.g., operating system version, software (e.g., R, Python) versions, R or Python package versions, etc.\n\nIn R, sessionInfo() will report all this for you."
  },
  {
    "objectID": "units/unit4-goodPractices.html#formal-tools",
    "href": "units/unit4-goodPractices.html#formal-tools",
    "title": "Good practices: coding practices, debugging, and reproducible research",
    "section": "Formal tools",
    "text": "Formal tools\n\nIn some cases you may be able to carry out your complete workflow in a knitr/R Markdown document or in a Jupyter notebook.\nYou might consider workflow/pipeline management software such as Drake or other tools discussed in the CRAN Reproducible Research Task View. Alternatively, one can use the make tool, which is generally used for compiling code, as a tool for reproducible research: if interested, see the tutorial on Using make for workflows or this Journal of Statistical Software article for more details.\nYou might organize your workflow as an R package as described in this article.\nPackage management:\n\nR: You can manage the versions of R packages (and dependent packages) used in your project using package management packages such as renv, checkpoint, and packrat.\nPython: You can manage the versions of Python packages (and dependent packages) used in your project using Conda environments (or virtualenvs).\n\nIf your project uses multiple pieces of software (e.g., not just R or Python), you can set up a reproducible environment using containers, of which Docker containers are the best known. These provide something that is like a lightweight virtual machine in which you can install exactly the software (and versions) you want and then share with others. Docker container images are a key building block of various tools such as GitHub Actions and the Binder project\nYou can manage the configuration of a project using the config package. This allows you to set configuration values that control how your code/workflow runs, thereby enabling you to run the workflow under different scenarios (e.g., different input datasets, different models, different model configurations, etc.)."
  }
]