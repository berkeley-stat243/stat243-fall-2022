[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "about.html#sec-2a",
    "href": "about.html#sec-2a",
    "title": "About",
    "section": "Sec 2a",
    "text": "Sec 2a"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "mysite",
    "section": "",
    "text": "To learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "units/unit1.html",
    "href": "units/unit1.html",
    "title": "R bootcamp, Module 0: Recruit processing",
    "section": "",
    "text": "The GitHub site (https://github.com/berkeley-scf/r-bootcamp-fall-2022) is the main site for the bootcamp. It has information on logistics, software installation, and is the master repository for materials for the modules.\nWe have an Ed Discussion site for discussion and answering questions online during (and before) the bootcamp.\nIf you have an administrative question, email r-bootcamp@lists.berkeley.edu.\n\n\n\nThe campus WiFi is now eduroam, not AirBears. Follow these instructions for how to set up your eduroam account. If you need wireless access as a guest (i.e., you don’t have a CalNet ID), connect to ‘CalVisitor’."
  },
  {
    "objectID": "units/unit1.html#within-rstudio",
    "href": "units/unit1.html#within-rstudio",
    "title": "R bootcamp, Module 0: Recruit processing",
    "section": "Within RStudio",
    "text": "Within RStudio\nWithin RStudio go to File->New Project->Version Control->Git and enter:\n\n“Repository URL”: https://github.com/berkeley-scf/r-bootcamp-fall-2022\n“Project Directory Name”: r-bootcamp-fall-2022 (or something else of your choosing)\n“Directory”: ~/Desktop (or somewhere of your choosing)\n\nThen to update from the repository to get any changes we’ve made, you can select (from within RStudio): Tools->Version Control->Pull Branches\nor from the Environment/History/Git window, click on the Git tab and then on the blue down arrow.\nBe warned that you probably do not want to make your own notes or changes to the files we are providing. Because if you do, and you then do a “Git Pull” to update the materials, you’ll have to deal with the conflict between your local version and our version. You probably will want to make a personal copy of such files in another directory or by making copies of files with new names."
  },
  {
    "objectID": "units/unit1.html#from-a-maclinux-terminal-window",
    "href": "units/unit1.html#from-a-maclinux-terminal-window",
    "title": "R bootcamp, Module 0: Recruit processing",
    "section": "From a Mac/Linux terminal window",
    "text": "From a Mac/Linux terminal window\nRun the following commands:\n\ncd /directory/where/you/want/repository/located\ngit clone https://github.com/berkeley-scf/r-bootcamp-fall-2022\n\nThen to update from the repository to get any changes we’ve made:\n\ncd /directory/where/you/put/the/repository/r-bootcamp-fall-2022\ngit pull"
  },
  {
    "objectID": "units/unit1.html#as-a-zip-file",
    "href": "units/unit1.html#as-a-zip-file",
    "title": "R bootcamp, Module 0: Recruit processing",
    "section": "As a zip file",
    "text": "As a zip file\nIf you don’t want to bother using Git or have problems, simply download a zip file with all the material from https://github.com/berkeley-scf/r-bootcamp-fall-2022/archive/main.zip."
  },
  {
    "objectID": "units/unit2.html",
    "href": "units/unit2.html",
    "title": "Unit 2: Data Technologies",
    "section": "",
    "text": "References:"
  },
  {
    "objectID": "units/unit2.html#text-and-binary-files",
    "href": "units/unit2.html#text-and-binary-files",
    "title": "Unit 2: Data Technologies",
    "section": "Text and binary files",
    "text": "Text and binary files\nIn general, files can be divided into text files and binary files. In both cases, information is stored as a series of bits. Recall that a bit is a single value in base 2 (i.e., a 0 or a 1), while a byte is 8 bits.\nA text file is one in which the bits in the file encode individual characters. Note that the characters can include the digit characters 0-9, so one can include numbers in a text file by writing down the digits needed for the number of interest. Examples of text file formats include CSV, XML, HTML, and JSON.\nText files may be simple ASCII files (i.e., files encoded using ASCII) or in other encodings such as UTF-8, both covered in Section 4. ASCII files have 8 bits (1 byte) per character and can represent 128 characters (the 52 lower and upper case letters in English, 10 digits, punctuation and a few other things – basically what you see on a standard US keyboard). UTF-8 files have between 1 and 4 bytes per character.\nA binary file is one in which the bits in the file encode the information in a custom format and not simply individual characters. Binary formats are not (easily) human readable but can be more space-efficient and faster to work with (because it can allow random access into the data rather than requiring sequential reading). The meaning of the bytes in such files depends on the specific binary format being used and a program that uses the file needs to know how the format represents information. Examples of binary files include netCDF files, R data (e.g., .Rda) files, and compiled code files.\nNumbers in binary files are usually stored as 8 bytes per number. We’ll discuss this much more in Unit 6."
  },
  {
    "objectID": "units/unit2.html#common-file-types",
    "href": "units/unit2.html#common-file-types",
    "title": "Unit 2: Data Technologies",
    "section": "Common file types",
    "text": "Common file types\nHere are some of the common file types. Any of these types can be categorized as text or binary.\n\n‘Flat’ text files: data are often provided as simple text files. Often one has one record or observation per row and each column or field is a different variable or type of information about the record. Such files can either have a fixed number of characters in each field (fixed width format) or a special character (a delimiter) that separates the fields in each row. Common delimiters are tabs, commas, one or more spaces, and the pipe (|). Common file extensions are .txt and .csv. Metadata (information about the data) are often stored in a separate file. CSV files are quite common, but if you have files where the data contain commas, other delimiters can be good. Text can be put in quotes in CSV files, and this can allow use of commas within the data. This is difficult to deal with in bash, but read.table() in R handles this situation.\n\nOne occasionally tricky difficulty is as follows. If you have a text file created in Windows, the line endings are coded differently than in UNIX (a newline (the ASCII character \\n) and a carriage return (the ASCII character \\r) in Windows vs. only a newline in UNIX). There are UNIX utilities (fromdos in Ubuntu, including the SCF Linux machines and dos2unix in other Linux distributions) that can do the necessary conversion. If you see ^M at the end of the lines in a file, that’s the tool you need. Alternatively, if you open a UNIX file in Windows, it may treat all the lines as a single line. You can fix this with todos or unix2dos.\n\nIn some contexts, such as textual data and bioinformatics data, the data may in a text file with one piece of information per row, but without meaningful columns/fields.\nIn scientific contexts, netCDF (.nc) (and the related HDF5) are popular format for gridded data that allows for highly-efficient storage and contains the metadata within the file. The basic structure of a netCDF file is that each variable is an array with multiple dimensions (e.g., latitude, longitude, and time), and one can also extract the values of and metadata about each dimension. The ncdf4 package in R nicely handles working with netCDF files.\nData may also be in text files in formats designed for data interchange between various languages, in particular XML or JSON. These formats are “self-describing”; namely the metadata is part of the file. The XML2, rvest, and jsonlite packages are useful for reading and writing from these formats.\nYou may be scraping information on the web, so dealing with text files in various formats, including HTML. The XML2 and rvest packages are also useful for reading HTML.\nData may already be in a database or in the data storage of another statistical package (Stata, SAS, SPSS, etc.). The foreign package in R has excellent capabilities for importing Stata (read.dta()), SPSS (read.spss()), and SAS (read.ssd() and, for XPORT files, read.xport()), among others.\nFor Excel, there are capabilities to read an Excel file (see the readxl and XLConnect package among others), but you can also just go into Excel and export as a CSV file or the like and then read that into R. In general, it’s best not to pass around data files as Excel or other spreadsheet format files because (1) Excel is proprietary, so someone may not have Excel and the format is subject to change, (2) Excel imposes limits on the number of rows, (3) one can easily manipulate text files such as CSV using UNIX tools, but this is not possible with an Excel file, (4) Excel files often have more than one sheet, graphs, macros, etc., so they’re not a data storage format per se.\nR can easily interact with databases (SQLite, PostgreSQL, MySQL, Oracle, etc.), querying the database using SQL and returning results to R. More in the big data unit and in the large datasets tutorial mentioned above."
  },
  {
    "objectID": "units/unit2.html#core-r-functions",
    "href": "units/unit2.html#core-r-functions",
    "title": "Unit 2: Data Technologies",
    "section": "Core R functions",
    "text": "Core R functions\nread.table() is probably the most commonly-used function for reading in data. It reads in delimited files (read.csv() and read.delim() are special cases of read.table()). The key arguments are the delimiter (the sep argument) and whether the file contains a header, a line with the variable names. We can use read.fwf() to read from a fixed width text file into a data frame.\nThe most difficult part of reading in such files can be dealing with how R determines the classes of the fields that are read in. There are a number of arguments to read.table() and read.fwf() that allow the user to control the classes. One difficulty in older versions of R was that character fields were read in as factors.\nLet’s work through a couple examples. Before we do that, let’s look at the arguments to read.table(). Note that sep=“ separates on any amount of white space. In the code chunk below, I’ve told knitr not to print the output to the PDF; you can see the full output by running the code yourself.\n\ndat <- read.table(file.path('..', 'data', 'RTADataSub.csv'),\n                  sep = ',', header = TRUE)\nsapply(dat, class)\n## whoops, there is an 'x', presumably indicating missingness:\nunique(dat[ , 2])\n## let's treat 'x' as a missing value indicator\ndat2 <- read.table(file.path('..', 'data', 'RTADataSub.csv'),\n                   sep = ',', header = TRUE,\n                   na.strings = c(\"NA\", \"x\"))\nunique(dat2[ ,2])\n## hmmm, what happened to the blank values this time?\nwhich(dat[ ,2] == \"\")\ndat2[which(dat[, 2] == \"\")[1], ] # pull out a line with a missing string\n\n# using 'colClasses'\nsequ <- read.table(file.path('..', 'data', 'hivSequ.csv'),\n  sep = ',', header = TRUE,\n  colClasses = c('integer','integer','character',\n    'character','numeric','integer'))\n## let's make sure the coercion worked - sometimes R is obstinant\nsapply(sequ, class)\n## that made use of the fact that a data frame is a list\n\nNote that you can avoid reading in one or more columns by specifying NULL as the column class for those columns to be omitted. Also, specifying the colClasses argument explicitly should make for faster file reading. Finally, setting stringsAsFactors=FALSE is standard practice and is the default in R as of version 4.0. (readr::read_csv() has always set stringsAsFactors=FALSE.\nIf possible, it’s a good idea to look through the input file in the shell or in an editor before reading into R to catch such issues in advance. Using less on RTADataSub.csv would have revealed these various issues, but note that RTADataSub.csv is a 1000-line subset of a much larger file of data available from the kaggle.com website. So more sophisticated use of UNIX utilities as we saw in Unit 2 is often useful before trying to read something into R.\nThe basic function scan() simply reads everything in, ignoring lines, which works well and very quickly if you are reading in a numeric vector or matrix. scan() is also useful if your file is free format - i.e., if it’s not one line per observation, but just all the data one value after another; in this case you can use scan() to read it in and then format the resulting character or numeric vector as a matrix with as many columns as fields in the dataset. Remember that the default is to fill the matrix by column.\nIf the file is not nicely arranged by field (e.g., if it has ragged lines), we’ll need to do some more work. readLines() will read in each line into a separate character vector, after which we can process the lines using text manipulation. Here’s an example from some US meteorological data where I know from metadata (not provided here) that the 4-11th values are an identifier, the 17-20th are the year, the 22-23rd the month, etc.\nActually, that file, precip.txt, is in a fixed-width format (i.e., every element in a given column has the exact same number of characters),so reading in using read.fwf() would be a good strategy.\nR allows you to read in not just from a file but from a more general construct called a connection. Here are some examples of connections:\n\nreadLines(pipe(\"ls -al\"))\n\nIn some cases, you might need to create the connection using url() or using the curl() function from the curl package. Though for the example here, simply passing the URL to readLines() does work. (In general, curl::curl() provides some nice features for reading off the internet.)\nIf a file is large, we may want to read it in in chunks (of lines), do some computations to reduce the size of things, and iterate. read.table(), read.fwf() and readLines() all have the arguments that let you read in a fixed number of lines. To read-on-the-fly in blocks, we need to first establish the connection and then read from it sequentially. (If you don’t, you’ll read from the start of the file every time you read from the file.)"
  }
]