[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistics 243 Fall 2022",
    "section": "",
    "text": "This site is UNDER CONSTRUCTION. Do not rely on materials here before August 22.\nSee the links above for the key resources for the course."
  },
  {
    "objectID": "index.html#questions-about-taking-the-class",
    "href": "index.html#questions-about-taking-the-class",
    "title": "Statistics 243 Fall 2022",
    "section": "Questions about taking the class",
    "text": "Questions about taking the class\nIf you would like to audit the class, enroll as a UC Berkeley undergraduate, or enroll as a concurrent enrollment student (i.e., for visiting students), or for some other reason are not enrolled, please fill out this survey as soon as possible. All those enrolled or wishing to take the class should have filled it out by Friday August 26 at noon.\nUndergraduates can only take the course with my permission and once all graduate students have an opportunity to register. I expect there will be space, and will likely start admitting undergraduates early the week of August 29 or late the previous week.\nConcurrent enrollment students can take the course with my permission and once all UC Berkeley students have had an opportunity to register. I expect there will be space, and will likely start admitting concurrent enrollment students early the week of August 29.\nPlease see the syllabus for the math and statistics background I expect, as well as the need to be familiar with R or to get up to speed in R during the first few weeks of the semester.\nThe first three weeks involve a lot of moving pieces, in part related to trying to get everyone up to speed with the bash shell and R. Please use these notes to keep on top of what you need to do."
  },
  {
    "objectID": "index.html#course-content",
    "href": "index.html#course-content",
    "title": "Statistics 243 Fall 2022",
    "section": "Course content",
    "text": "Course content\n\nThis site: most non-recorded course material\n\nFor html versions of the Units, see the navigation bar above.\nFor PDF versions of the Units, clone the GitHub repository, or if you’re not yet familiar with Git, download a zip file.\n\nVarious SCF tutorials: These include the various tutorials referred to in the class materials (e.g., the UNIX and bash shell tutorials, the dynamic documents tutorial, the Git tutorial, the string processing tutorial, etc.).\nbCourses: links to class course captures and any pre-recorded material.\n\nIf you’re not yet familiar with Git, go to the upper right of this page and click on ‘Clone or download’ and then ‘Download ZIP’."
  },
  {
    "objectID": "units/unit1-unix.html",
    "href": "units/unit1-unix.html",
    "title": "Introduction to UNIX, computers, and key tools",
    "section": "",
    "text": "PDF\nBy UNIX, I mean any UNIX-like operating system, including Linux and Mac OS X. On the Mac you can access a UNIX terminal window with the Terminal application (via Applications -> Utilities -> Terminal from the Finder). Most modern scientific computing is done on UNIX-based machines, often by remotely logging in to a UNIX-based server.\nThere are a variety of ways to get access to a UNIX-style command line environment, outlined in the software tips on the course website."
  },
  {
    "objectID": "units/unit1-unix.html#some-useful-editors",
    "href": "units/unit1-unix.html#some-useful-editors",
    "title": "Introduction to UNIX, computers, and key tools",
    "section": "Some useful editors",
    "text": "Some useful editors\n\nvarious editors available on all operating systems:\n\ntraditional editors born in UNIX: emacs, vim\nsome newer editors: Atom, Sublime Text (Sublime is proprietary/not free)\n\nWindows-specific: WinEdt\nMac-specific: Aquamacs Emacs, TextMate, TextEdit\nRStudio provides a built-in editor for R code and R Markdown files. (Note: RStudio as a whole is an IDE (integrated development environment. The editor is just the editing window where you edit code (and R Markdown) files.)\nVSCode has a powerful code editor that is customized to work with various languages.\n\nAs you get started it’s ok to use a very simple text editor such as Notepad in Windows, but you should take the time in the next few weeks to try out more powerful editors such as one of those listed above. It will be well worth your time over the course of your graduate work and then your career.\nBe careful in Windows - file suffixes are often hidden."
  },
  {
    "objectID": "units/unit1-unix.html#optional-basic-emacs",
    "href": "units/unit1-unix.html#optional-basic-emacs",
    "title": "Introduction to UNIX, computers, and key tools",
    "section": "(Optional) Basic emacs",
    "text": "(Optional) Basic emacs\nEmacs is one option as an editor. I use Emacs a fair amount, so I’m including some tips here, but other editors listed above are just as good.\n\nEmacs has special modes for different types of files: R code files, C code files, Latex files – it’s worth your time to figure out how to set this up on your machine for the kinds of files you often work on\n\nFor working with R, ESS (emacs speaks statistics) mode is helpful. This is built into Aquamacs Emacs.\n\nTo open emacs in the terminal window rather than as a new window, which is handy when it’s too slow (or impossible) to pass (i.e., tunnel) the graphical emacs window through ssh: emacs -nw file.txt\nSome helpful keystroke sequence shorcuts\n\n\n\n\n\n\n\n\nSequence\nResult\n\n\n\n\nC-x,C-c\nClose the file\n\n\nC-x,C-s\nSave the file\n\n\nC-x,C-w\nSave with a new name\n\n\nC-s\nSearch\n\n\nESC\nGet out of command buffer at bottom of screen\n\n\nC-a\nGo to beginning of line\n\n\nC-e\nGo to end of line\n\n\nC-k\nDelete the rest of the line from cursor forward\n\n\nC-space, then move to end of block\nHighlight a block of text\n\n\nC-w\nRemove the highlighted block, putting it in the kill buffer\n\n\nC-y (after using C-k or C-w)\nPaste from kill buffer (‘y’ is for ‘yank’)"
  },
  {
    "objectID": "units/unit2-dataTech.html",
    "href": "units/unit2-dataTech.html",
    "title": "Data technologies, formats, and structures",
    "section": "",
    "text": "PDF\nReferences:"
  },
  {
    "objectID": "units/unit2-dataTech.html#text-and-binary-files",
    "href": "units/unit2-dataTech.html#text-and-binary-files",
    "title": "Data technologies, formats, and structures",
    "section": "Text and binary files",
    "text": "Text and binary files\nIn general, files can be divided into text files and binary files. In both cases, information is stored as a series of bits. Recall that a bit is a single value in base 2 (i.e., a 0 or a 1), while a byte is 8 bits.\nA text file is one in which the bits in the file encode individual characters. Note that the characters can include the digit characters 0-9, so one can include numbers in a text file by writing down the digits needed for the number of interest. Examples of text file formats include CSV, XML, HTML, and JSON.\nText files may be simple ASCII files (i.e., files encoded using ASCII) or in other encodings such as UTF-8, both covered in Section 4. ASCII files have 8 bits (1 byte) per character and can represent 128 characters (the 52 lower and upper case letters in English, 10 digits, punctuation and a few other things – basically what you see on a standard US keyboard). UTF-8 files have between 1 and 4 bytes per character.\nA binary file is one in which the bits in the file encode the information in a custom format and not simply individual characters. Binary formats are not (easily) human readable but can be more space-efficient and faster to work with (because it can allow random access into the data rather than requiring sequential reading). The meaning of the bytes in such files depends on the specific binary format being used and a program that uses the file needs to know how the format represents information. Examples of binary files include netCDF files, R data (e.g., .Rda) files, and compiled code files.\nNumbers in binary files are usually stored as 8 bytes per number. We’ll discuss this much more in Unit 6."
  },
  {
    "objectID": "units/unit2-dataTech.html#common-file-types",
    "href": "units/unit2-dataTech.html#common-file-types",
    "title": "Data technologies, formats, and structures",
    "section": "Common file types",
    "text": "Common file types\nHere are some of the common file types. Any of these types can be categorized as text or binary.\n\n‘Flat’ text files: data are often provided as simple text files. Often one has one record or observation per row and each column or field is a different variable or type of information about the record. Such files can either have a fixed number of characters in each field (fixed width format) or a special character (a delimiter) that separates the fields in each row. Common delimiters are tabs, commas, one or more spaces, and the pipe (|). Common file extensions are .txt and .csv. Metadata (information about the data) are often stored in a separate file. CSV files are quite common, but if you have files where the data contain commas, other delimiters can be good. Text can be put in quotes in CSV files, and this can allow use of commas within the data. This is difficult to deal with in bash, but read.table() in R handles this situation.\n\nOne occasionally tricky difficulty is as follows. If you have a text file created in Windows, the line endings are coded differently than in UNIX (a newline (the ASCII character \\n) and a carriage return (the ASCII character \\r) in Windows vs. only a newline in UNIX). There are UNIX utilities (fromdos in Ubuntu, including the SCF Linux machines and dos2unix in other Linux distributions) that can do the necessary conversion. If you see ^M at the end of the lines in a file, that’s the tool you need. Alternatively, if you open a UNIX file in Windows, it may treat all the lines as a single line. You can fix this with todos or unix2dos.\n\nIn some contexts, such as textual data and bioinformatics data, the data may in a text file with one piece of information per row, but without meaningful columns/fields.\nIn scientific contexts, netCDF (.nc) (and the related HDF5) are popular format for gridded data that allows for highly-efficient storage and contains the metadata within the file. The basic structure of a netCDF file is that each variable is an array with multiple dimensions (e.g., latitude, longitude, and time), and one can also extract the values of and metadata about each dimension. The ncdf4 package in R nicely handles working with netCDF files.\nData may also be in text files in formats designed for data interchange between various languages, in particular XML or JSON. These formats are “self-describing”; namely the metadata is part of the file. The XML2, rvest, and jsonlite packages are useful for reading and writing from these formats.\nYou may be scraping information on the web, so dealing with text files in various formats, including HTML. The XML2 and rvest packages are also useful for reading HTML.\nData may already be in a database or in the data storage of another statistical package (Stata, SAS, SPSS, etc.). The foreign package in R has excellent capabilities for importing Stata (read.dta()), SPSS (read.spss()), and SAS (read.ssd() and, for XPORT files, read.xport()), among others.\nFor Excel, there are capabilities to read an Excel file (see the readxl and XLConnect package among others), but you can also just go into Excel and export as a CSV file or the like and then read that into R. In general, it’s best not to pass around data files as Excel or other spreadsheet format files because (1) Excel is proprietary, so someone may not have Excel and the format is subject to change, (2) Excel imposes limits on the number of rows, (3) one can easily manipulate text files such as CSV using UNIX tools, but this is not possible with an Excel file, (4) Excel files often have more than one sheet, graphs, macros, etc., so they’re not a data storage format per se.\nR can easily interact with databases (SQLite, PostgreSQL, MySQL, Oracle, etc.), querying the database using SQL and returning results to R. More in the big data unit and in the large datasets tutorial mentioned above."
  },
  {
    "objectID": "units/unit2-dataTech.html#core-r-functions",
    "href": "units/unit2-dataTech.html#core-r-functions",
    "title": "Data technologies, formats, and structures",
    "section": "Core R functions",
    "text": "Core R functions\nread.table() is probably the most commonly-used function for reading in data. It reads in delimited files (read.csv() and read.delim() are special cases of read.table()). The key arguments are the delimiter (the sep argument) and whether the file contains a header, a line with the variable names. We can use read.fwf() to read from a fixed width text file into a data frame.\nThe most difficult part of reading in such files can be dealing with how R determines the classes of the fields that are read in. There are a number of arguments to read.table() and read.fwf() that allow the user to control the classes. One difficulty in older versions of R was that character fields were read in as factors.\nLet’s work through a couple examples. Before we do that, let’s look at the arguments to read.table(). Note that sep=“ separates on any amount of white space. In the code chunk below, I’ve told knitr not to print the output to the PDF; you can see the full output by running the code yourself.\n\ndat <- read.table(file.path('..', 'data', 'RTADataSub.csv'),\n                  sep = ',', header = TRUE)\nsapply(dat, class)\n## whoops, there is an 'x', presumably indicating missingness:\nunique(dat[ , 2])\n## let's treat 'x' as a missing value indicator\ndat2 <- read.table(file.path('..', 'data', 'RTADataSub.csv'),\n                   sep = ',', header = TRUE,\n                   na.strings = c(\"NA\", \"x\"))\nunique(dat2[ ,2])\n## hmmm, what happened to the blank values this time?\nwhich(dat[ ,2] == \"\")\ndat2[which(dat[, 2] == \"\")[1], ] # pull out a line with a missing string\n\n# using 'colClasses'\nsequ <- read.table(file.path('..', 'data', 'hivSequ.csv'),\n  sep = ',', header = TRUE,\n  colClasses = c('integer','integer','character',\n    'character','numeric','integer'))\n## let's make sure the coercion worked - sometimes R is obstinant\nsapply(sequ, class)\n## that made use of the fact that a data frame is a list\n\nNote that you can avoid reading in one or more columns by specifying NULL as the column class for those columns to be omitted. Also, specifying the colClasses argument explicitly should make for faster file reading. Finally, setting stringsAsFactors=FALSE is standard practice and is the default in R as of version 4.0. (readr::read_csv() has always set stringsAsFactors=FALSE.\nIf possible, it’s a good idea to look through the input file in the shell or in an editor before reading into R to catch such issues in advance. Using less on RTADataSub.csv would have revealed these various issues, but note that RTADataSub.csv is a 1000-line subset of a much larger file of data available from the kaggle.com website. So more sophisticated use of UNIX utilities as we saw in Unit 2 is often useful before trying to read something into R.\nThe basic function scan() simply reads everything in, ignoring lines, which works well and very quickly if you are reading in a numeric vector or matrix. scan() is also useful if your file is free format - i.e., if it’s not one line per observation, but just all the data one value after another; in this case you can use scan() to read it in and then format the resulting character or numeric vector as a matrix with as many columns as fields in the dataset. Remember that the default is to fill the matrix by column.\nIf the file is not nicely arranged by field (e.g., if it has ragged lines), we’ll need to do some more work. readLines() will read in each line into a separate character vector, after which we can process the lines using text manipulation. Here’s an example from some US meteorological data where I know from metadata (not provided here) that the 4-11th values are an identifier, the 17-20th are the year, the 22-23rd the month, etc.\n\ndat <- readLines(file.path('..', 'data', 'precip.txt'))\nid <- as.factor(substring(dat, 4, 11) )\nyear <- substring(dat, 18, 21)\nyear[1:5]\n\n[1] \"2010\" \"2010\" \"2010\" \"2010\" \"2010\"\n\nclass(year)\n\n[1] \"character\"\n\nyear <- as.integer(substring(dat, 18, 21))\nmonth <- as.integer(substring(dat, 22, 23))\nnvalues <- as.integer(substring(dat, 28, 30))\n\nActually, that file, precip.txt, is in a fixed-width format (i.e., every element in a given column has the exact same number of characters),so reading in using read.fwf() would be a good strategy.\nR allows you to read in not just from a file but from a more general construct called a connection. Here are some examples of connections:\n\ndat <- readLines(pipe(\"ls -al\"))\ndat <- read.table(pipe(\"unzip dat.zip\"))\ndat <- read.csv(gzfile(\"dat.csv.gz\"))\ndat <- readLines(\"http://www.stat.berkeley.edu/~paciorek/index.html\")\n\nIn some cases, you might need to create the connection using url() or using the curl() function from the curl package. Though for the example here, simply passing the URL to readLines() does work. (In general, curl::curl() provides some nice features for reading off the internet.)\n\nwikip1 <- readLines(\"https://wikipedia.org\")\nwikip2 <- readLines(url(\"https://wikipedia.org\"))\nlibrary(curl)\n\nUsing libcurl 7.68.0 with GnuTLS/3.6.13\n\nwikip3 <- readLines(curl(\"https://wikipedia.org\"))\n\nIf a file is large, we may want to read it in in chunks (of lines), do some computations to reduce the size of things, and iterate. read.table(), read.fwf() and readLines() all have the arguments that let you read in a fixed number of lines. To read-on-the-fly in blocks, we need to first establish the connection and then read from it sequentially. (If you don’t, you’ll read from the start of the file every time you read from the file.)\n\ncon <- file(file.path(\"..\", \"data\", \"precip.txt\"), \"r\")\n## \"r\" for 'read' - you can also open files for writing with \"w\"\n## (or \"a\" for appending)\nclass(con)\nblockSize <- 1000 # obviously this would be large in any real application\nnLines <- 300000\nfor(i in 1:ceiling(nLines / blockSize)){\n    lines <- readLines(con, n = blockSize)\n    # manipulate the lines and store the key stuff\n}\nclose(con)\n\n \nHere’s an example of using curl() to do this for a file on the web.\n\nURL <- \"https://www.stat.berkeley.edu/share/paciorek/2008.csv.gz\"\ncon <- gzcon(curl(URL, open = \"r\"))\n## url() in place of curl() works too\nfor(i in 1:8) {\n    print(i)\n    print(system.time(tmp <- readLines(con, n = 100000)))\n    print(tmp[1])\n}\n\n[1] 1\n   user  system elapsed \n  0.436   0.002   0.440 \n[1] \"Year,Month,DayofMonth,DayOfWeek,DepTime,CRSDepTime,ArrTime,CRSArrTime,UniqueCarrier,FlightNum,TailNum,ActualElapsedTime,CRSElapsedTime,AirTime,ArrDelay,DepDelay,Origin,Dest,Distance,TaxiIn,TaxiOut,Cancelled,CancellationCode,Diverted,CarrierDelay,WeatherDelay,NASDelay,SecurityDelay,LateAircraftDelay\"\n[1] 2\n   user  system elapsed \n  0.488   0.006   0.495 \n[1] \"2008,1,29,2,1938,1935,2308,2257,XE,7676,N11176,150,142,104,11,3,SLC,OKC,866,5,41,0,,0,NA,NA,NA,NA,NA\"\n[1] 3\n   user  system elapsed \n  0.424   0.000   0.424 \n[1] \"2008,1,20,7,1540,1525,1651,1637,OO,5703,N227SW,71,72,58,14,15,SBA,SJC,234,5,8,0,,0,NA,NA,NA,NA,NA\"\n[1] 4\n   user  system elapsed \n  0.577   0.000   0.578 \n[1] \"2008,1,2,3,1313,1250,1443,1425,WN,440,N461WN,150,155,138,18,23,MCO,STL,880,3,9,0,,0,2,0,0,0,16\"\n[1] 5\n   user  system elapsed \n  0.463   0.002   0.465 \n[1] \"2008,1,24,4,1026,1015,1116,1110,MQ,3926,N653AE,50,55,38,6,11,MLI,ORD,139,6,6,0,,0,NA,NA,NA,NA,NA\"\n[1] 6\n   user  system elapsed \n  0.448   0.000   0.449 \n[1] \"2008,1,4,5,1129,1125,1352,1350,AA,1145,N438AA,203,205,187,2,4,ORD,SLC,1249,3,13,0,,0,NA,NA,NA,NA,NA\"\n[1] 7\n   user  system elapsed \n  0.501   0.011   0.513 \n[1] \"2008,1,10,4,716,720,1025,1024,DL,1590,N991DL,129,124,107,1,-4,AUS,ATL,813,6,16,0,,0,NA,NA,NA,NA,NA\"\n[1] 8\n   user  system elapsed \n  0.433   0.000   0.434 \n[1] \"2008,2,15,5,2127,2132,2254,2312,XE,7663,N33182,87,100,71,-18,-5,SLC,ABQ,493,6,10,0,,0,NA,NA,NA,NA,NA\"\n\nclose(con)\n\nMore details on sequential (on-line) processing of large files can be found in the tutorial on large datasets mentioned in the reference list above.\nOne cool trick that can come in handy is to create a text connection. This lets you ‘read’ from an R character vector as if it were a text file and could be handy for processing text. For example, you could then use read.fwf() applied to con.\n\ndat <- readLines('../data/precip.txt')\ncon <- textConnection(dat[1], \"r\")\nread.fwf(con, c(3,8,4,2,4,2))\n\n   V1      V2   V3 V4   V5 V6\n1 DLY 1000807 PRCP HI 2010  2\n\n\nWe can create connections for writing output too. Just make sure to open the connection first."
  },
  {
    "objectID": "units/unit2-dataTech.html#file-paths",
    "href": "units/unit2-dataTech.html#file-paths",
    "title": "Data technologies, formats, and structures",
    "section": "File paths",
    "text": "File paths\nA few notes on file paths, related to ideas of reproducibility.\n\nIn general, you don’t want to hard-code absolute paths into your code files because those absolute paths won’t be available on the machines of anyone you share the code with. Instead, use paths relative to the directory the code file is in, or relative to a baseline directory for the project, e.g.:\n\n\ndat <- read.csv('../data/cpds.csv')\n\nBe careful with the directory separator in Windows files: you can either do “C:\\\\mydir\\\\file.txt” or “C:/mydir/file.txt”, but not “C:\\mydir\\file.txt”, and note the next comment about avoiding use of ‘\\\\’ for portability.\nUsing UNIX style directory separators will work in Windows, Mac or Linux, but using Windows style separators is not portable across operating systems.\n\n\n## good: will work on Windows\ndat <- read.csv('../data/cpds.csv')\n## bad: won't work on Mac or Linux\ndat <- read.csv('..\\\\data\\\\cpds.csv')  \n\n \nEven better, use file.path() so that paths are constructed specifically for the operating system the user is using:\n\n\n## good: operating-system independent\ndat <- read.csv(file.path('..', 'data', 'cpds.csv'))"
  },
  {
    "objectID": "units/unit2-dataTech.html#the-readr-package",
    "href": "units/unit2-dataTech.html#the-readr-package",
    "title": "Data technologies, formats, and structures",
    "section": "The readr package",
    "text": "The readr package\nreadr is intended to deal with some of the shortcomings of the base R functions, such as defaulting to stringsAsFactors=FALSE (no longer relevant with R 4.0), leaving column names unmodified, and recognizing dates/times. It reads data in much more quickly than the base R equivalents. See this blog post. Some of the readr functions that are analogs to the comparably-named base R functions are read_csv(), read_fwf(), read_lines(), and read_table().\nLet’s try out read_csv() on the airline dataset used in the R bootcamp.\n\nlibrary(readr)\n\n\nAttaching package: 'readr'\n\n\nThe following object is masked from 'package:curl':\n\n    parse_date\n\n## I'm violating the rule about absolute paths here!!\n## (airline.csv is big enough that I don't want to put it in the\n##    course repository)\ndir <- \"~/staff/workshops/r-bootcamp-fall-2020/data\"\nsystem.time(dat <- read.csv(file.path(dir, 'airline.csv'), stringsAsFactors = FALSE)) \n\n   user  system elapsed \n  3.162   0.082   3.252 \n\nsystem.time(dat2 <- read_csv(file.path(dir, 'airline.csv')))\n\nRows: 539895 Columns: 29\n\n\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (5): UniqueCarrier, TailNum, Origin, Dest, CancellationCode\ndbl (24): Year, Month, DayOfMonth, DayOfWeek, DepTime, CRSDepTime, ArrTime, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n   user  system elapsed \n  3.987   0.140   1.514"
  },
  {
    "objectID": "units/unit2-dataTech.html#reading-data-quickly",
    "href": "units/unit2-dataTech.html#reading-data-quickly",
    "title": "Data technologies, formats, and structures",
    "section": "Reading data quickly",
    "text": "Reading data quickly\nIn addition to the tips above, there are a number of packages that allow one to read large data files quickly, in particular data.table, ff, and bigmemory. In general, these provide the ability to load datasets into R without having them in memory, but rather stored in clever ways on disk that allow for fast access. Metadata is stored in R. More on this in the unit on big data and in the tutorial on large datasets mentioned in the reference list above."
  },
  {
    "objectID": "units/unit2-dataTech.html#writing-output-to-files",
    "href": "units/unit2-dataTech.html#writing-output-to-files",
    "title": "Data technologies, formats, and structures",
    "section": "Writing output to files",
    "text": "Writing output to files\nFunctions for text output are generally analogous to those for input. write.table(), write.csv(), and writeLines() are analogs of read.table(), read.csv(), and readLines(). write_csv() is the readr version of write.csv. write() can be used to write a matrix to a file, specifying the number of columns desired. cat() can be used when you want fine control of the format of what is written out and allows for outputting to a connection (e.g., a file).\ntoJSON() in the jsonlite package will output R objects as JSON. One use of JSON as output from R would be to serialize the information in an R object such that it could be read into another program.\nAnd of course you can always save to an R data file using save.image() (to save all the objects in the workspace or save() to save only some objects. Happily this is platform-independent so can be used to transfer R objects between different OS."
  },
  {
    "objectID": "units/unit2-dataTech.html#formatting-output",
    "href": "units/unit2-dataTech.html#formatting-output",
    "title": "Data technologies, formats, and structures",
    "section": "Formatting output",
    "text": "Formatting output\ncat() is a good choice for printing a message to the screen, often better than print(), which is an object-oriented method. You generally won’t have control over how the output of a print() statement is actually printed.\n\nval <- 1.5\ncat('My value is ', val, '.\\n', sep = '')\n\nMy value is 1.5.\n\nprint(paste('My value is ', val, '.', sep = ''))\n\n[1] \"My value is 1.5.\"\n\n\nWe can do more to control formatting with cat():\n\n## input\nx <- 7\nn <- 5\n## display powers\ncat(\"Powers of\", x, \"\\n\")\n\nPowers of 7 \n\ncat(\"exponent   result\\n\\n\")\n\nexponent   result\n\nresult <- 1\nfor (i in 1:n) {\n    result <- result * x\n    cat(format(i, width = 8), format(result, width = 10),\n            \"\\n\", sep = \"\")\n}\n\n       1         7\n       2        49\n       3       343\n       4      2401\n       5     16807\n\nx <- 7\nn <- 5\n## display powers\ncat(\"Powers of\", x, \"\\n\")\n\nPowers of 7 \n\ncat(\"exponent result\\n\\n\")\n\nexponent result\n\nresult <- 1\nfor (i in 1:n) {\n    result <- result * x\n    cat(i, '\\t', result, '\\n', sep = '')\n}\n\n1   7\n2   49\n3   343\n4   2401\n5   16807\n\n\nOne thing to be aware of when writing out numerical data is how many digits are included. For example, the default with write() and **** cat() is the number of digits that R displays to the screen, controlled by options()$digits. But note that options()$digits seems to have some variability in behavior across operating systems. If you want finer control, use sprintf(), e.g., to print out print out temperatures as reals (“f”=floating points) with four decimal places and nine total character positions, followed by a C for Celsius:\n\ntemps <- c(12.5, 37.234324, 1342434324.79997234, 2.3456e-6, 1e10)\nsprintf(\"%9.4f C\", temps)\n\n[1] \"  12.5000 C\"        \"  37.2343 C\"        \"1342434324.8000 C\" \n[4] \"   0.0000 C\"        \"10000000000.0000 C\"\n\ncity <- \"Boston\"\nsprintf(\"The temperature in %s was %.4f C.\", city, temps[1])\n\n[1] \"The temperature in Boston was 12.5000 C.\"\n\nsprintf(\"The temperature in %s was %9.4f C.\", city, temps[1])\n\n[1] \"The temperature in Boston was   12.5000 C.\"\n\n\nNote, to change the number of digits printed to the screen, do``options(digits = 5) or specify as an argument to print() or use sprintf()."
  },
  {
    "objectID": "units/unit2-dataTech.html#reading-html",
    "href": "units/unit2-dataTech.html#reading-html",
    "title": "Data technologies, formats, and structures",
    "section": "Reading HTML",
    "text": "Reading HTML\nHTML (Hypertext Markup Language) is the standard markup language used for displaying content in a web browser. In simple webpages (ignoring the more complicated pages that involve Javascript), what you see in your browser is simply a rendering of a text file containing HTML.\nHowever, instead of rendering the HTML in a browser, we might want to use code to extract information from the HTML.\nLet’s see a brief example of reading in HTML tables.\nNote that before doing any coding, it can be helpful to look at the raw HTML source code for a given page. We can explore the underlying HTML source in advance of writing our code by looking at the page source directly in the browser (e.g., in Firefox under the 3-lines “open menu” symbol, see Web Developer (or More Tools) -> Page Source and in Chrome View -> Developer -> View Source), or by downloading the webpage and looking at it in an editor, although in some cases (such as the nytimes.com case), what we might see is a lot of JavaScript.\nOne lesson here is not to write a lot of your own code to do something that someone else has probably already written a package for. We’ll use the rvest package.\n\nlibrary(rvest)  # uses xml2\n\n\nAttaching package: 'rvest'\n\n\nThe following object is masked from 'package:readr':\n\n    guess_encoding\n\nURL <- \"https://en.wikipedia.org/wiki/List_of_countries_and_dependencies_by_population\"\nhtml <- read_html(URL)\ntbls <- html_table(html_elements(html, \"table\"))\nsapply(tbls, nrow)\n\n[1] 242  12\n\npop <- tbls[[1]]\nhead(pop)\n\n# A tibble: 6 × 9\n  Rank  `Country / Dependency` Conti…¹ Popul…² Perce…³ Date  Sourc…⁴ Notes ``   \n  <chr> <chr>                  <chr>   <chr>   <chr>   <chr> <chr>   <chr> <lgl>\n1 –     World                  All     7,972,… 100%    19 A… UN pro… \"\"    NA   \n2 1     China                  Asia    1,412,… 17.7%   31 D… Offici… \"The… NA   \n3 2     India                  Asia    1,380,… 17.3%   31 D… Offici… \"The… NA   \n4 3     United States          North … 333,00… 4.18%   19 A… Nation… \"The… NA   \n5 4     Indonesia              Asia[b] 272,24… 3.42%   1 Ju… Offici… \"\"    NA   \n6 5     Pakistan               Asia    235,82… 2.96%   1 Ju… UN pro… \"The… NA   \n# … with abbreviated variable names ¹​Continent, ²​Population,\n#   ³​`Percentage of the world`, ⁴​`Source (official or from the United Nations)`\n\n\nread_html() works by reading in the HTML as text and then parsing it to build up a tree containing the HTML elements. Then html_nodes() finds the HTML tables and html_table() converts them to data frames. rvest is part of the tidyverse, so it’s often used with piping, e.g.,\n\nlibrary(magrittr)\n## Turns out that html_table can take the entire html doc as input\ntbls <- URL %>% read_html() %>% html_table()\n\nIt’s often useful to be able to extract the hyperlinks in an HTML document. We’ll find the link using CSS selectors, which allow you to search for elements within HTML:\n\nURL <- \"http://www1.ncdc.noaa.gov/pub/data/ghcn/daily/by_year\"\n## approach 1: search for elements with 'href' attribute\nlinks <- read_html(URL) %>% html_elements(\"[href]\") %>% html_attr('href')\n## approach 2: search for HTML 'a' tags\nlinks <- read_html(URL) %>% html_elements(\"a\") %>% html_attr('href')\nhead(links, n = 10)\n\n [1] \"?C=N;O=D\"              \"?C=M;O=A\"              \"?C=S;O=A\"             \n [4] \"?C=D;O=A\"              \"/pub/data/ghcn/daily/\" \"1750.csv.gz\"          \n [7] \"1763.csv.gz\"           \"1764.csv.gz\"           \"1765.csv.gz\"          \n[10] \"1766.csv.gz\"          \n\n\nMore generally, we may want to read an HTML document, parse it into its components (i.e., the HTML elements), and navigate through the tree structure of the HTML. Here we use the XPath language to specify elements rather than CSS selectors. XPath can also be used for navigating through XML documents.\n\n## find all 'a' elements that have attribute 'href'; then\n## extract the 'href' attribute\nlinks <- read_html(URL) %>% html_elements(xpath = \"//a[@href]\") %>%\n    html_attr('href')\nhead(links)\n\n[1] \"?C=N;O=D\"              \"?C=M;O=A\"              \"?C=S;O=A\"             \n[4] \"?C=D;O=A\"              \"/pub/data/ghcn/daily/\" \"1750.csv.gz\"          \n\n## we can extract various information\nlistOfANodes <- read_html(URL) %>% html_elements(xpath = \"//a[@href]\")\nlistOfANodes %>% html_attr('href') %>% head(n = 10)\n\n [1] \"?C=N;O=D\"              \"?C=M;O=A\"              \"?C=S;O=A\"             \n [4] \"?C=D;O=A\"              \"/pub/data/ghcn/daily/\" \"1750.csv.gz\"          \n [7] \"1763.csv.gz\"           \"1764.csv.gz\"           \"1765.csv.gz\"          \n[10] \"1766.csv.gz\"          \n\nlistOfANodes %>% html_name() %>% head(n = 10)\n\n [1] \"a\" \"a\" \"a\" \"a\" \"a\" \"a\" \"a\" \"a\" \"a\" \"a\"\n\nlistOfANodes %>% html_text()  %>% head(n = 10)\n\n [1] \"Name\"             \"Last modified\"    \"Size\"             \"Description\"     \n [5] \"Parent Directory\" \"1750.csv.gz\"      \"1763.csv.gz\"      \"1764.csv.gz\"     \n [9] \"1765.csv.gz\"      \"1766.csv.gz\"     \n\n\nHere’s another example of extracting specific components of information from a webpage (results not shown, since headlines will vary from day to day).\n\nURL <- \"https://www.nytimes.com\"\nheadlines2 <- read_html(URL) %>% html_elements(\"h2\") %>% html_text()\nhead(headlines2)\nheadlines3 <- read_html(URL) %>% html_elements(\"h3\") %>% html_text()\nhead(headlines3)"
  },
  {
    "objectID": "units/unit2-dataTech.html#xml",
    "href": "units/unit2-dataTech.html#xml",
    "title": "Data technologies, formats, and structures",
    "section": "XML",
    "text": "XML\nXML is a markup language used to store data in self-describing (no metadata needed) format, often with a hierarchical structure. It consists of sets of elements (also known as nodes because they generally occur in a hierarchical structure and therefore have parents, children, etc.) with tags that identify/name the elements, with some similarity to HTML. Some examples of the use of XML include serving as the underlying format for Microsoft Office and Google Docs documents and for the KML language used for spatial information in Google Earth.\nHere’s a brief example. The book with id attribute bk101 is an element; the author of the book is also an element that is a child element of the book. The id attribute allows us to uniquely identify the element.\n    <?xml version=\"1.0\"?>\n    <catalog>\n       <book id=\"bk101\">\n          <author>Gambardella, Matthew</author>\n          <title>XML Developer's Guide</title>\n          <genre>Computer</genre>\n          <price>44.95</price>\n          <publish_date>2000-10-01</publish_date>\n          <description>An in-depth look at creating applications with XML.</description>\n       </book>\n       <book id=\"bk102\">\n          <author>Ralls, Kim</author>\n          <title>Midnight Rain</title>\n          <genre>Fantasy</genre>\n          <price>5.95</price>\n          <publish_date>2000-12-16</publish_date>\n         <description>A former architect battles corporate zombies, an evil sorceress, and her own childhood to become queen of the world.</description>\n       </book>\n    </catalog>\nWe can read XML documents into R using xml2::read_xml() and then manipulate it using other functions from the xml2 package. Here’s an example of working with lending data from the Kiva lending non-profit. You can see the XML format in a browser at\nhttp://api.kivaws.org/v1/loans/newest.xml.\nXML documents have a tree structure with information at nodes. As above with HTML, one can use the XPath language for navigating the tree and finding and extracting information from the node(s) of interest. Here is some example code for extracting loan info from the Kiva data.\n\nlibrary(xml2)\ndoc <- read_xml(\"https://api.kivaws.org/v1/loans/newest.xml\")\ndata <- as_list(doc)\nnames(data)\n\n[1] \"response\"\n\nnames(data$response)\n\n[1] \"paging\" \"loans\" \n\nlength(data$response$loans)\n\n[1] 20\n\ndata$response$loans[[2]][c('name', 'activity',\n                           'sector', 'location', 'loan_amount')]\n\n$name\n$name[[1]]\n[1] \"Alelie Grace\"\n\n\n$activity\n$activity[[1]]\n[1] \"Fishing\"\n\n\n$sector\n$sector[[1]]\n[1] \"Food\"\n\n\n$location\n$location$country_code\n$location$country_code[[1]]\n[1] \"PH\"\n\n\n$location$country\n$location$country[[1]]\n[1] \"Philippines\"\n\n\n$location$town\n$location$town[[1]]\n[1] \"Roxas Palawan\"\n\n\n$location$geo\n$location$geo$level\n$location$geo$level[[1]]\n[1] \"town\"\n\n\n$location$geo$pairs\n$location$geo$pairs[[1]]\n[1] \"10.306908 119.256946\"\n\n\n$location$geo$type\n$location$geo$type[[1]]\n[1] \"point\"\n\n\n\n\n$loan_amount\n$loan_amount[[1]]\n[1] \"550\"\n\n## alternatively, extract only the 'loans' info (and use pipes)\nloansNode <- doc %>% html_elements('loans')\nloanInfo <- loansNode %>% xml_children() %>% as_list()\nlength(loanInfo)\n\n[1] 20\n\nnames(loanInfo[[1]])\n\n [1] \"id\"                       \"name\"                    \n [3] \"description\"              \"status\"                  \n [5] \"funded_amount\"            \"basket_amount\"           \n [7] \"image\"                    \"activity\"                \n [9] \"sector\"                   \"themes\"                  \n[11] \"use\"                      \"location\"                \n[13] \"partner_id\"               \"posted_date\"             \n[15] \"planned_expiration_date\"  \"loan_amount\"             \n[17] \"borrower_count\"           \"lender_count\"            \n[19] \"bonus_credit_eligibility\" \"tags\"                    \n\nnames(loanInfo[[1]]$location)\n\n[1] \"country_code\" \"country\"      \"town\"         \"geo\"         \n\n## suppose we only want the country locations of the loans (using XPath)\nxml_find_all(loansNode, '//location//country')\n\n{xml_nodeset (20)}\n [1] <country>Philippines</country>\n [2] <country>Philippines</country>\n [3] <country>Philippines</country>\n [4] <country>Philippines</country>\n [5] <country>Philippines</country>\n [6] <country>Philippines</country>\n [7] <country>Philippines</country>\n [8] <country>Philippines</country>\n [9] <country>Philippines</country>\n[10] <country>Philippines</country>\n[11] <country>Philippines</country>\n[12] <country>Philippines</country>\n[13] <country>Philippines</country>\n[14] <country>Philippines</country>\n[15] <country>Philippines</country>\n[16] <country>Philippines</country>\n[17] <country>Philippines</country>\n[18] <country>Philippines</country>\n[19] <country>Philippines</country>\n[20] <country>Philippines</country>\n\nxml_find_all(loansNode, '//location//country') %>% xml_text()\n\n [1] \"Philippines\" \"Philippines\" \"Philippines\" \"Philippines\" \"Philippines\"\n [6] \"Philippines\" \"Philippines\" \"Philippines\" \"Philippines\" \"Philippines\"\n[11] \"Philippines\" \"Philippines\" \"Philippines\" \"Philippines\" \"Philippines\"\n[16] \"Philippines\" \"Philippines\" \"Philippines\" \"Philippines\" \"Philippines\"\n\n## or extract the geographic coordinates\nxml_find_all(loansNode, '//location//geo/pairs')\n\n{xml_nodeset (20)}\n [1] <pairs>10.752818 123.087562</pairs>\n [2] <pairs>10.306908 119.256946</pairs>\n [3] <pairs>9.783662 123.505602</pairs>\n [4] <pairs>9.719789 122.492009</pairs>\n [5] <pairs>10.752818 123.087562</pairs>\n [6] <pairs>9.719789 122.492009</pairs>\n [7] <pairs>10.878345 123.408176</pairs>\n [8] <pairs>9.719789 122.492009</pairs>\n [9] <pairs>10.878345 123.408176</pairs>\n[10] <pairs>10.752818 123.087562</pairs>\n[11] <pairs>9.783662 123.505602</pairs>\n[12] <pairs>11.254339 124.961687</pairs>\n[13] <pairs>9.967216 118.78551</pairs>\n[14] <pairs>10.878345 123.408176</pairs>\n[15] <pairs>11.254339 124.961687</pairs>\n[16] <pairs>9.783662 123.505602</pairs>\n[17] <pairs>9.942986 122.58252</pairs>\n[18] <pairs>11.254339 124.961687</pairs>\n[19] <pairs>11.254339 124.961687</pairs>\n[20] <pairs>10.752818 123.087562</pairs>"
  },
  {
    "objectID": "units/unit2-dataTech.html#json",
    "href": "units/unit2-dataTech.html#json",
    "title": "Data technologies, formats, and structures",
    "section": "JSON",
    "text": "JSON\nJSON files are structured as “attribute-value” pairs (aka “key-value” pairs), often with a hierarchical structure. Here’s a brief example:\n    {\n      \"firstName\": \"John\",\n      \"lastName\": \"Smith\",\n      \"isAlive\": true,\n      \"age\": 25,\n      \"address\": {\n        \"streetAddress\": \"21 2nd Street\",\n        \"city\": \"New York\",\n        \"state\": \"NY\",\n        \"postalCode\": \"10021-3100\"\n      },\n      \"phoneNumbers\": [\n        {\n          \"type\": \"home\",\n          \"number\": \"212 555-1234\"\n        },\n        {\n          \"type\": \"office\",\n          \"number\": \"646 555-4567\"\n        }\n      ],\n      \"children\": [],\n      \"spouse\": null\n    }\nA set of key-value pairs is a named array and is placed inside braces (squiggly brackets). Note the nestedness of arrays within arrays (e.g., address within the overarching person array and the use of square brackets for unnamed arrays (i.e., vectors of information), as well as the use of different types: character strings, numbers, null, and (not shown) boolean/logical values. JSON and XML can be used in similar ways, but JSON is less verbose than XML.\nWe can read JSON into R using fromJSON() in the jsonlite package. Let’s play again with the Kiva data. The same data that we had worked with in XML format is also available in JSON format: http://api.kivaws.org/v1/loans/newest.json.\n\nlibrary(jsonlite)\ndata <- fromJSON(\"http://api.kivaws.org/v1/loans/newest.json\")\nclass(data)\n\n[1] \"list\"\n\nnames(data)\n\n[1] \"paging\" \"loans\" \n\nclass(data$loans) # nice!\n\n[1] \"data.frame\"\n\nhead(data$loans)\n\n       id         name languages      status funded_amount basket_amount\n1 2417761     Teresita        en fundraising             0             0\n2 2417758 Alelie Grace        en fundraising             0             0\n3 2416514       Margie        en fundraising             0             0\n4 2416515    Genevieve        en fundraising             0             0\n5 2417740        Mercy        en fundraising             0             0\n6 2417757         Erma        en fundraising             0             0\n  image.id image.template_id            activity sector\n1  4901993                 1       General Store Retail\n2  4901994                 1             Fishing   Food\n3  4900333                 1 Fruits & Vegetables   Food\n4  4900336                 1       General Store Retail\n5  4901966                 1                Food   Food\n6  4901995                 1       General Store Retail\n                                        themes\n1 Underfunded Areas, Start-Up, Rural Exclusion\n2 Underfunded Areas, Start-Up, Rural Exclusion\n3 Underfunded Areas, Start-Up, Rural Exclusion\n4 Underfunded Areas, Start-Up, Rural Exclusion\n5 Underfunded Areas, Start-Up, Rural Exclusion\n6 Underfunded Areas, Start-Up, Rural Exclusion\n                                                                                         use\n1 to buy items like canned goods, personal care products, etc. to sell in her general store.\n2                  to buy fishing materials, fishing nets, tackle and gasoline for her boat.\n3                                               to buy vegetables to sell in her food stall.\n4                         to buy items to sell like canned goods and personal care products.\n5                                   to purchase additional packs of frozen products to sell.\n6                    to purchase items like canned goods and personal care products to sell.\n  location.country_code location.country                   location.town\n1                    PH      Philippines        Silay, Negros Occidental\n2                    PH      Philippines                   Roxas Palawan\n3                    PH      Philippines                 Dalaguete, Cebu\n4                    PH      Philippines Sipalay City, Negros Occidental\n5                    PH      Philippines        Silay, Negros Occidental\n6                    PH      Philippines Sipalay City, Negros Occidental\n  location.geo.level   location.geo.pairs location.geo.type partner_id\n1               town 10.752818 123.087562             point        145\n2               town 10.306908 119.256946             point        145\n3               town  9.783662 123.505602             point        145\n4               town  9.719789 122.492009             point        145\n5               town 10.752818 123.087562             point        145\n6               town  9.719789 122.492009             point        145\n           posted_date planned_expiration_date loan_amount borrower_count\n1 2022-08-19T23:30:17Z    2022-09-23T23:30:17Z         225              1\n2 2022-08-19T23:30:13Z    2022-09-23T23:30:13Z         550              1\n3 2022-08-19T23:30:11Z    2022-09-23T23:30:11Z         275              1\n4 2022-08-19T23:30:11Z    2022-09-23T23:30:11Z         550              1\n5 2022-08-19T23:30:11Z    2022-09-23T23:30:11Z         200              1\n6 2022-08-19T23:30:11Z    2022-09-23T23:30:11Z         375              1\n  lender_count bonus_credit_eligibility\n1            0                     TRUE\n2            0                     TRUE\n3            0                     TRUE\n4            0                     TRUE\n5            0                     TRUE\n6            0                     TRUE\n                                                       tags\n1                                                      NULL\n2                                                      NULL\n3                                                      NULL\n4                                                      NULL\n5 #Woman-Owned Business, #Parent, #Single Parent, 6, 16, 17\n6            #Woman-Owned Business, #Repeat Borrower, 6, 28\n\ndata$loans[1, 'location.geo.pairs'] # hmmm...\n\nNULL\n\ndata$loans[1, 'location']\n\n  country_code     country                     town geo.level\n1           PH Philippines Silay, Negros Occidental      town\n             geo.pairs geo.type\n1 10.752818 123.087562    point\n\n\nOne disadvantage of JSON is that it is not set up to deal with missing values, infinity, etc."
  },
  {
    "objectID": "units/unit2-dataTech.html#webscraping-and-web-apis",
    "href": "units/unit2-dataTech.html#webscraping-and-web-apis",
    "title": "Data technologies, formats, and structures",
    "section": "Webscraping and web APIs",
    "text": "Webscraping and web APIs\nHere we’ll see some examples of making requests over the Web to get data. We’ll use APIs to systematically query a website for information. Ideally, but not always, the API will be documented. In many cases that simply amounts to making an HTTP GET request, which is done by constructing a URL.\nThe packages RCurl and httr are useful for a wide variety of such functionality. Note that much of the functionality I describe below is also possible within bash using either wget or curl.\n\nWebscraping ethics and best practices\nWebscraping is the process of extracting data from the web, either directly from a website or using a web API (application programming interface).\n\nShould you webscrape? In general, if we can avoid webscraping (particularly if there is not an API) and instead directly download a data file from a website, that is greatly preferred.\nMay you webscrape? Before you set up any automated downloading of materials/data from the web you should make sure that what you are about to do is consistent with the rules provided by the website.\n\nSome places to look for information on what the website allows are:\n\nlegal pages such as Terms of Service or Terms and Conditions on the website.\ncheck the robots.txt file (e.g., https://scholar.google.com/robots.txt) to see what a web crawler is allowed to do, and whether the site requires a particular delay between requests to the sites\npotentially contact the site owner if you plan to scrape a large amount of data\n\nHere are some links with useful information:\n\nA blog post overview on webscraping and robots.txt\nBlog post on webscraping ethics\nSome information on how to understand a robots.txt file\n\nIn many cases you will want to include a time delay between your automated requests to a site, including if you are not actually crawling a site but just want to automate a small number of queries.\n\n\nWhat is HTTP?\nHTTP (hypertext transfer protocol) is a system for communicating information from a server (i.e., the website of interest) to a client (e.g., your laptop). The client sends a request and the server sends a response.\nWhen you go to a website in a browser, your browser makes an HTTP GET request to the website. Similarly, when we did some downloading of html from webpages above, we used an HTTP GET request.\nAnytime the URL you enter includes ‘param’ information (www.somewebsite.com?param=arg), you are using an API.\nThe response to an HTTP request will include a status code, which can be interpreted based on this information.\nThe response will generally contain content in the form of text (e.g., HTML, XML, JSON) or raw bytes.\n\n\nAPIs: REST- and SOAP-based web services\nIdeally a web service documents their API (Applications Programming Interface) that serves data or allows other interactions. REST and SOAP are popular API standards/styles. Both REST and SOAP use HTTP requests; we’ll focus on REST as it is more common and simpler. The API will (hopefully) document what information it expects from the user and will return the result in a standard format (e.g., a particular file format rather than producing a webpage).\nWhen using REST, we access resources, which might be a Facebook account or a database of stock quotes. The resource may return information in the form of an HTML file or JSON, CSV or something else.\nOften the format of the request is a URL (aka an endpoint) plus a query string, passed as a GET request. Let’s search for plumbers near Berkeley, and we’ll see the GET request, in the form:\nhttps://www.yelp.com/search?find_desc=plumbers&find_loc=Berkeley+CA&ns=1\n\nthe query string begins with ?\nthere are one or more Parameter=Argument pairs\npairs are separated by &\n+ is used in place of each space\n\nWe don’t always get HTML back - try searching for “Purple Rain” at apple.com. What format do you get back?\nLet’s see an example of accessing climate model output data from the World Bank. The API is documented by following some links from here: http://datahelpdesk.worldbank.org/knowledgebase. Following that documentation we can download monthly average precipitation predictions for 2080-2099 for the US (ISO3 code ‘USA’) based on global climate model simulations. In this case our REST-based query is simply constructing a straightforward URL.\n[TODO: the URL for the API is no longer working - 2022-08-18 // CJP]\n\ntimes <- c(2080, 2099)\ncountryCode <- 'USA'\nbaseURL <- \"http://data.worldbank.org/developers/climate-data-api\nbaseURL <- \"http://climatedataapi.worldbank.org/climateweb/rest/v1/country\"\n##\" http://climatedataapi.worldbank.org/climateweb/rest/v1/country\"\ntype <- \"mavg\"\nvar <- \"pr\"\ndata <- read.csv(paste(baseURL, type, var, times[1], times[2],\n                       paste0(countryCode, '.csv'), sep = '/'))\nhead(data)\n\nAs another example, here we can see the Kiva API, which allows us to construct queries on the Kiva data that we saw some of earlier.\nThe Nolan and Temple Lang book provides a number of examples of different ways of authenticating with web services that control access to the service.\nFinally, some web services allow us to pass information to the service in addition to just getting data or information. E.g., you can programmatically interact with your Facebook, Dropbox, and Google Drive accounts using REST based on HTTP POST, PUT, and DELETE requests. Authentication is of course important in these contexts and some times you would first authenticate with your login and password and receive a “token”. This token would then be used in subsequent interactions in the same session.\nI created your github.berkeley.edu accounts from Python by interacting with the Github API using the requests package.\n\n\nHTTP requests by deconstructing an (undocumented) API\nIn some cases an API may not be documented or we might be lazy and not use the documentation. Instead we might deconstruct the queries a browser makes and then mimic that behavior, in some cases having to parse HTML output to get at data. Note that if the webpage changes even a little bit, our carefully constructed query syntax may fail.\nLet’s look at some UN data (agricultural crop data). By going to\nhttp://data.un.org/Explorer.aspx?d=FAO, and clicking on “Crops”, we’ll see a bunch of agricultural products with “View data” links. Click on “apricots” as an example and you’ll see a “Download” button that allows you to download a CSV of the data. Let’s select a range of years and then try to download “by hand”. Sometimes we can right-click on the link that will download the data and directly see the URL that is being accessed and then one can deconstruct it so that you can create URLs programmatically to download the data you want.\nIn this case, we can’t see the full URL that is being used because there’s some Javascript involved. Therefore, rather than looking at the URL associated with a link we need to view the actual HTTP request sent by our browser to the server. We can do this using features of the browser (e.g., in Firefox see Web Developer -> Network and in Chrome More tools -> Developer tools -> Network) (or right-click on the webpage and select Inspect and then Network). Based on this we can see that an HTTP GET request is being used with a URL such as:\nhttp://data.un.org/Handlers/DownloadHandler.ashx?DataFilter=itemCode:526;year:2012,2013,2014,2015,2016,2017&DataMartId=FAO&Format=csv&c=2,4,5,6,7&s=countryName:asc,elementCode:asc,year:desc.\nWe’e now able to easily download the data using that URL, which we can fairly easily construct using string processing in bash, R, or Python, such as this:\n\n## example URL:\n## http://data.un.org/Handlers/DownloadHandler.ashx?DataFilter=itemCode:526;\n##year:2012,2013,2014,2015,2016,2017&DataMartId=FAO&Format=csv&c=2,4,5,6,7&\n##s=countryName:asc,elementCode:asc,year:desc\nitemCode <- 526\nbaseURL <- \"http://data.un.org/Handlers/DownloadHandler.ashx\"\nyrs <- paste(as.character(2012:2017), collapse = \",\")\nfilter <- paste0(\"?DataFilter=itemCode:\", itemCode, \";year:\", yrs)\nargs1 <- \"&DataMartId=FAO&Format=csv&c=2,3,4,5,6,7&\"\nargs2 <- \"s=countryName:asc,elementCode:asc,year:desc\"\nurl <- paste0(baseURL, filter, args1, args2)\n## if the website provided a CSV we could just do this:\n## apricots <- read.csv(url)\n## but it zips the file\ntemp <- tempfile()  ## give name for a temporary file\ndownload.file(url, temp)\ndat <- read.csv(unzip(temp))  ## using a connection (see Section 2)\n\nhead(dat)\n\n  Country.or.Area Element.Code                                         Element\n1     Afghanistan          432 Gross Production Index Number (2014-2016 = 100)\n2     Afghanistan          432 Gross Production Index Number (2014-2016 = 100)\n3     Afghanistan          432 Gross Production Index Number (2014-2016 = 100)\n4     Afghanistan          432 Gross Production Index Number (2014-2016 = 100)\n5     Afghanistan          432 Gross Production Index Number (2014-2016 = 100)\n6     Afghanistan          432 Gross Production Index Number (2014-2016 = 100)\n  Year  Unit  Value Value.Footnotes\n1 2017 index 202.19              Fc\n2 2016 index  27.45              Fc\n3 2015 index 134.50              Fc\n4 2014 index 138.05              Fc\n5 2013 index 138.05              Fc\n6 2012 index 128.08              Fc\n\n\nSo, what have we achieved?\n\nWe have a reproducible workflow we can share with others (perhaps ourself in the future).\nWe can automate the process of downloading many such files.\n\n\n\nMore details on HTTP requests\nA more sophisticated way to do the download is to pass the request in a structured way with named input parameters. This request is easier to construct programmatically. Here what is returned is a zip file, which is represented in R as a sequence of “raw” bytes. We can use httr’s GET(), followed by writing to disk and reading back in, as follows (for some reason knitr won’t print the output…):\n\nlibrary(httr)\n\n\nAttaching package: 'httr'\n\n\nThe following object is masked from 'package:curl':\n\n    handle_reset\n\noutput2 <- GET(baseURL, query = list(\n               DataFilter = paste0(\"itemCode:\", itemCode, \";year:\", yrs),\n               DataMartID = \"FAO\", Format = \"csv\", c = \"2,3,4,5,6,7\",\n               s = \"countryName:asc,elementCode:asc,year:desc\"))\ntemp <- tempfile()  ## give name for a temporary file\nwriteBin(content(output2, 'raw'), temp)  ## write out as zip file\ndat <- read.csv(unzip(temp))\nhead(dat)\n\n  Country.or.Area Element.Code                                         Element\n1     Afghanistan          432 Gross Production Index Number (2014-2016 = 100)\n2     Afghanistan          432 Gross Production Index Number (2014-2016 = 100)\n3     Afghanistan          432 Gross Production Index Number (2014-2016 = 100)\n4     Afghanistan          432 Gross Production Index Number (2014-2016 = 100)\n5     Afghanistan          432 Gross Production Index Number (2014-2016 = 100)\n6     Afghanistan          432 Gross Production Index Number (2014-2016 = 100)\n  Year  Unit  Value Value.Footnotes\n1 2017 index 202.19              Fc\n2 2016 index  27.45              Fc\n3 2015 index 134.50              Fc\n4 2014 index 138.05              Fc\n5 2013 index 138.05              Fc\n6 2012 index 128.08              Fc\n\n\nIn some cases we may need to send a lot of information as part of the URL in a GET request. If it gets to be too long (e.g,, more than 2048 characters) many web servers will reject the request. Instead we may need to use an HTTP POST request (POST requests are often used for submitting web forms). A typical request would have syntax like this search (using RCurl):\n\nif(url.exists('http://www.wormbase.org/db/searches/advanced/dumper')) {\n      x = postForm('http://www.wormbase.org/db/searches/advanced/dumper',\n              species=\"briggsae\",\n              list=\"\",\n              flank3=\"0\",\n              flank5=\"0\",\n              feature=\"Gene Models\",\n              dump = \"Plain TEXT\",\n              orientation = \"Relative to feature\",\n              relative = \"Chromsome\",\n              DNA =\"flanking sequences only\",\n              .cgifields = paste(c(\"feature\", \"orientation\", \"DNA\",\n                                   \"dump\",\"relative\"), collapse=\", \"))\n}\n\nUnfortunately that specific search doesn’t work because the server URL and/or API seem to have changed. But it gives you an idea of what the format would look like.\nhttr and RCurl can handle other kinds of HTTP requests such as PUT and DELETE. Finally, some websites use cookies to keep track of users and you may need to download a cookie in the first interaction with the HTTP server and then send that cookie with later interactions. More details are available in the Nolan and Temple Lang book.\n\n\nPackaged access to an API\nFor popular websites/data sources, a developer may have packaged up the API calls in a user-friendly fashion for use from R, Python or other software. For example there are Python (twitter) and R (twitteR) packages for interfacing with Twitter via its API.\nHere’s some example code for Python (the Python package seems to be more fully-featured than the R package). This looks up the US senators’ Twitter names and then downloads a portion of each of their timelines, i.e., the time series of their tweets. Note that Twitter has limits on how much one can download at once.\n\nimport json\nimport twitter\n\n# You will need to set the following variables with your\n# personal information.  To do this you will need to create\n# a personal account on Twitter (if you don't already have\n# one).  Once you've created an account, create a new\n# application here:\n#    https://dev.twitter.com/apps\n#\n# You can manage your applications here:\n#    https://apps.twitter.com/\n#\n# Select your application and then under the section labeled\n# \"Key and Access Tokens\", you will find the information needed\n# below.  Keep this information private.\nCONSUMER_KEY       = \"\"\nCONSUMER_SECRET    = \"\"\nOAUTH_TOKEN        = \"\"\nOAUTH_TOKEN_SECRET = \"\"\n\nauth = twitter.oauth.OAuth(OAUTH_TOKEN, OAUTH_TOKEN_SECRET,\n                           CONSUMER_KEY, CONSUMER_SECRET)\napi = twitter.Twitter(auth=auth)\n\n# get the list of senators\nsenators = api.lists.members(owner_screen_name=\"gov\", slug=\"us-senate\", count=100)\n\n# get all the senators' timelines\nnames = [d[\"screen_name\"] for d in senators[\"users\"]]\ntimelines = [api.statuses.user_timeline(screen_name=name, count = 500) \n             for name in names]\n\n# save information out to JSON\nwith open(\"senators-list.json\", \"w\") as f:\n    json.dump(senators, f, indent=4, sort_keys=True)\nwith open(\"timelines.json\", \"w\") as f:\n    json.dump(timelines, f, indent=4, sort_keys=True)\n\n\n\nAccessing dynamic pages\nSome websites dynamically change in reaction to the user behavior. In these cases you need a tool that can mimic the behavior of a human interacting with a site. Some options are:\n\nselenium (and the RSelenium wrapper for R) is a popular tool for doing this.\nsplash (and the splashr wrapper for R) is another approach.\nhtmlunit is another tool for this."
  },
  {
    "objectID": "office_hours.html",
    "href": "office_hours.html",
    "title": "Statistics 243 Fall 2022",
    "section": "",
    "text": "Chris (Evans 495 or Zoom (see Ed Discussion post for link))\n\nTBD\ngenerally available immediately after class\nfeel free to schedule an appointment or drop by if my door is open\n\nJames:\n\nTBD\nFridays during unused section time, generally 2-3 pm and 4-4:30 pm (Evans 344)"
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Statistics 243 Fall 2022",
    "section": "",
    "text": "Statistics 243 is an introduction to statistical computing, taught using R. The course will cover both programming concepts and statistical computing concepts. Programming concepts will include data and text manipulation, data structures, functions and variable scope, regular expressions, debugging/testing, and parallel processing. Statistical computing topics will include working with large datasets, numerical linear algebra, computer arithmetic/precision, simulation studies and Monte Carlo methods, numerical optimization, and numerical integration/differentiation. A goal is that coverage of these topics complement the models/methods discussed in the rest of the statistics/biostatistics graduate curriculum. We will also cover the basics of UNIX/Linux, in particular some basic shell scripting and operating on remote servers, as well as a moderate amount of Python.\n\n\nWhile the course is taught using R and you will learn a lot about using R at an advanced level, this is not a course about learning R. Rather the focus of the course is computing for statistics and data science more generally, using R to illustrate the concepts. Also, this is not a course that will cover specific statistical/machine learning/data analysis methods.\n\n\n\nInformal prerequisites: If you are not a statistics or biostatistics graduate student, please chat with me if you’re not sure if this course makes sense for you. A background in calculus, linear algebra, probability and statistics is expected, as well as a basic ability to operate on a computer (but I do not assume familiarity with the UNIX-style command line/terminal/shell). Furthermore, I’m expecting you will know the basics of R, at the level of the Modules 1-5 in the R bootcamp offered Aug. 20-21, 2022. If you don’t have that background you’ll need to spend time in the initial couple weeks getting up to speed. I may also be able to give access to the bootcamp videos. In addition, we’ll have an optional hands-on practice session during the second or third week of class, and the GSI can also provide assistance."
  },
  {
    "objectID": "syllabus.html#covid-considerations",
    "href": "syllabus.html#covid-considerations",
    "title": "Statistics 243 Fall 2022",
    "section": "Covid considerations",
    "text": "Covid considerations\nWe’ll be following university policy through the semester.\n\nClass and section are in person. I will be recording class (but not section) via the room’s course capture capabilities so if anyone needs to miss a class they should be able to catch up. There is in-class discussion and problem-solving so I expect students to attend class in general. However, please do not come to class if you feel any symptoms – you can watch the recording and it will not affect your grade in any way.\nMasks are not required at the moment, but you are welcome to wear one. For the sake of communicating most clearly I won’t wear one while teaching."
  },
  {
    "objectID": "syllabus.html#objectives-of-the-course",
    "href": "syllabus.html#objectives-of-the-course",
    "title": "Statistics 243 Fall 2022",
    "section": "Objectives of the course",
    "text": "Objectives of the course\nThe goals of the course are that, by the end of the course, students be able to:\n\noperate effectively in a UNIX environment and on remote servers and compute clusters;\nhave a solid understanding of general programming concepts and principles, and be able to program effectively in R with an advanced knowledge of R functionality;\nbe familiar with concepts and tools for reproducible research and good scientific computing practices; and\nunderstand in depth and be able to make use of principles of numerical linear algebra, optimization, and simulation for statistics- and data science-related analyses and research."
  },
  {
    "objectID": "syllabus.html#topics-in-order-with-rough-timing",
    "href": "syllabus.html#topics-in-order-with-rough-timing",
    "title": "Statistics 243 Fall 2022",
    "section": "Topics (in order with rough timing)",
    "text": "Topics (in order with rough timing)\nThe ‘days’ here are (roughly) class sessions, as general guidance.\n\nIntroduction to UNIX, operating on a compute server (1 day)\nData formats, data access, webscraping, data structures (2 days)\nDebugging, good programming practices, reproducible research (1 day)\nThe bash shell and shell scripting, version control (3 days)\nProgramming concepts and advanced R programming: text processing and regular expressions, object-oriented programming, functions and variable scope, efficient programming, memory use (9 days)\nComputer arithmetic/representation of numbers on a computer (3 days)\nParallel processing (2 days)\nWorking with databases, hashing, and big data (3 days)\nNumerical linear algebra (5 days)\nSimulation studies and Monte Carlo (2 days)\nOptimization (7 days)\nNumerical integration and differentiation (1 day)\nGraphics (1 day)\n\nIf you want to get a sense of what material we will cover in more detail, in advance, you can take a look at the materials in the units directory of GitHub repository from when I taught the class in 2021. See https://github.com/berkeley-stat243/stat243-fall-2021."
  },
  {
    "objectID": "syllabus.html#personnel",
    "href": "syllabus.html#personnel",
    "title": "Statistics 243 Fall 2022",
    "section": "Personnel",
    "text": "Personnel\n\nInstructor:\n\nChris Paciorek (paciorek@stat.berkeley.edu)\n\nGSI\n\nJames Duncan\n\nOffice hours can be found here.\nWhen to see us about an assignment: We’re here to help, including providing guidance on assignments. You don’t want to be futilely spinning your wheels for a long time getting nowhere. That said, before coming to see us about a difficulty, you should try something a few different ways and define/summarize for yourself what is going wrong or where you are getting stuck."
  },
  {
    "objectID": "syllabus.html#course-websites-github-piazza-gradescope-and-bcourses",
    "href": "syllabus.html#course-websites-github-piazza-gradescope-and-bcourses",
    "title": "Statistics 243 Fall 2022",
    "section": "Course websites: GitHub, Ed Discussion, Gradescope, and bCourses",
    "text": "Course websites: GitHub, Ed Discussion, Gradescope, and bCourses\nKey websites for the course are:\n\nThis course website, which is hosted on GitHub pages, and the GitHub repository containing the source materials: https://github.com/berkeley-stat243/stat243-fall-2022\nSCF tutorials for additional content: https://statistics.berkeley.edu/computing/training/tutorials\nEd Discussion site for discussions/Q&A: https://edstem.org/us/courses/25090/discussion/\nbCourses site for course capture recordings and possibly some other materials: https://bcourses.berkeley.edu/courses/1507757.\nGradescope for assignments (also linked from bCourses): [UNDER CONSTRUCTION]https://www.gradescope.com/courses/XYZ\n\nAll course materials will be posted on here / on GitHub except for video content, which will be in bCourses.\n\nCourse discussion\nWe will use the course Ed Discussion site for communication (announcements, questions, and discussion). You should ask questions about class material and problem sets through the site. Please use this site for your questions so that either James or I can respond and so that everyone can benefit from the discussion. I suggest you to modify your settings on Ed Discussion so you are informed by email of postings. I strongly encourage you to respond to or comment on each other’s questions as well (this will help your class participation grade), although of course you should not provide a solution to a problem set problem. If you have a specific administrative question you need to direct just to me, it’s fine to email me directly or post private on the Discussion site. But if you simply want to privately ask a question about content, then just come to an office hour or see me after class or James during/after section.\nIf you’re enrolled in the class you should be a member of the group and be able to access it. If you’re auditing or not yet enrolled and would like access, make sure to fill out the course survey and I will add you. In addition, we will use Gradescope for viewing grades."
  },
  {
    "objectID": "syllabus.html#course-material",
    "href": "syllabus.html#course-material",
    "title": "Statistics 243 Fall 2022",
    "section": "Course material",
    "text": "Course material\n\nPrimary materials: Course notes on GitHub, SCF tutorials, and potentially pre-recorded videos on bCourses.\nBack-up textbooks:\n\nFor bash: Newham, Cameron and Rosenblatt, Bill. Learning the bash Shell (available electronically through OskiCat: http://uclibs.org/PID/77225)\nFor R:\n\nAdler, Joseph; R in a Nutshell (available electronically through OskiCat: http://uclibs.org/PID/151634)\nWickham, Hadley: Advanced R: http://adv-r.had.co.nz/\n\nFor statistical computing topics:\n\nGentle, James. Computational Statistics (available electronically through OskiCat: http://dx.doi.org/10.1007/978-0-387-98144-4)\nGentle, James. Matrix Algebra https://link-springer-com.libproxy.berkeley.edu/book/10.1007%2F978-3-319-64867-5 or Numerical Linear Algebra with Applications in Statistics https://link-springer-com.libproxy.berkeley.edu/chapter/10.1007/978-1-4612-0623-1_1\n\nOther resources with more details on particular aspects of R:\n\nChambers, John; Software for Data Analysis: Programming with R (available electronically through OskiCat: http://dx.doi.org/10.1007/978-0-387-75936-4)\nXie, Yihui; Dynamic documents with R and knitr. (available electronically through Oskicat)\nNolan, Deborah and Temple Lang, Duncan. XML and Web Technologies for Data Sciences with R. https://link.springer.com/book/10.1007%2F978-1-4614-7900-0\nThe R-intro and R-lang documentation. https://www.cran.r-project.org/manuals.html\nMurrell, Paul; R Graphics, 2nd ed. http://www.stat.auckland.ac.nz/\\(\\sim\\)paul/RG2e/\nMurrell, Paul; Introduction to Data Technologies. http://www.stat.auckland.ac.nz/\\(\\sim\\)paul/ItDT/\n\nOther resources with more detail on particular aspects of statistical computing concepts:\n\nLange, Kenneth; Numerical Analysis for Statisticians, 2nd ed. (first edition is available electronically through OskiCat:\nhttps://link.springer.com/book/10.1007%2Fb98850)\nMonahan, John; Numerical Methods of Statistics (available electronically through OskiCat:\nhttp://dx.doi.org/10.1017/CBO9780511977176)"
  },
  {
    "objectID": "syllabus.html#section",
    "href": "syllabus.html#section",
    "title": "Statistics 243 Fall 2022",
    "section": "Section",
    "text": "Section\nThe GSI will lead a two-hour discussion section each week (there are two sections). By and large, these will only last for about one hour of actual content, but the second hour may be used as an office hour with the GSI or for troubleshooting software during the early weeks. The discussion sections will vary in format and topic, but material will include demonstrations on various topics (version control, debugging, testing, etc.), group work on these topics, discussion of relevant papers, and discussion of problem set solutions. The first section (noon - 2 pm) generally has more demand, so to avoid having too many people in the room, you should go to your assigned section unless you talk to me first."
  },
  {
    "objectID": "syllabus.html#computing-resources",
    "href": "syllabus.html#computing-resources",
    "title": "Statistics 243 Fall 2022",
    "section": "Computing Resources",
    "text": "Computing Resources\nMost work for the course can be done on your laptop. Later in the course we’ll also use the Statistics Department cluster. You can also use the SCF JupyterHub or the campus DataHub to access a bash shell or run RStudio.\nThe software needed for the course is as follows:\n\nAccess to the UNIX command line (bash shell)\nGit\nR (RStudio is recommended but by no means required)\nPython (later in the course)\n\nWe have some tips for software installation (and access to DataHub), including suggestions for how to access a UNIX shell, which you’ll need to be able to do by the second week of class."
  },
  {
    "objectID": "syllabus.html#class-time",
    "href": "syllabus.html#class-time",
    "title": "Statistics 243 Fall 2022",
    "section": "Class time",
    "text": "Class time\nMy goal is to have classes be an interactive environment. This is both more interesting for all of us and more effective in learning the material. I encourage you to ask questions and will pose questions to the class to think about, respond to via Google forms, and discuss. To increase time for discussion and assimilation of the material in class, before some classes I may ask that you read material or work through tutorials in advance of class. Occasionally, I will ask you to submit answers to questions in advance of class as well.\nPlease do not use phones during class and limit laptop use to the material being covered.\nStudent backgrounds with computing will vary. For those of you with limited background on a topic, I encourage you to ask questions during class so I know what you find confusing. For those of you with extensive background on a topic (there will invariably be some topics where one of you will know more about it than I do or have more real-world experience), I encourage you to pitch in with your perspective. In general, there are many ways to do things on a computer, particularly in a UNIX environment and in R, so it will help everyone (including me) if we hear multiple perspectives/ideas.\nFinally, class recordings for review or to make up for absence will be available through the bCourses Media Gallery, available on the Media Gallery tab on the bCourses page for the class."
  },
  {
    "objectID": "syllabus.html#course-requirements-and-grading",
    "href": "syllabus.html#course-requirements-and-grading",
    "title": "Statistics 243 Fall 2022",
    "section": "Course requirements and grading",
    "text": "Course requirements and grading\n\nScheduling Conflicts\nCampus asks that I include this information about conflicts: Please notify me in writing by the second week of the term about any known or potential extracurricular conflicts (such as religious observances, graduate or medical school interviews, or team activities). I will try my best to help you with making accommodations, but I cannot promise them in all cases. In the event there is no mutually-workable solution, you may be dropped from the class.\nThe main conflict that would be a problem would be the quizzes, whose dates I will determine in late August / early September.\nQuizzes are in-person. There is no remote option, and the only make-up accommodations I will make are for illness or serious personal issues. Do not schedule any travel that may conflict with a quiz.\n\n\nCourse grades\nThe grade for this course is primarily based on assignments due every 1-2 weeks, two quizzes (likely in early-mid October and mid-late November), and a final group project. I will also provide extra credit questions on some problem sets. There is no final exam. 50% of the grade is based on the problem sets, 25% on the quizzes, 15% on the project, and 10% on your participation in discussions on Ed, your responses to the in-class Google forms questions, as well as occasional brief questions that I will ask you to answer in advance of the next class.\nGrades will generally be As and Bs. An A involves doing all the work, getting full credit on most of the problem sets, showing competence on the quizzes, and doing a thorough job on the final project.\n\n\nProblem sets\nWe will be less willing to help you if you come to our office hours or post a question online at the last minute. Working with computers can be unpredictable, so give yourself plenty of time for the assignments.\nThere are several rules for submitting your assignments.\n\nYou should prepare your assignments using either R Markdown (or the new Quarto format) or LaTeX plus knitr.\nProblem set submission consists of both of the following:\n\nA PDF submitted electronically through Gradescope, by the start of class (10 am) on the due date, and\nAn electronic copy of the PDF, code file, and R Markdown/Quarto/knitr document pushed to your class GitHub repository, following the instructions to be provided by the GSI.\n\nOn-time submission will be determined based on the time stamp of when the PDF is submitted to Gradescope.\nAnswers should consist of textual response or mathematical expressions as appropriate, with key chunks of code embedded within the document. Extensive additional code can be provided as an appendix. Before diving into the code for a problem, you should say what the goal of the code is and your strategy for solving the problem. Raw code without explanation is not an appropriate solution. Please see our qualitative grading rubric for guidance. In general the rubric is meant to reinforce good coding practices and high-quality scientific communication.\nAny mathematical derivations may be done by hand and scanned with your phone if you prefer that to writing up LaTeX equations.\n\nNote: R Markdown is an extension to the Markdown markup language that allows one to embed R code within an HTML document. Quarto is a new tool that generalizes R Markdown and provides the compatible qmd format. knitr is a tool that allows one to embed chunks of code within LaTeX documents, including with Overleaf and the LyX GUI front-end to LaTeX. Please see the SCF dynamics document tutorial; there will be additional information in the first section and on the first problem set.\n\n\nSubmitting assignments\nIn the first section (September 2), we’ll discuss how to submit your problem sets both on Gradescope and via your class GitHub repository, located at https:github.berkeley.edu/<your_calnet_username>.\n\n\nProblem set grading\nThe grading scheme for problem sets is as follows. Each problem set will receive a numeric score for (1) presentation and explanation of results, (2) technical accuracy of code or mathematical derivation, and (3) code quality/style and creativity. For each of these three components, the possible scores are:\n\n0 = no credit,\n1 = partial credit (you did some of the problems but not all),\n2 = satisfactory (you tried everything but there were pieces of what you did that didn’t solve or present/explain one or more problems in a complete way), and\n3 = full credit.\n\nAgain, the qualitative grading rubric provides guidance on what we want to see for full credit.\nFor components #1 and #3, many of you will get a score of 2 for some problem sets as you develop good coding practices. You can still get an A in the class despite this.\nYour total score for the PS is a weighted sum of the scores for the three components. If you turn in a PS late, I’ll bump you down by two points (out of the available). If you turn it in really late (i.e., after we start grading them), I will bump you down by four points. No credit after solutions are distributed.\n\n\nFinal project\nThe final project will be a joint coding project in groups of 3-4. I’ll assign an overall task, and you’ll be responsible for dividing up the work, coding, debugging, testing, and documentation. You’ll need to use the Git version control system for working in your group.\n\n\nRules for working together and the campus honor code\nI encourage you to work together and help each other out. However, the problem set solutions you submit must be your own. What do I mean by that?\n\nYou must first try to figure out a given problem on your own. After that, if you’re stuck or want to explore alternative approaches or check what you’ve done, feel free to consult with your fellow students and with the GSI and me.\nWhat does “consult with a fellow student mean”? You can discuss a problem with another student, brainstorm approaches, and share code syntax (generally not more than one line) on how to do small individual coding tasks within a problem.\n\nYou should not ask another student for complete code or solutions, or look at their code/solution.\nYou should not share complete code or solutions with another student or on Ed Discussion.\n\nYou must provide attribution for ideas obtained elsewhere, including other students.\n\nIf you got a specific idea for how to do part of a problem from a fellow student, you should note that in your solution in the appropriate place (for specific syntax ideas, note this in a code comment), just as you would cite a book or URL.\nYou MUST note on your problem set solution any fellow students who you worked/consulted with.\nYou do not need to cite any Ed Discussion posts nor any discussions with Chris or James.\n\nUltimately, your solution to a problem set (writeup and code) must be your own, and you’ll hear from me if either look too similar to someone else’s.\n\nPlease see the last section of this document for more information on the Campus Honor Code, which I expect you to follow."
  },
  {
    "objectID": "syllabus.html#feedback",
    "href": "syllabus.html#feedback",
    "title": "Statistics 243 Fall 2022",
    "section": "Feedback",
    "text": "Feedback\nI welcome comments and suggestions and concerns. Particularly good suggestions will count towards your class participation grade."
  },
  {
    "objectID": "syllabus.html#accomodations-for-students-with-disabilities",
    "href": "syllabus.html#accomodations-for-students-with-disabilities",
    "title": "Statistics 243 Fall 2022",
    "section": "Accomodations for Students with Disabilities",
    "text": "Accomodations for Students with Disabilities\nPlease see me as soon as possible if you need particular accommodations, and we will work out the necessary arrangements."
  },
  {
    "objectID": "syllabus.html#campus-honor-code",
    "href": "syllabus.html#campus-honor-code",
    "title": "Statistics 243 Fall 2022",
    "section": "Campus Honor Code",
    "text": "Campus Honor Code\nThe following is the Campus Honor Code. With regard to collaboration and independence, please see my rules regarding problem sets above – Chris.\nThe student community at UC Berkeley has adopted the following Honor Code: “As a member of the UC Berkeley community, I act with honesty, integrity, and respect for others.” The hope and expectation is that you will adhere to this code.\nCollaboration and Independence: Reviewing lecture and reading materials and studying for exams can be enjoyable and enriching things to do with fellow students. This is recommended. However, unless otherwise instructed, homework assignments are to be completed independently and materials submitted as homework should be the result of one’s own independent work.\nCheating: A good lifetime strategy is always to act in such a way that no one would ever imagine that you would even consider cheating. Anyone caught cheating on a quiz or exam in this course will receive a failing grade in the course and will also be reported to the University Center for Student Conduct. In order to guarantee that you are not suspected of cheating, please keep your eyes on your own materials and do not converse with others during the quizzes and exams.\nPlagiarism: To copy text or ideas from another source without appropriate reference is plagiarism and will result in a failing grade for your assignment and usually further disciplinary action. For additional information on plagiarism and how to avoid it, see, for example: http://gsi.berkeley.edu/teachingguide/misconduct/prevent-plag.html\nAcademic Integrity and Ethics: Cheating on exams and plagiarism are two common examples of dishonest, unethical behavior. Honesty and integrity are of great importance in all facets of life. They help to build a sense of self-confidence, and are key to building trust within relationships, whether personal or professional. There is no tolerance for dishonesty in the academic world, for it undermines what we are dedicated to doing – furthering knowledge for the benefit of humanity.\nYour experience as a student at UC Berkeley is hopefully fueled by passion for learning and replete with fulfilling activities. And we also appreciate that being a student may be stressful. There may be times when there is temptation to engage in some kind of cheating in order to improve a grade or otherwise advance your career. This could be as blatant as having someone else sit for you in an exam, or submitting a written assignment that has been copied from another source. And it could be as subtle as glancing at a fellow student’s exam when you are unsure of an answer to a question and are looking for some confirmation. One might do any of these things and potentially not get caught. However, if you cheat, no matter how much you may have learned in this class, you have failed to learn perhaps the most important lesson of all."
  },
  {
    "objectID": "rubric.html",
    "href": "rubric.html",
    "title": "Statistics 243 Fall 2022",
    "section": "",
    "text": "This document provides guidance for submitting high-quality problem set (and project) solutions. This guidance is based on general good practices for scientific communication, reproducible research, and software development."
  },
  {
    "objectID": "rubric.html#general-presentation",
    "href": "rubric.html#general-presentation",
    "title": "Statistics 243 Fall 2022",
    "section": "General presentation",
    "text": "General presentation\n\nSimply presenting code or derivations is not sufficient.\nBriefly describe the overall goal or strategy before providing code/derivations.\nAs needed describe what the pieces of your code/derivation are doing to make it easier for a reader to follow the steps."
  },
  {
    "objectID": "rubric.html#coding-practice",
    "href": "rubric.html#coding-practice",
    "title": "Statistics 243 Fall 2022",
    "section": "Coding practice",
    "text": "Coding practice\n\nMinimize (or eliminate) use of global variables.\nBreak down work into core tasks and develop small, modular, self-contained functions (or class methods) to carry out those tasks.\nDon’t repeat code. As needed refactor code to create new functions (or class methods).\nFunctions and classes should be “weakly coupled”, interacting via their interfaces and not by having to know the internals of how they work.\nUse data structures appropriate for the computations that need to be done.\nDon’t hard code ‘magic’ numbers. Assign such numbers to variables with clear names, e.g., speedOfLight = 3e8.\nProvide reasonable default arguments to functions (or class methods) when possible.\nProvide tests (including unit tests) when requested (this is good general practice but we won’t require it in all cases).\nAvoid overly complicated syntax – try to use the clearest syntax you can to solve the problem.\nIn terms of speed, don’t worry about it too much so long as the code finishes real-world tasks in a reasonable amount of time. When optimizing, focus on the parts of the code that are the bottlenecks.\nUse functions already available in the language rather than recreating yourself."
  },
  {
    "objectID": "rubric.html#code-style",
    "href": "rubric.html#code-style",
    "title": "Statistics 243 Fall 2022",
    "section": "Code style",
    "text": "Code style\n\nFollow a consistent style. For example (not required) the tidyverse style guide or its offshoot Google R style.\nUse informative variable and function names and have a consistent naming style.\nUse whitespace (spaces, newlines) and parentheses to make the structure of the code easy to understand and the individual syntax pieces clear.\nUse consistent indentation to make the structure of the code easy to understand.\nProvide comments that give the goal of a given piece of code and why it does things, but don’t use comments to restate what the code does when it should be obvious from reading the code.\n\nProvide summaries for blocks of code.\nFor particularly complicated syntax, say what a given piece of code does."
  },
  {
    "objectID": "software.html",
    "href": "software.html",
    "title": "Statistics 243 Fall 2022",
    "section": "",
    "text": "Accessing the UNIX command line (i.e., bash or zsh shell)\nInstalling Git\nInstalling R and RStudio\n\nInstalling R and RStudio on Windows\n\nUsing Linux via the Windows Subsystem for Linux\nAccessing Python\nSubmitting problem sets"
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Statistics 243 Fall 2022",
    "section": "",
    "text": "Week\nDay\nDate\nTime\nActivity/Assignment\n\n\n\n\n1\nThursday\n2022-08-25\n4:00-5:30 pm\nOptional: Introduction to LaTeX session run by the library (see Details below)\n\n\n\nFriday\n2022-08-26\nnoon\nRequired (part of your class participation): class survey\n\n\n\nFriday\n2022-08-26\nnoon\nRequired: office hour time survey\n\n\n\nFriday\n2022-08-26\nnoon\nOptional: survey for time for extra R help session\n\n\n\nFriday\n2022-08-26\nlab (1:00-4:30 pm)\nOptional: Lab 0 help session for software installation/setup and UNIX command line basics (see Details below)\n\n\n2\nMonday\n2022-08-29\n10 am\nRequired: read first three sections of Unit 2 (sections before ‘Webscraping’)\n\n\n\nMonday\n2022-08-29\n\nOptional: work through the UNIX basics tutorial and answer (for yourself) the questions at the end\n\n\n\nTBD\nTBD\nTBD\nOptional: R help session (see Details below)\n\n\n\nFriday\n2022-09-02\n10 am\nRequired: Bash shell tutorial and exercises (see Details below)\n\n\n\nFriday\n2022-09-02\nlab\nRequired: Lab 1 on using course tools and problem set submission (see Details below)\n\n\n3\nWednesday\n2022-09-07\n4:00-5:30 pm\nOptional: Introduction to LaTeX session run by the library (see Details below)\n\n\n\nFriday\n2022-09-09\n10 am\nRegular expression tutorial and exercises (see Details below)\n\n\n\nFriday\n2022-09-09\nlab\nRequired: Lab 2 on assertions and testing"
  },
  {
    "objectID": "schedule.html#notes-on-assignments-and-activities",
    "href": "schedule.html#notes-on-assignments-and-activities",
    "title": "Statistics 243 Fall 2022",
    "section": "Notes on assignments and activities",
    "text": "Notes on assignments and activities\n\nOptional library LaTeX sessions: I highly recommend (in particular if you are a Statistics graduate student) that you know how to create equations in LaTeX.\nOptional Lab 0 software/command line help session (August 26 in Evans 344): Help session for installing software, accessing a UNIX-style command line, and basic UNIX usage (e.g., the UNIX basics tutorial). You should have software installed, be able to accesss the command line, and have started to become familiar with basic UNIX usage before class on Wednesday August 31.\nOptional R help session (date/location TBD): If you are not familiar with R at the level of modules 1-5 of the R bootcamp, work through those modules and the breakout problems associated with the modules. If you’d like help and an opportunity for extra practice, please attend the special R catch-up session listed above. Some other resources for R are listed at the end of module 11 of the bootcamp materials, so you could also use those resources. You should do this by the end of the week of September 6.\nBash shell tutorial and exercises: (by September 2), read through this tutorial on using the bash shell. See the notes in Unit 3 on what topics you can skip; in particular you can skip the pages on Regular Expressions and Managing Processes. Work through the first 10 problems in the exercises the end of the tutorial and submit answers via TBD as text. This is not a formal problem set, so you don’t need to worry about formatting nor about explaining/commenting your answers, nor do you need to put your answers in your GitHub class repository. In fact it’s even fine with me if you hand-write the answers and scan them to an electronic document. I just want to make sure you’ve worked through the tutorial. I’ll be doing demonstrations on using the bash shell in class starting on Wednesday August 31, so that will be helpful as you work through the tutorial.\nLab 1 (September 2): First section/lab on using Git, setting up your GitHub repository for problem sets, and using R Markdown/Quarto/knitr to generate dynamic documents. Please come only to the section you are registered for given space limits in the room, unless you have talked with Chris and have his permission.\nRegular expression tutorial and exercises (by September 9), read the regular expression material in the tutorial on using the bash shell. Also read Section 2.1 of the string processing tutorial (you can focus on Section 2.1.2 on stringr if you wish). Then answer the regular expressions (regex) practice problems and submit your answers via TBD. This is not one of the graded problem sets but rather an ‘assignment’ that will simply be noted as being completed or not."
  },
  {
    "objectID": "schedule.html#quiz-schedule",
    "href": "schedule.html#quiz-schedule",
    "title": "Statistics 243 Fall 2022",
    "section": "Quiz schedule",
    "text": "Quiz schedule\nQuizzes are in-person only.\nDates: TBD"
  },
  {
    "objectID": "schedule.html#project-schedule",
    "href": "schedule.html#project-schedule",
    "title": "Statistics 243 Fall 2022",
    "section": "Project schedule",
    "text": "Project schedule\nDue date: TBD, but due either the reading week or the exam week."
  }
]